{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Path to the dataset\n",
    "file_path = \"/scratch2/bsow/Documents/ACSR/data/claire_dialogue/train.txt\"\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    text = re.sub(r\"\\[.*?\\]\\s*\", \"\", text)\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "\n",
    "    cleaned_text = []\n",
    "    current_sentence = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.lower()\n",
    "        cleaned_text.append(line)\n",
    "\n",
    "    if current_sentence:\n",
    "        cleaned_text.append(current_sentence.strip())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to remove punctuation from a list of sentences\n",
    "def remove_punctuation(sentences):\n",
    "    punctuation_pattern = re.compile(r\"[^\\w\\s'-]\")  \n",
    "    cleaned_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Remove punctuation using the regex pattern\n",
    "        cleaned_sentence = re.sub(punctuation_pattern, \"\", sentence)\n",
    "        cleaned_sentences.append(cleaned_sentence.strip())\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "# Function to convert text to IPA using espeak-ng\n",
    "def text_to_ipa(text, language=\"fr\"):\n",
    "    \"\"\"\n",
    "    Convert text to IPA using espeak-ng.\n",
    "    \"\"\"\n",
    "    # Remove special characters\n",
    "    text = text.replace(\"?\", \"\").replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\"'\", \"\").replace(\"-\", \" \")\n",
    "\n",
    "    command = [\"espeak-ng\", \"-v\", language, \"-q\", \"--ipa\"]\n",
    "    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate(input=text.encode())\n",
    "    ipa_output = stdout.decode().strip()\n",
    "    ipa_output = ipa_output.replace(\"ˈ\", \"\").replace(\"ˌ\", \"\").replace(\"-\", \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "    return ipa_output\n",
    "\n",
    "# Function to syllabify IPA text\n",
    "def syllabify_ipa(ipa_text):\n",
    "    consonants = \"ptkbdgmnlrsfvzʃʒɡʁjwŋtrɥgʀycɲ\"\n",
    "    vowels = \"aeɛioɔuøœəɑ̃ɛ̃ɔ̃œ̃ɑ̃ɔ̃ɑ̃ɔ̃\"\n",
    "    phonemes = list(ipa_text.replace(\" \", \"\"))\n",
    "    syllables = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(phonemes):\n",
    "        phone = phonemes[i]\n",
    "        if phone in vowels:\n",
    "            # Check if the next character is a combining diacritic\n",
    "            if i + 1 < len(phonemes) and phonemes[i + 1] == \"̃\":  # Corrected: No space after tilde\n",
    "                syllable = phone + phonemes[i + 1]  # Combine base character with diacritic\n",
    "                syllables.append(syllable)\n",
    "                i += 2  # Skip the diacritic in the next iteration\n",
    "            else:\n",
    "                syllables.append(phone)\n",
    "                i += 1\n",
    "        elif phone in consonants:\n",
    "            # Check if there is a next phone\n",
    "            if i + 1 < len(phonemes):\n",
    "                next_phone = phonemes[i + 1]\n",
    "                if next_phone in vowels:\n",
    "                    # Check if the vowel has a combining diacritic\n",
    "                    if i + 2 < len(phonemes) and phonemes[i + 2] == \"̃\":  # Corrected: No space after tilde\n",
    "                        syllable = phone + next_phone + phonemes[i + 2]  # Combine consonant, vowel, and diacritic\n",
    "                        syllables.append(syllable)\n",
    "                        i += 3  # Skip the diacritic in the next iteration\n",
    "                    else:\n",
    "                        syllable = phone + next_phone\n",
    "                        syllables.append(syllable)\n",
    "                        i += 2\n",
    "                else:\n",
    "                    syllables.append(phone)\n",
    "                    i += 1\n",
    "            else:\n",
    "                syllables.append(phone)\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʒɑ̃tɑ̃\n",
      "syllables:  ['ʒɑ̃', 'tɑ̃']\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def text_to_ipa(text, language=\"fr\"):\n",
    "    \"\"\"\n",
    "    Convert text to IPA using espeak-ng.\n",
    "    \"\"\"\n",
    "    # Remove special characters\n",
    "    text = text.replace(\"?\", \"\").replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\"'\", \"\").replace(\"-\", \" \")\n",
    "\n",
    "    command = [\"espeak-ng\", \"-v\", language, \"-q\", \"--ipa\"]\n",
    "    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate(input=text.encode())\n",
    "    ipa_output = stdout.decode().strip()\n",
    "    ipa_output = ipa_output.replace(\"ˈ\", \"\").replace(\"ˌ\", \"\").replace(\"-\", \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "    return ipa_output\n",
    "\n",
    "def save_ipa_to_file(ipa_text, filename):\n",
    "    \"\"\"\n",
    "    Save IPA text to a file with UTF-8 encoding.\n",
    "    \"\"\"\n",
    "    ipa_text = syllabify_ipa(ipa_text)\n",
    "    print(\"syllables: \", ipa_text)\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\" \".join(ipa_text))\n",
    "\n",
    "# Example usage\n",
    "ipa_text = text_to_ipa(\"j'entends\")\n",
    "print(ipa_text)  # Output: ʒɑ̃tɑ̃\n",
    "save_ipa_to_file(ipa_text, \"output.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old syllables:  bɔ̃\n",
      "new syllables:  bo~\n"
     ]
    }
   ],
   "source": [
    "ipa_to_target = {\n",
    "    # Vowels\n",
    "    \"a\": \"a\", \"ɑ\": \"a\", \"ə\": \"e\", \"ɛ\": \"e\", \"ø\": \"e^\", \"œ\": \"e^\", \"i\": \"i\", \"y\": \"y\", \n",
    "    \"u\": \"u\", \"o\": \"o\", \"ɔ\": \"o^\", \"ɑ̃\": \"a~\", \"ɛ̃\": \"e~\", \"ɔ̃\": \"o~\",\n",
    "\n",
    "    # Consonants\n",
    "    \"b\": \"b\", \"c\": \"k\", \"d\": \"d\", \"f\": \"f\", \"ɡ\": \"g\", \"j\": \"j\", \"k\": \"k\", \"l\": \"l\", \n",
    "    \"m\": \"m\", \"n\": \"n\", \"p\": \"p\", \"s\": \"s\", \"t\": \"t\", \"v\": \"v\", \"w\": \"w\", \"z\": \"z\", \n",
    "    \"ɥ\": \"w\", \"ʁ\": \"r\", \"ʃ\": \"s^\", \"ʒ\": \"z^\", \"ɲ\": \"gn\", \n",
    "}\n",
    "\n",
    "import re\n",
    "\n",
    "def convert_ipa_to_syllables(ipa_text, ipa_to_target):\n",
    "    # Step 1: Convert IPA phonemes to target phonemes\n",
    "    converted_text = []\n",
    "    i = 0\n",
    "    while i < len(ipa_text):\n",
    "        char = ipa_text[i]\n",
    "        # Check if the next character is a combining diacritic\n",
    "        if i + 1 < len(ipa_text) and ipa_text[i + 1] == \"̃\":\n",
    "            # Combine the base character with the diacritic\n",
    "            combined_char = char + ipa_text[i + 1]\n",
    "            # Map the combined character if it exists in the dictionary\n",
    "            mapped_char = ipa_to_target.get(combined_char, combined_char)\n",
    "            converted_text.append(mapped_char)\n",
    "            i += 2  # Skip the diacritic in the next iteration\n",
    "        else:\n",
    "            # Map the single character\n",
    "            mapped_char = ipa_to_target.get(char, char)\n",
    "            converted_text.append(mapped_char)\n",
    "            i += 1\n",
    "    return \"\".join(converted_text)\n",
    "\n",
    "# Example usage\n",
    "ipa_text = \"bɔ̃\"\n",
    "syllables = \" \".join(syllabify_ipa(ipa_text))\n",
    "print(\"old syllables: \", syllables)\n",
    "new_syllables = convert_ipa_to_syllables(syllables, ipa_to_target)\n",
    "print(\"new syllables: \", new_syllables)  # Output: ['ma', 's^e', 'miz', 'e', 'ru', 'si']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Function to convert text to IPA using espeak-ng (modified for multiprocessing)\n",
    "def text_to_ipa_worker(sentence):\n",
    "    \"\"\"\n",
    "    Worker function for multiprocessing to convert text to IPA.\n",
    "    \"\"\"\n",
    "    return text_to_ipa(sentence)\n",
    "\n",
    "# Function to syllabify IPA text (modified for multiprocessing)\n",
    "def syllabify_ipa_worker(ipa_sentence):\n",
    "    \"\"\"\n",
    "    Worker function for multiprocessing to syllabify IPA text.\n",
    "    \"\"\"\n",
    "    syllables = syllabify_ipa(ipa_sentence)\n",
    "    new_syllables = convert_ipa_to_syllables(\" \".join(syllables), ipa_to_target)\n",
    "    return new_syllables\n",
    "\n",
    "# load ipa sentences\n",
    "ipa_path = \"/scratch2/bsow/Documents/ACSR/data/claire_dialogue/ipa_train.txt\"\n",
    "ipa_sentences = []\n",
    "with open(ipa_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        ipa_sentences.append(line.strip())\n",
    "\n",
    "# Use multiprocessing to syllabify IPA sentences\n",
    "with multiprocessing.Pool(8) as pool:\n",
    "    syllabized_ipa_sentences = pool.map(syllabify_ipa_worker, ipa_sentences)\n",
    "\n",
    "# Save the syllabized IPA sentences to a file\n",
    "syllabized_ipa_output_path = \"/scratch2/bsow/Documents/ACSR/data/claire_dialogue/syllabized_ipa_train.txt\"\n",
    "with open(syllabized_ipa_output_path, \"a\", encoding=\"utf-8\") as file:\n",
    "    for syllables in syllabized_ipa_sentences:\n",
    "        file.write(\" \".join(syllables) + \"\\n\")\n",
    "\n",
    "print(f\"Syllabized IPA sentences saved to {syllabized_ipa_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Preprocess the Data\n",
    "class SyllableDataset(Dataset):\n",
    "    def __init__(self, syllabized_ipa_sentences, seq_length=5):\n",
    "        self.syllabized_ipa_sentences = syllabized_ipa_sentences\n",
    "        self.seq_length = seq_length\n",
    "        self.syllables = self._get_syllables()\n",
    "        self.syllable_to_idx = {syllable: i for i, syllable in enumerate(self.syllables)}\n",
    "        self.idx_to_syllable = {i: syllable for syllable, i in self.syllable_to_idx.items()}\n",
    "        self.vocab_size = len(self.syllables)\n",
    "        self.data = self._create_sequences()\n",
    "\n",
    "    def _get_syllables(self):\n",
    "        # Flatten the list of syllabized sentences and count syllables\n",
    "        all_syllables = [syllable for sentence in self.syllabized_ipa_sentences for syllable in sentence]\n",
    "        syllable_counts = Counter(all_syllables)\n",
    "        return sorted(syllable_counts.keys())  # Sort for consistent ordering\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        # Create input-output pairs\n",
    "        sequences = []\n",
    "        for sentence in self.syllabized_ipa_sentences:\n",
    "            for i in range(len(sentence) - self.seq_length):\n",
    "                input_seq = sentence[i:i + self.seq_length]\n",
    "                output_seq = sentence[i + self.seq_length]\n",
    "                sequences.append((input_seq, output_seq))\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, output_seq = self.data[idx]\n",
    "        input_indices = [self.syllable_to_idx[syllable] for syllable in input_seq]\n",
    "        output_index = self.syllable_to_idx[output_seq]\n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(output_index, dtype=torch.long)\n",
    "\n",
    "# Step 2: Build the Model\n",
    "class NextSyllableLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(NextSyllableLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_out[:, -1, :])  # Use the last hidden state\n",
    "        return logits\n",
    "\n",
    "# Step 3: Train the Model\n",
    "def train_model(dataset, model, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    # Split data into training and validation sets\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print training loss\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 4.1513, Validation Loss: 3.8234, Validation Accuracy: 0.2243\n",
      "Epoch [2/100], Train Loss: 3.6539, Validation Loss: 3.6260, Validation Accuracy: 0.2563\n",
      "Epoch [3/100], Train Loss: 3.4418, Validation Loss: 3.5114, Validation Accuracy: 0.2728\n",
      "Epoch [4/100], Train Loss: 3.2739, Validation Loss: 3.4621, Validation Accuracy: 0.2872\n",
      "Epoch [5/100], Train Loss: 3.1221, Validation Loss: 3.4223, Validation Accuracy: 0.2957\n",
      "Epoch [6/100], Train Loss: 2.9802, Validation Loss: 3.4220, Validation Accuracy: 0.2954\n",
      "Epoch [7/100], Train Loss: 2.8470, Validation Loss: 3.4073, Validation Accuracy: 0.3023\n",
      "Epoch [8/100], Train Loss: 2.7193, Validation Loss: 3.4322, Validation Accuracy: 0.2972\n",
      "Epoch [9/100], Train Loss: 2.5957, Validation Loss: 3.4483, Validation Accuracy: 0.3001\n",
      "Epoch [10/100], Train Loss: 2.4755, Validation Loss: 3.4697, Validation Accuracy: 0.3029\n",
      "Epoch [11/100], Train Loss: 2.3610, Validation Loss: 3.5014, Validation Accuracy: 0.3029\n",
      "Epoch [12/100], Train Loss: 2.2494, Validation Loss: 3.5494, Validation Accuracy: 0.2954\n",
      "Epoch [13/100], Train Loss: 2.1415, Validation Loss: 3.5920, Validation Accuracy: 0.2929\n",
      "Epoch [14/100], Train Loss: 2.0362, Validation Loss: 3.6419, Validation Accuracy: 0.2916\n",
      "Epoch [15/100], Train Loss: 1.9364, Validation Loss: 3.7016, Validation Accuracy: 0.2944\n",
      "Epoch [16/100], Train Loss: 1.8404, Validation Loss: 3.7593, Validation Accuracy: 0.2891\n",
      "Epoch [17/100], Train Loss: 1.7454, Validation Loss: 3.8152, Validation Accuracy: 0.2907\n",
      "Epoch [18/100], Train Loss: 1.6534, Validation Loss: 3.8924, Validation Accuracy: 0.2875\n",
      "Epoch [19/100], Train Loss: 1.5654, Validation Loss: 3.9699, Validation Accuracy: 0.2847\n",
      "Epoch [20/100], Train Loss: 1.4831, Validation Loss: 4.0351, Validation Accuracy: 0.2819\n",
      "Epoch [21/100], Train Loss: 1.4033, Validation Loss: 4.1083, Validation Accuracy: 0.2844\n",
      "Epoch [22/100], Train Loss: 1.3260, Validation Loss: 4.2036, Validation Accuracy: 0.2763\n",
      "Epoch [23/100], Train Loss: 1.2503, Validation Loss: 4.2753, Validation Accuracy: 0.2738\n",
      "Epoch [24/100], Train Loss: 1.1824, Validation Loss: 4.3554, Validation Accuracy: 0.2716\n",
      "Epoch [25/100], Train Loss: 1.1157, Validation Loss: 4.4365, Validation Accuracy: 0.2744\n",
      "Epoch [26/100], Train Loss: 1.0506, Validation Loss: 4.5355, Validation Accuracy: 0.2716\n",
      "Epoch [27/100], Train Loss: 0.9935, Validation Loss: 4.6071, Validation Accuracy: 0.2691\n",
      "Epoch [28/100], Train Loss: 0.9370, Validation Loss: 4.6694, Validation Accuracy: 0.2672\n",
      "Epoch [29/100], Train Loss: 0.8833, Validation Loss: 4.7736, Validation Accuracy: 0.2700\n",
      "Epoch [30/100], Train Loss: 0.8306, Validation Loss: 4.8466, Validation Accuracy: 0.2622\n",
      "Epoch [31/100], Train Loss: 0.7828, Validation Loss: 4.9415, Validation Accuracy: 0.2653\n",
      "Epoch [32/100], Train Loss: 0.7355, Validation Loss: 5.0489, Validation Accuracy: 0.2569\n",
      "Epoch [33/100], Train Loss: 0.6954, Validation Loss: 5.1220, Validation Accuracy: 0.2635\n",
      "Epoch [34/100], Train Loss: 0.6521, Validation Loss: 5.2089, Validation Accuracy: 0.2616\n",
      "Epoch [35/100], Train Loss: 0.6174, Validation Loss: 5.3096, Validation Accuracy: 0.2610\n",
      "Epoch [36/100], Train Loss: 0.5816, Validation Loss: 5.3950, Validation Accuracy: 0.2578\n",
      "Epoch [37/100], Train Loss: 0.5483, Validation Loss: 5.4937, Validation Accuracy: 0.2603\n",
      "Epoch [38/100], Train Loss: 0.5196, Validation Loss: 5.5630, Validation Accuracy: 0.2575\n",
      "Epoch [39/100], Train Loss: 0.4893, Validation Loss: 5.6409, Validation Accuracy: 0.2619\n",
      "Epoch [40/100], Train Loss: 0.4668, Validation Loss: 5.7288, Validation Accuracy: 0.2578\n",
      "Epoch [41/100], Train Loss: 0.4432, Validation Loss: 5.8274, Validation Accuracy: 0.2584\n",
      "Epoch [42/100], Train Loss: 0.4208, Validation Loss: 5.9241, Validation Accuracy: 0.2541\n",
      "Epoch [43/100], Train Loss: 0.4013, Validation Loss: 5.9985, Validation Accuracy: 0.2600\n",
      "Epoch [44/100], Train Loss: 0.3804, Validation Loss: 6.0952, Validation Accuracy: 0.2528\n",
      "Epoch [45/100], Train Loss: 0.3668, Validation Loss: 6.1358, Validation Accuracy: 0.2572\n",
      "Epoch [46/100], Train Loss: 0.3536, Validation Loss: 6.2295, Validation Accuracy: 0.2544\n",
      "Epoch [47/100], Train Loss: 0.3398, Validation Loss: 6.3024, Validation Accuracy: 0.2528\n",
      "Epoch [48/100], Train Loss: 0.3212, Validation Loss: 6.3696, Validation Accuracy: 0.2544\n",
      "Epoch [49/100], Train Loss: 0.3093, Validation Loss: 6.4320, Validation Accuracy: 0.2541\n",
      "Epoch [50/100], Train Loss: 0.3078, Validation Loss: 6.5344, Validation Accuracy: 0.2475\n",
      "Epoch [51/100], Train Loss: 0.2977, Validation Loss: 6.5786, Validation Accuracy: 0.2559\n",
      "Epoch [52/100], Train Loss: 0.2857, Validation Loss: 6.6475, Validation Accuracy: 0.2544\n",
      "Epoch [53/100], Train Loss: 0.2790, Validation Loss: 6.7223, Validation Accuracy: 0.2569\n",
      "Epoch [54/100], Train Loss: 0.2718, Validation Loss: 6.8064, Validation Accuracy: 0.2509\n",
      "Epoch [55/100], Train Loss: 0.2651, Validation Loss: 6.8108, Validation Accuracy: 0.2563\n",
      "Epoch [56/100], Train Loss: 0.2568, Validation Loss: 6.9090, Validation Accuracy: 0.2584\n",
      "Epoch [57/100], Train Loss: 0.2588, Validation Loss: 6.9604, Validation Accuracy: 0.2541\n",
      "Epoch [58/100], Train Loss: 0.2478, Validation Loss: 6.9921, Validation Accuracy: 0.2550\n",
      "Epoch [59/100], Train Loss: 0.2447, Validation Loss: 7.0684, Validation Accuracy: 0.2541\n",
      "Epoch [60/100], Train Loss: 0.2443, Validation Loss: 7.0828, Validation Accuracy: 0.2484\n",
      "Epoch [61/100], Train Loss: 0.2385, Validation Loss: 7.1463, Validation Accuracy: 0.2516\n",
      "Epoch [62/100], Train Loss: 0.2320, Validation Loss: 7.1956, Validation Accuracy: 0.2550\n",
      "Epoch [63/100], Train Loss: 0.2313, Validation Loss: 7.2547, Validation Accuracy: 0.2541\n",
      "Epoch [64/100], Train Loss: 0.2267, Validation Loss: 7.3291, Validation Accuracy: 0.2500\n",
      "Epoch [65/100], Train Loss: 0.2284, Validation Loss: 7.3677, Validation Accuracy: 0.2525\n",
      "Epoch [66/100], Train Loss: 0.2156, Validation Loss: 7.3637, Validation Accuracy: 0.2487\n",
      "Epoch [67/100], Train Loss: 0.2190, Validation Loss: 7.4313, Validation Accuracy: 0.2519\n",
      "Epoch [68/100], Train Loss: 0.2182, Validation Loss: 7.4824, Validation Accuracy: 0.2559\n",
      "Epoch [69/100], Train Loss: 0.2149, Validation Loss: 7.4930, Validation Accuracy: 0.2541\n",
      "Epoch [70/100], Train Loss: 0.2087, Validation Loss: 7.5773, Validation Accuracy: 0.2528\n",
      "Epoch [71/100], Train Loss: 0.2226, Validation Loss: 7.6052, Validation Accuracy: 0.2472\n",
      "Epoch [72/100], Train Loss: 0.2076, Validation Loss: 7.6112, Validation Accuracy: 0.2538\n",
      "Epoch [73/100], Train Loss: 0.2047, Validation Loss: 7.6140, Validation Accuracy: 0.2522\n",
      "Epoch [74/100], Train Loss: 0.2008, Validation Loss: 7.6757, Validation Accuracy: 0.2506\n",
      "Epoch [75/100], Train Loss: 0.2088, Validation Loss: 7.6702, Validation Accuracy: 0.2538\n",
      "Epoch [76/100], Train Loss: 0.2064, Validation Loss: 7.7009, Validation Accuracy: 0.2563\n",
      "Epoch [77/100], Train Loss: 0.2003, Validation Loss: 7.7932, Validation Accuracy: 0.2447\n",
      "Epoch [78/100], Train Loss: 0.2005, Validation Loss: 7.7990, Validation Accuracy: 0.2503\n",
      "Epoch [79/100], Train Loss: 0.2019, Validation Loss: 7.8148, Validation Accuracy: 0.2556\n",
      "Epoch [80/100], Train Loss: 0.1965, Validation Loss: 7.8587, Validation Accuracy: 0.2544\n",
      "Epoch [81/100], Train Loss: 0.2003, Validation Loss: 7.8694, Validation Accuracy: 0.2509\n",
      "Epoch [82/100], Train Loss: 0.1976, Validation Loss: 7.8865, Validation Accuracy: 0.2522\n",
      "Epoch [83/100], Train Loss: 0.1933, Validation Loss: 7.9320, Validation Accuracy: 0.2509\n",
      "Epoch [84/100], Train Loss: 0.1940, Validation Loss: 7.9842, Validation Accuracy: 0.2469\n",
      "Epoch [85/100], Train Loss: 0.1935, Validation Loss: 8.0037, Validation Accuracy: 0.2522\n",
      "Epoch [86/100], Train Loss: 0.1895, Validation Loss: 8.0158, Validation Accuracy: 0.2519\n",
      "Epoch [87/100], Train Loss: 0.1899, Validation Loss: 8.0675, Validation Accuracy: 0.2494\n",
      "Epoch [88/100], Train Loss: 0.1894, Validation Loss: 8.0812, Validation Accuracy: 0.2494\n",
      "Epoch [89/100], Train Loss: 0.1881, Validation Loss: 8.1290, Validation Accuracy: 0.2503\n",
      "Epoch [90/100], Train Loss: 0.1850, Validation Loss: 8.1169, Validation Accuracy: 0.2550\n",
      "Epoch [91/100], Train Loss: 0.1868, Validation Loss: 8.1513, Validation Accuracy: 0.2506\n",
      "Epoch [92/100], Train Loss: 0.1949, Validation Loss: 8.1347, Validation Accuracy: 0.2472\n",
      "Epoch [93/100], Train Loss: 0.1828, Validation Loss: 8.1728, Validation Accuracy: 0.2425\n",
      "Epoch [94/100], Train Loss: 0.1903, Validation Loss: 8.2115, Validation Accuracy: 0.2516\n",
      "Epoch [95/100], Train Loss: 0.1799, Validation Loss: 8.2853, Validation Accuracy: 0.2525\n",
      "Epoch [96/100], Train Loss: 0.1848, Validation Loss: 8.2720, Validation Accuracy: 0.2497\n",
      "Epoch [97/100], Train Loss: 0.1766, Validation Loss: 8.2778, Validation Accuracy: 0.2484\n",
      "Epoch [98/100], Train Loss: 0.1836, Validation Loss: 8.2933, Validation Accuracy: 0.2500\n",
      "Epoch [99/100], Train Loss: 0.1871, Validation Loss: 8.3390, Validation Accuracy: 0.2484\n",
      "Epoch [100/100], Train Loss: 0.1797, Validation Loss: 8.2886, Validation Accuracy: 0.2478\n",
      "Model saved to next_syllable_lstm.pth\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load syllabized IPA sentences\n",
    "    syllabized_ipa_sentences = []\n",
    "    with open(\"/scratch2/bsow/Documents/ACSR/data/claire_dialogue/syllabized_ipa_train.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            syllabized_ipa_sentences.append(line.strip().split())\n",
    "        \n",
    "    # Create dataset\n",
    "    dataset = SyllableDataset(syllabized_ipa_sentences, seq_length=5)\n",
    "\n",
    "    # Initialize model\n",
    "    model = NextSyllableLSTM(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=128,\n",
    "        num_layers=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_model(dataset, model, epochs=100, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"next_syllable_lstm.pth\")\n",
    "    print(\"Model saved to next_syllable_lstm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 3.9495, Validation Loss: 3.7184, Validation Accuracy: 0.2355\n",
      "Epoch [2/100], Train Loss: 3.5261, Validation Loss: 3.5341, Validation Accuracy: 0.2653\n",
      "Epoch [3/100], Train Loss: 3.3096, Validation Loss: 3.4296, Validation Accuracy: 0.2786\n",
      "Epoch [4/100], Train Loss: 3.1389, Validation Loss: 3.3811, Validation Accuracy: 0.2955\n",
      "Epoch [5/100], Train Loss: 2.9917, Validation Loss: 3.3627, Validation Accuracy: 0.2972\n",
      "Epoch [6/100], Train Loss: 2.8550, Validation Loss: 3.3741, Validation Accuracy: 0.2995\n",
      "Epoch [7/100], Train Loss: 2.7289, Validation Loss: 3.3950, Validation Accuracy: 0.2992\n",
      "Epoch [8/100], Train Loss: 2.6044, Validation Loss: 3.4274, Validation Accuracy: 0.3008\n",
      "Epoch [9/100], Train Loss: 2.4852, Validation Loss: 3.4880, Validation Accuracy: 0.2972\n",
      "Epoch [10/100], Train Loss: 2.3679, Validation Loss: 3.5473, Validation Accuracy: 0.2947\n",
      "Epoch [11/100], Train Loss: 2.2556, Validation Loss: 3.6062, Validation Accuracy: 0.2957\n",
      "Epoch [12/100], Train Loss: 2.1493, Validation Loss: 3.6896, Validation Accuracy: 0.2900\n",
      "Epoch [13/100], Train Loss: 2.0438, Validation Loss: 3.7820, Validation Accuracy: 0.2838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m NextSyllableLSTM(\n\u001b[1;32m     14\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m     15\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     16\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     17\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_syllable_lstm.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 79\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset, model, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     78\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 79\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Print training loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 432\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4: Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load syllabized IPA sentences\n",
    "    syllabized_ipa_sentences = []\n",
    "    with open(\"/scratch2/bsow/Documents/ACSR/data/claire_dialogue/syllabized_ipa_train.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            syllabized_ipa_sentences.append(line.strip().split())\n",
    "        \n",
    "    # Create dataset\n",
    "    dataset = SyllableDataset(syllabized_ipa_sentences, seq_length=5)\n",
    "\n",
    "    # Initialize model\n",
    "    model = NextSyllableLSTM(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_model(dataset, model, epochs=100, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"next_syllable_lstm.pth\")\n",
    "    print(\"Model saved to next_syllable_lstm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
