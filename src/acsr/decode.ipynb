{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# cued_speech_sentences_1\n",
    "cued_speech_sentences_1 = [\n",
    "    \"al bɛr a u b li e la ra di o kə sa mɛ ra vɛ kɔ̃ s t r ɥ i t\",\n",
    "    \"le zɔ mə zɔ̃ fa i tɔ̃ be dɑ̃ lə t ru ki la vɛ k rø ze\",\n",
    "    \"le zɔ l da pa s tu ʒu r  pa r lə ʃə mɛ ki lɔ̃ k rø ze\",\n",
    "    \"e mi li la vɛ de te s te dɛ lə mɔ mɑ̃  u ɛl sɛ tɛ̃ rɔ̃ kɔ̃ t re\",\n",
    "    \"le po li si e na vɛ pã dɥ tu kɔ̃ p ri kɛ l sɛ tɛ̃ rɔ̃ kɔ̃ t re\",\n",
    "    \"i l a p re si la r i ʃɛ s ka r i l a g rã di dɑ̃ la po v re te\",\n",
    "    \"mo p ro fɛ s œr ma di ki la vɛ t u ʒu rə pœ r d lə po v re te\",\n",
    "    \"ɔ kɔ̃ sɛ r de ro k la my zi k e tɛ t ro fɔ r t\",\n",
    "    \"a vɛ k lə p wa sɔ̃ la so s ne te pa t rɛ fɔ r t\",\n",
    "    \"pu r f e t e s ɔ̃ d e pa r t sə z a mi ljɔ̃ ɔ̃ r ga ni z e y n e f e t\",\n",
    "    \"də l e ʃã brə l e z ã fã tɑ̃ d œ̃ b j ɛ̃ l e b ri də l e f e t\",\n",
    "    \"k ri di k ə s ɔ̃ ʃ i ɛ̃ av ɛ t u ʒu re t ɛ s ɔ̃ m ɛ j œ̃ r\",\n",
    "    \"ʒy li ai ta də no vo pa r ti ɑ̃ va kã s a v ɛ k s ɔ̃ m ɛ j\",\n",
    "    \"pu r ɑ̃ t re d ã lə ma z ɔ̃ l ɛ̃ s pɛ k t œ̃ r a dy mõ r s ɔ̃ b a d ʒ\"\n",
    "]\n",
    "\n",
    "# cued_speech_sentences_2\n",
    "cued_speech_sentences_2 = [\n",
    "    \"pu r a v wa r pa r ti si pe a la k ʊ r s lɑ̃ fɑ̃ a rə sy ỹ b a dʒ\",\n",
    "    \"ɡa s pa r de t ɛ p ɛr dy e nə sa v ɛ pa kwa fɛr\",\n",
    "    \"le o e t ɛ ʃ ɔ ke də vã s ɛ t ʃ o z e t r ɑ̃ ʒ a fɛr\",\n",
    "    \"la m ɛ r ɛ̃ k ɥ i ɛ t av ɛ p l ɥ z j œ r bu ʃ a nu rir\",\n",
    "    \"mõ f r ɛr po sɛ d ɛ de z a ni mo di fi si l a nu rir\",\n",
    "    \"b ɛr nar l a v ɛ a k k ɥ i ji a v ɛ k œ̃ te a la mɑ̃ t\",\n",
    "    \"i za b ɛl e t ɛ s ɔr ti k ɥ i ji r ɛ̃ bu ke də mɑ̃ t\",\n",
    "    \"la ku l œ r la p l ɥ s a so si e a la ko l ɛ r e lə ru ʒ\",\n",
    "    \"sa nu v ɛ l ʃɑ̃ br a ɛ̃ m yr ɑ̃ tjɛr mə p ɛ̃ t ɑ̃ ru ʒ\",\n",
    "    \"la pə ti fi j e t ɛ k ɔ̃ t ɑ̃ t də p ɛr drə sa p rə mj ɛr dɑ̃\",\n",
    "    \"lə po v r ɔ̃ m sɛ̃ k ɥ i ɛ t ɛ tu ʒu r də p ɛr drə y nə nu v ɛ l dɑ̃\",\n",
    "    \"d ɛ sa ma ʒɔ ri te i l sɛ t ɛ ɑ̃ ɡ a ʒe dɑ̃ l a r me\",\n",
    "    \"də p ɥ i t r wɑ̃ zɑ̃ ɛ l t r a va j ɛ dɑ̃ l a r me\",\n",
    "    \"lə ʒy də si t r ɔ̃ ɛ t r ɛ a sid\",\n",
    "    \"sə p la ɛ bo ku t r o a sid\",\n",
    "    \"ɛ l a sy ʒe ʁe a s ɔ̃ pə ti a mi baʁ by də sə r ɑ se\"\n",
    "]\n",
    "\n",
    "# cued_speech_sentences_3\n",
    "cued_speech_sentences_3 = [\n",
    "    \"i la di a s ɔ̃ f r ɛr ki l də v ɛ v r ɛ mɑ̃ sə ra ze\",\n",
    "    \"ɑ̃ sa p r ɔ ʃɑ̃ d y n r y ʃɛ l a e te pi ke pa r y n a b ɛ l\",\n",
    "    \"ɑ̃ r ə ɡa r dɑ̃ s ɔ̃ li v r i l a də vi ne l ɛ s p ɛ s də la b ɛ l\",\n",
    "    \"a p r ɛ y n ba la də dɑ̃ le ʃɑ̃ m ɛ b ɔ tə e te k u v ɛ r də bu\",\n",
    "    \"a p r ɛ a v wa r e te la v e lə k ɔ l n ɛ te p l y ta ʃe də bu\",\n",
    "    \"la n ɔ̃ s də s ɔ l d t r ɛ p ɔr tɑ̃ t a a ti re y n ɛ mɑ̃ s f ul\",\n",
    "    \"la bu tik də a k sɛ swa ɛ te dɑ̃ ɛ k a r tje e lwa nje də la f ul\",\n",
    "    \"la fi lj ɛ tə a r ə sy sə ka do pu r s ɔ̃ a ni v ɛr sɛr\",\n",
    "    \"ɛ l a a ʃə te ɛ̃ po də fl œ r pu r s ɔ̃ a ni v ɛr sɛr\",\n",
    "    \"ɑ̃ f ɛ lə si dr a v ɛ k lə ʒy də p ɔ m\",\n",
    "    \"s ɔ̃ a mi a tu ʒu r a d ɔ re mɑ̃ ʒe də p ɔ m\",\n",
    "    \"l ɛ n ɔr mə va l i s sɛ̃ bl ɛ pə ze y n t ɔ̃\",\n",
    "    \"s ɛ t ɛ s pas p ø ɑ̃ sto ke r y n t ɔ̃\",\n",
    "    \"i lɥ i a mi ɛ̃ k u to su la ɡ ɔr j\",\n",
    "    \"ɛ l a tu ʒu r ø mal a la ɡ ɔr j\",\n",
    "    \"a p r ɛ sa pa n y n pa s ɑ̃ la e də a pu se sa v wa ti r\",\n",
    "    \"k ɔm sɛ v wa z ɛ̃ ɛ l a ɛl m ɛ m r ə p ɛ̃ sa v wa ti r\",\n",
    "    \"pu r a r i ve a tɑ̃ z i l ɔ rɛ dy ma r ʃe p l y vit\",\n",
    "    \"pu r nə pa v wa r f ɛ̃ z i l ɔ rɛ dy mɑ̃ ʒe p l y vit\"\n",
    "]\n",
    "\n",
    "all_sentences = cued_speech_sentences_1 + cued_speech_sentences_2 + cued_speech_sentences_3\n",
    "\n",
    "directory = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\stimuli\\sentences\\txt'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i, sentence in enumerate(all_sentences, start=2):\n",
    "    filename = f\"sent_{i:02}.lpc\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load CSV files from a directory based on a filename pattern\n",
    "def load_csv_files(directory, filename_pattern, type=\"position\"):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            df.fillna(-1, inplace=True)\n",
    "            base_name = filename.split(f'_{type}_')[1].split('.mp4.csv')[0]\n",
    "            files_data[base_name] = df\n",
    "    return files_data\n",
    "\n",
    "# Directories\n",
    "data_dir = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\output_new'\n",
    "phoneme_dir = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\stimuli\\sentences\\txt'\n",
    "\n",
    "# Load position and shape data\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "hand_shape_data = load_csv_files(data_dir, 'predictions_rf_shape', type='shape')\n",
    "\n",
    "\n",
    "# Find corresponding phoneme files based on the base names of position filenames\n",
    "def find_phoneme_files(directory, base_names):\n",
    "    phoneme_files = {}\n",
    "    for base_name in base_names:\n",
    "        phoneme_file = os.path.join(directory, f'{base_name}.lpc')\n",
    "        if os.path.exists(phoneme_file):\n",
    "            phoneme_files[base_name] = phoneme_file\n",
    "    return phoneme_files\n",
    "\n",
    "# Find phoneme files\n",
    "base_names = hand_position_data.keys()\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sliding window for each data frame\n",
    "def create_sliding_window(data, window_size):\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "    return np.array(X)\n",
    "\n",
    "# Function to pad sequences to the maximum length\n",
    "def pad_sequences(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.zeros((max_length - len(seq), seq.shape[1]))\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# Extract and concatenate probabilities from multiple dataframes\n",
    "def extract_probabilities(data, columns):\n",
    "    data = [df.fillna(-1) for df in data]\n",
    "    probs_list = [df[columns].to_numpy() for df in data]\n",
    "    return np.concatenate(probs_list, axis=0)\n",
    "\n",
    "# Prepare data for all videos\n",
    "def prepare_data_for_videos_no_sliding_windows(hand_position_data, hand_shape_data, phoneme_files):\n",
    "    all_videos_data = {}\n",
    "    for base_name in hand_position_data:\n",
    "        if base_name in phoneme_files:\n",
    "            position_df = hand_position_data[base_name]\n",
    "            shape_df = hand_shape_data[base_name]\n",
    "            phoneme_file = phoneme_files[base_name]\n",
    "\n",
    "            # Extract probabilities\n",
    "            hand_position_probs = extract_probabilities([position_df], ['p_class_1', 'p_class_2', 'p_class_3', 'p_class_4', 'p_class_5'])\n",
    "            hand_shape_probs = extract_probabilities([shape_df], ['p_class_1', 'p_class_2', 'p_class_3', 'p_class_4', 'p_class_5', 'p_class_6', 'p_class_7', 'p_class_8'])\n",
    "            combined_probs = np.concatenate((hand_position_probs, hand_shape_probs), axis=1)\n",
    "\n",
    "            # Read phoneme sequences\n",
    "            with open(phoneme_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.read().splitlines()\n",
    "                phoneme_sequence = [phoneme for line in lines for phoneme in line.split()]\n",
    "\n",
    "            # Convert phoneme sequence to indices\n",
    "            phoneme_indices = [phoneme_to_index[phoneme] for phoneme in phoneme_sequence]\n",
    "            all_videos_data[base_name] = {'X': combined_probs, 'y': phoneme_indices}\n",
    "    return all_videos_data\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "hand_shape_data = load_csv_files(data_dir, 'predictions_rf_shape', type='shape')\n",
    "base_names = hand_position_data.keys()\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n",
    "# take only the first 5 phoneme files for demonstration\n",
    "hand_position_data = {key: hand_position_data[key] for key in list(hand_position_data.keys())[:5]}\n",
    "hand_shape_data = {key: hand_shape_data[key] for key in list(hand_shape_data.keys())[:5]}\n",
    "phoneme_files = {key: phoneme_files[key] for key in list(phoneme_files.keys())[:5]}\n",
    "\n",
    "# Function to apply phonotactic rules\n",
    "def apply_phonotactic_rules(combinations):\n",
    "    valid_combinations = []\n",
    "    for combination in combinations:\n",
    "        # Example phonotactic rules:\n",
    "        # 1. No consecutive vowels (e.g., \"ae\")\n",
    "        # 2. Certain consonant clusters are invalid (e.g., \"tl\")\n",
    "        if re.search(r'[aeiouy]{2}', combination):\n",
    "            continue  # Skip invalid combinations with consecutive vowels\n",
    "        if re.search(r'([s])\\1', combination):  # Skip invalid combinations with double consonants\n",
    "            continue\n",
    "        valid_combinations.append(combination)\n",
    "    return valid_combinations\n",
    "\n",
    "# Phoneme Mapping\n",
    "consonants = ['b', 'ch', 'd', 'f', 'g', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 'sh', 't', 'v', 'w', 'y', 'z']\n",
    "vowels = ['a', 'e', 'ɛ', 'i', 'o', 'ɔ', 'u', 'ø', 'œ']\n",
    "phoneme_combinations = consonants + vowels + [c + v for c in consonants for v in vowels] + [v + c for v in vowels for c in consonants]\n",
    "\n",
    "all_phonemes = []\n",
    "for sentence in all_sentences:  # Assuming all_sentences is a list of sentences already defined\n",
    "    all_phonemes.extend(sentence.split())\n",
    "\n",
    "phoneme_combinations += set(all_phonemes)\n",
    "valid_phoneme_combinations = set(apply_phonotactic_rules(phoneme_combinations))\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(valid_phoneme_combinations)}\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "\n",
    "phoneme_to_index[' '] = len(phoneme_to_index)\n",
    "index_to_phoneme[len(index_to_phoneme)] = ' '\n",
    "\n",
    "\n",
    "# Function to pad sequences to the maximum length\n",
    "def pad_sequences(sequences, max_length, pad_value=-1):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.full((max_length - len(seq), seq.shape[1]), pad_value)\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# Combine sequences with padding\n",
    "def combine_sequences_with_padding(video_data):\n",
    "    max_length = max(len(video_data[video]['X']) for video in video_data)\n",
    "    X_padded = [pad_sequences([video_data[video]['X']], max_length)[0] for video in video_data]\n",
    "    y_padded = [video_data[video]['y'] + [phoneme_to_index[' ']] * (max_length - len(video_data[video]['y'])) for video in video_data]\n",
    "    return X_padded, y_padded\n",
    "\n",
    "\n",
    "# Prepare data for all videos\n",
    "all_videos_data = prepare_data_for_videos_no_sliding_windows(hand_position_data, hand_shape_data, phoneme_files)\n",
    "X_combined, y_combined = combine_sequences_with_padding(all_videos_data)\n",
    "\n",
    "# convert phoneme sequences to indices\n",
    "y_tensors = [torch.tensor([index for index in video_data['y']], dtype=torch.long) for video_data in all_videos_data.values()]\n",
    "all_videos_data = {key: {'X': video_data['X'], 'y': y_tensors[i]} for i, (key, video_data) in enumerate(all_videos_data.items())}\n",
    "\n",
    "# Combine all data\n",
    "X_combined = torch.tensor(X_combined, dtype=torch.float32)\n",
    "y_combined = torch.tensor(y_combined, dtype=torch.long)\n",
    "\n",
    "all_videos_data = {'X': X_combined, 'y': y_combined}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in index_to_phoneme.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into training and validation sets\n",
    "def train_val_split(data, train_ratio=1):\n",
    "    video_names = list(data['X'])\n",
    "    split_idx = int(len(video_names) * train_ratio)\n",
    "    train_data = {'X': data['X'][:split_idx], 'y': data['y'][:split_idx]}\n",
    "    val_data = {'X': data['X'][0:], 'y': data['y'][0:]}\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_val_split(all_videos_data)\n",
    "\n",
    "# Convert data to DataLoader format\n",
    "def data_to_dataloader(data):\n",
    "    X_tensors = data['X']\n",
    "    y_tensors = data['y']\n",
    "    \n",
    "    X_dataset = TensorDataset(X_tensors)\n",
    "    y_dataset = y_tensors  # Phoneme sequences are kept separately for each video\n",
    "    \n",
    "    return DataLoader(X_dataset, batch_size=1, shuffle=True), y_dataset\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader, y_train_tensors = data_to_dataloader(train_data)\n",
    "val_loader, y_val_tensors = data_to_dataloader(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/840, Loss: 319.0329528808594\n",
      "Epoch 2/840, Loss: 264.2211486816406\n",
      "Epoch 3/840, Loss: 145.17344665527344\n",
      "Epoch 4/840, Loss: 65.03530578613281\n",
      "Epoch 5/840, Loss: 28.707087326049805\n",
      "Epoch 6/840, Loss: 13.7190673828125\n",
      "Epoch 7/840, Loss: 8.398638057708741\n",
      "Epoch 8/840, Loss: 6.510363388061523\n",
      "Epoch 9/840, Loss: 5.771200275421142\n",
      "Epoch 10/840, Loss: 5.430020904541015\n",
      "Epoch 11/840, Loss: 5.2350200653076175\n",
      "Epoch 12/840, Loss: 5.097847652435303\n",
      "Epoch 13/840, Loss: 4.985731506347657\n",
      "Epoch 14/840, Loss: 4.884995555877685\n",
      "Epoch 15/840, Loss: 4.796831035614014\n",
      "Epoch 16/840, Loss: 4.721562004089355\n",
      "Epoch 17/840, Loss: 4.656914806365966\n",
      "Epoch 18/840, Loss: 4.601824855804443\n",
      "Epoch 19/840, Loss: 4.554894638061524\n",
      "Epoch 20/840, Loss: 4.5148530960083\n",
      "Epoch 21/840, Loss: 4.480357456207275\n",
      "Epoch 22/840, Loss: 4.450225448608398\n",
      "Epoch 23/840, Loss: 4.423550033569336\n",
      "Epoch 24/840, Loss: 4.399747467041015\n",
      "Epoch 25/840, Loss: 4.378297901153564\n",
      "Epoch 26/840, Loss: 4.358916759490967\n",
      "Epoch 27/840, Loss: 4.341190528869629\n",
      "Epoch 28/840, Loss: 4.324957656860351\n",
      "Epoch 29/840, Loss: 4.3100425720214846\n",
      "Epoch 30/840, Loss: 4.296333694458008\n",
      "Epoch 31/840, Loss: 4.283584785461426\n",
      "Epoch 32/840, Loss: 4.271703147888184\n",
      "Epoch 33/840, Loss: 4.26064977645874\n",
      "Epoch 34/840, Loss: 4.250305652618408\n",
      "Epoch 35/840, Loss: 4.2406535148620605\n",
      "Epoch 36/840, Loss: 4.231527328491211\n",
      "Epoch 37/840, Loss: 4.222996807098388\n",
      "Epoch 38/840, Loss: 4.214950084686279\n",
      "Epoch 39/840, Loss: 4.207308673858643\n",
      "Epoch 40/840, Loss: 4.200109672546387\n",
      "Epoch 41/840, Loss: 4.193360042572022\n",
      "Epoch 42/840, Loss: 4.1869120597839355\n",
      "Epoch 43/840, Loss: 4.180795097351075\n",
      "Epoch 44/840, Loss: 4.174994850158692\n",
      "Epoch 45/840, Loss: 4.169507789611816\n",
      "Epoch 46/840, Loss: 4.164255046844483\n",
      "Epoch 47/840, Loss: 4.1592559814453125\n",
      "Epoch 48/840, Loss: 4.154500961303711\n",
      "Epoch 49/840, Loss: 4.149975395202636\n",
      "Epoch 50/840, Loss: 4.145604610443115\n",
      "Epoch 51/840, Loss: 4.141461658477783\n",
      "Epoch 52/840, Loss: 4.1375151634216305\n",
      "Epoch 53/840, Loss: 4.133652210235596\n",
      "Epoch 54/840, Loss: 4.130020999908448\n",
      "Epoch 55/840, Loss: 4.126539516448974\n",
      "Epoch 56/840, Loss: 4.123168659210205\n",
      "Epoch 57/840, Loss: 4.119973373413086\n",
      "Epoch 58/840, Loss: 4.116871356964111\n",
      "Epoch 59/840, Loss: 4.113869762420654\n",
      "Epoch 60/840, Loss: 4.1110118389129635\n",
      "Epoch 61/840, Loss: 4.108246564865112\n",
      "Epoch 62/840, Loss: 4.105619192123413\n",
      "Epoch 63/840, Loss: 4.103028106689453\n",
      "Epoch 64/840, Loss: 4.100575304031372\n",
      "Epoch 65/840, Loss: 4.098183345794678\n",
      "Epoch 66/840, Loss: 4.0959004878997805\n",
      "Epoch 67/840, Loss: 4.093682479858399\n",
      "Epoch 68/840, Loss: 4.091523551940918\n",
      "Epoch 69/840, Loss: 4.089430904388427\n",
      "Epoch 70/840, Loss: 4.087429428100586\n",
      "Epoch 71/840, Loss: 4.085487937927246\n",
      "Epoch 72/840, Loss: 4.0835973739624025\n",
      "Epoch 73/840, Loss: 4.081776666641235\n",
      "Epoch 74/840, Loss: 4.0800353527069095\n",
      "Epoch 75/840, Loss: 4.078331756591797\n",
      "Epoch 76/840, Loss: 4.076638126373291\n",
      "Epoch 77/840, Loss: 4.0750086307525635\n",
      "Epoch 78/840, Loss: 4.073449373245239\n",
      "Epoch 79/840, Loss: 4.071988534927368\n",
      "Epoch 80/840, Loss: 4.070456171035767\n",
      "Epoch 81/840, Loss: 4.069105339050293\n",
      "Epoch 82/840, Loss: 4.067629718780518\n",
      "Epoch 83/840, Loss: 4.066344547271728\n",
      "Epoch 84/840, Loss: 4.065025711059571\n",
      "Epoch 85/840, Loss: 4.063744401931762\n",
      "Epoch 86/840, Loss: 4.062477874755859\n",
      "Epoch 87/840, Loss: 4.061288261413575\n",
      "Epoch 88/840, Loss: 4.060090589523315\n",
      "Epoch 89/840, Loss: 4.058945798873902\n",
      "Epoch 90/840, Loss: 4.05784502029419\n",
      "Epoch 91/840, Loss: 4.056752347946167\n",
      "Epoch 92/840, Loss: 4.055713653564453\n",
      "Epoch 93/840, Loss: 4.0546777725219725\n",
      "Epoch 94/840, Loss: 4.053666591644287\n",
      "Epoch 95/840, Loss: 4.052691411972046\n",
      "Epoch 96/840, Loss: 4.051727771759033\n",
      "Epoch 97/840, Loss: 4.0507838249206545\n",
      "Epoch 98/840, Loss: 4.049870204925537\n",
      "Epoch 99/840, Loss: 4.048987007141113\n",
      "Epoch 100/840, Loss: 4.0481276512146\n",
      "Epoch 101/840, Loss: 4.047277593612671\n",
      "Epoch 102/840, Loss: 4.046449422836304\n",
      "Epoch 103/840, Loss: 4.0456431865692135\n",
      "Epoch 104/840, Loss: 4.044847297668457\n",
      "Epoch 105/840, Loss: 4.044082498550415\n",
      "Epoch 106/840, Loss: 4.043300628662109\n",
      "Epoch 107/840, Loss: 4.042563724517822\n",
      "Epoch 108/840, Loss: 4.041833543777466\n",
      "Epoch 109/840, Loss: 4.041134214401245\n",
      "Epoch 110/840, Loss: 4.040443468093872\n",
      "Epoch 111/840, Loss: 4.039758396148682\n",
      "Epoch 112/840, Loss: 4.03913311958313\n",
      "Epoch 113/840, Loss: 4.038463449478149\n",
      "Epoch 114/840, Loss: 4.037812757492065\n",
      "Epoch 115/840, Loss: 4.03719129562378\n",
      "Epoch 116/840, Loss: 4.03659954071045\n",
      "Epoch 117/840, Loss: 4.036011266708374\n",
      "Epoch 118/840, Loss: 4.035428762435913\n",
      "Epoch 119/840, Loss: 4.034850311279297\n",
      "Epoch 120/840, Loss: 4.034288311004639\n",
      "Epoch 121/840, Loss: 4.033733558654785\n",
      "Epoch 122/840, Loss: 4.033178472518921\n",
      "Epoch 123/840, Loss: 4.032659101486206\n",
      "Epoch 124/840, Loss: 4.0321348190307615\n",
      "Epoch 125/840, Loss: 4.031641817092895\n",
      "Epoch 126/840, Loss: 4.031116962432861\n",
      "Epoch 127/840, Loss: 4.030646133422851\n",
      "Epoch 128/840, Loss: 4.030140352249146\n",
      "Epoch 129/840, Loss: 4.029659080505371\n",
      "Epoch 130/840, Loss: 4.029245615005493\n",
      "Epoch 131/840, Loss: 4.028777837753296\n",
      "Epoch 132/840, Loss: 4.028294610977173\n",
      "Epoch 133/840, Loss: 4.0278595924377445\n",
      "Epoch 134/840, Loss: 4.027458429336548\n",
      "Epoch 135/840, Loss: 4.027036094665528\n",
      "Epoch 136/840, Loss: 4.0266028881073\n",
      "Epoch 137/840, Loss: 4.02617883682251\n",
      "Epoch 138/840, Loss: 4.02581787109375\n",
      "Epoch 139/840, Loss: 4.025413656234742\n",
      "Epoch 140/840, Loss: 4.025015783309937\n",
      "Epoch 141/840, Loss: 4.024637746810913\n",
      "Epoch 142/840, Loss: 4.024266862869263\n",
      "Epoch 143/840, Loss: 4.0238855361938475\n",
      "Epoch 144/840, Loss: 4.0235456943511965\n",
      "Epoch 145/840, Loss: 4.0231842517852785\n",
      "Epoch 146/840, Loss: 4.022813224792481\n",
      "Epoch 147/840, Loss: 4.0224895000457765\n",
      "Epoch 148/840, Loss: 4.022111320495606\n",
      "Epoch 149/840, Loss: 4.021787500381469\n",
      "Epoch 150/840, Loss: 4.0214780330657955\n",
      "Epoch 151/840, Loss: 4.02113618850708\n",
      "Epoch 152/840, Loss: 4.020841407775879\n",
      "Epoch 153/840, Loss: 4.020514249801636\n",
      "Epoch 154/840, Loss: 4.020223474502563\n",
      "Epoch 155/840, Loss: 4.019900846481323\n",
      "Epoch 156/840, Loss: 4.019654273986816\n",
      "Epoch 157/840, Loss: 4.019305610656739\n",
      "Epoch 158/840, Loss: 4.019063806533813\n",
      "Epoch 159/840, Loss: 4.0187458992004395\n",
      "Epoch 160/840, Loss: 4.018486166000367\n",
      "Epoch 161/840, Loss: 4.018180084228516\n",
      "Epoch 162/840, Loss: 4.017929983139038\n",
      "Epoch 163/840, Loss: 4.017636156082153\n",
      "Epoch 164/840, Loss: 4.017389106750488\n",
      "Epoch 165/840, Loss: 4.017102336883545\n",
      "Epoch 166/840, Loss: 4.016882419586182\n",
      "Epoch 167/840, Loss: 4.016614723205566\n",
      "Epoch 168/840, Loss: 4.016363620758057\n",
      "Epoch 169/840, Loss: 4.016108846664428\n",
      "Epoch 170/840, Loss: 4.015878915786743\n",
      "Epoch 171/840, Loss: 4.015660285949707\n",
      "Epoch 172/840, Loss: 4.015388298034668\n",
      "Epoch 173/840, Loss: 4.015192747116089\n",
      "Epoch 174/840, Loss: 4.01492862701416\n",
      "Epoch 175/840, Loss: 4.014701080322266\n",
      "Epoch 176/840, Loss: 4.01451301574707\n",
      "Epoch 177/840, Loss: 4.014242458343506\n",
      "Epoch 178/840, Loss: 4.014062690734863\n",
      "Epoch 179/840, Loss: 4.013854885101319\n",
      "Epoch 180/840, Loss: 4.013617038726807\n",
      "Epoch 181/840, Loss: 4.013439083099366\n",
      "Epoch 182/840, Loss: 4.01321496963501\n",
      "Epoch 183/840, Loss: 4.01301531791687\n",
      "Epoch 184/840, Loss: 4.0128332614898685\n",
      "Epoch 185/840, Loss: 4.012628173828125\n",
      "Epoch 186/840, Loss: 4.012402105331421\n",
      "Epoch 187/840, Loss: 4.012248373031616\n",
      "Epoch 188/840, Loss: 4.012052583694458\n",
      "Epoch 189/840, Loss: 4.01182861328125\n",
      "Epoch 190/840, Loss: 4.011684799194336\n",
      "Epoch 191/840, Loss: 4.011486053466797\n",
      "Epoch 192/840, Loss: 4.011304092407227\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model Definition\n",
    "class CuedSpeechRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_layers=3):\n",
    "        super(CuedSpeechRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim + 1)  # +1 for the CTC blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.relu()\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, y_train_tensors, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in zip(train_loader, y_train_tensors):\n",
    "            optimizer.zero_grad()\n",
    "            # Ensure X_batch is 3D: (batch_size, sequence_length, feature_dimension)\n",
    "            X_batch = X_batch[0]  # Extract the tensor from the batch tuple\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)  # Add batch dimension if needed\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.long) # Sequence length for each batch element \n",
    "            target_lengths = torch.tensor([len(y_batch[y_batch != phoneme_to_index[' ']])], dtype=torch.long) # Target sequence length ignoring padding \n",
    "            # Compute CTC loss \n",
    "            loss = criterion(outputs.log_softmax(2).transpose(0, 1), y_batch.unsqueeze(0), input_lengths, target_lengths) \n",
    "            loss.backward() \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1) \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader, y_val_tensors, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in zip(val_loader, y_val_tensors):\n",
    "            X_batch = X_batch[0]\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.long) # Sequence length for each batch element \n",
    "            target_lengths = torch.tensor([len(y_batch[y_batch != phoneme_to_index[' ']])], dtype=torch.long) # Target sequence length ignoring padding \n",
    "            val_loss = criterion(outputs.transpose(0, 1), y_batch.unsqueeze(0), input_lengths, target_lengths)\n",
    "            total_val_loss += val_loss.item()\n",
    "            break\n",
    "    print(f\"Validation Loss: {total_val_loss/len(val_loader)}\")\n",
    "\n",
    "# Decoding Function\n",
    "def greedy_decoder(output, blank):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank:\n",
    "                if j != 0 and index == args[j - 1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "# Instantiate and Train Model\n",
    "input_dim = X_combined.shape[-1]\n",
    "output_dim = len(phoneme_to_index)\n",
    "model = CuedSpeechRNN(input_dim, output_dim)\n",
    "criterion = nn.CTCLoss(blank=len(phoneme_to_index))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, y_train_tensors, criterion, optimizer, num_epochs=840)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, train_loader, y_val_tensors, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequences: [['e', 'mi', 'li', 'la', 'vɛ', 'de', 'te', 's', 'te', 'dɛ', 'u', 'ɛl', 'tɛ̃', 'rɔ̃', 'kɔ̃', 't', 're']]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(output, blank):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    for args in arg_maxes:\n",
    "        decode = []\n",
    "        previous_idx = None\n",
    "        for index in args:\n",
    "            if index != blank and (previous_idx is None or index != previous_idx):\n",
    "                decode.append(index.item())\n",
    "            previous_idx = index\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "# Decoding\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test[0].unsqueeze(0))\n",
    "    decoded_phoneme_sequences = greedy_decoder(outputs, blank=len(phoneme_to_index))\n",
    "decoded_phonemes = [[index_to_phoneme[idx] for idx in sequence] for sequence in decoded_phoneme_sequences]\n",
    "print(\"Decoded phoneme sequences:\", decoded_phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-MgaKDfGw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
