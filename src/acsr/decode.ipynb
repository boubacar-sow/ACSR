{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 201 unique syllables to /scratch2/bsow/Documents/ACSR/data/training_videos/syllable_dictionary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_syllables_from_lpc(directory, output_file):\n",
    "    \"\"\"\n",
    "    Extract all unique syllables from .lpc files and save them to a file.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .lpc files.\n",
    "        output_file (str): Path to the output file (CSV or text) where syllables will be saved.\n",
    "    \"\"\"\n",
    "    # Initialize a set to store unique syllables\n",
    "    unique_syllables = set()\n",
    "\n",
    "    # Iterate through all .lpc files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".lpc\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    # Extract the syllable (first column)\n",
    "                    syllable = line.strip().split()[0]\n",
    "                    unique_syllables.add(syllable.split('_')[0])\n",
    "\n",
    "    # Save the unique syllables to the output file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for syllable in sorted(unique_syllables):\n",
    "            f.write(f\"{syllable}\\n\")\n",
    "\n",
    "    print(f\"Saved {len(unique_syllables)} unique syllables to {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "lpc_directory = \"/scratch2/bsow/Documents/ACSR/data/training_videos/lpc\"\n",
    "output_file = \"/scratch2/bsow/Documents/ACSR/data/training_videos/syllable_dictionary.txt\"\n",
    "extract_syllables_from_lpc(lpc_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load CSV files from a directory based on a filename pattern\n",
    "def load_csv_files(directory, filename_pattern, type=\"position\"):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            df.dropna(inplace=True)\n",
    "            base_name = filename.split(f'_{type}_')[1].split('.csv')[0]\n",
    "            files_data[base_name] = df\n",
    "    return files_data\n",
    "\n",
    "# Load features from .npy files based on a filename pattern\n",
    "def load_features(directory, filename_pattern):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            features = pd.read_csv(os.path.join(directory, filename))\n",
    "            features.dropna(inplace=True)\n",
    "            base_name = filename.split('_features')[0]\n",
    "            files_data[base_name] = features\n",
    "    return files_data\n",
    "\n",
    "# Find corresponding phoneme files based on the base names of position filenames\n",
    "def find_phoneme_files(directory, base_names):\n",
    "    phoneme_files = {}\n",
    "    for base_name in base_names:\n",
    "        phoneme_file = os.path.join(directory, f'{base_name}.lpc')\n",
    "        if os.path.exists(phoneme_file):\n",
    "            phoneme_files[base_name] = phoneme_file\n",
    "    return phoneme_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================================\n",
    "# Helper Functions\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of sequences to pad.\n",
    "        max_length (int): Maximum length to pad to.\n",
    "        pad_value (int): Value to use for padding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded sequences.\n",
    "    \"\"\"\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.full((max_length - len(seq), seq.shape[1]), pad_value)\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "\n",
    "def extract_probabilities(data, columns):\n",
    "    \"\"\"\n",
    "    Extract and concatenate probabilities from multiple DataFrames.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of DataFrames.\n",
    "        columns (list): Columns to extract probabilities from.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Concatenated probabilities.\n",
    "    \"\"\"\n",
    "    data = [df.fillna(0) for df in data]\n",
    "    probs_list = [df[columns].to_numpy() for df in data]\n",
    "    return np.concatenate(probs_list, axis=0)\n",
    "\n",
    "\n",
    "def combine_sequences_with_padding(video_data):\n",
    "    \"\"\"\n",
    "    Combine sequences with padding to ensure uniform length.\n",
    "\n",
    "    Args:\n",
    "        video_data (dict): Dictionary containing video data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded input sequences (X) and padded labels (y).\n",
    "    \"\"\"\n",
    "    max_length = max(len(video_data[video][\"X\"]) for video in video_data)\n",
    "    X_padded = [\n",
    "        pad_sequences([video_data[video][\"X\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    y_padded = [\n",
    "        video_data[video][\"y\"]\n",
    "        + [phoneme_to_index[\" \"]] * (max_length - len(video_data[video][\"y\"]))\n",
    "        for video in video_data\n",
    "    ]\n",
    "    return X_padded, y_padded\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Data Preparation Functions\n",
    "# ==========================================================\n",
    "\n",
    "# Load phoneme-to-index mapping\n",
    "with open(\n",
    "    r\"/scratch2/bsow/Documents/ACSR/data/training_videos/syllable_dictionary.txt\", \"r\"\n",
    ") as file:\n",
    "    reader = csv.reader(file)\n",
    "    vocabulary_list = [row[0] for row in reader]\n",
    "\n",
    "\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(vocabulary_list)}\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "phoneme_to_index[\" \"] = len(phoneme_to_index)\n",
    "index_to_phoneme[len(index_to_phoneme)] = \" \"\n",
    "\n",
    "\n",
    "# Prepare data for all videos without sliding windows\n",
    "def prepare_data_for_videos_no_sliding_windows(\n",
    "    hand_position_data, hand_shape_data, phoneme_files, feature_files\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data for all videos without sliding windows.\n",
    "    Only frames specified in the .lpc files are included in the predictions.\n",
    "    Other frames are filled with -1 for the predicted class and 0 for probabilities.\n",
    "\n",
    "    Args:\n",
    "        hand_position_data (dict): Dictionary of hand position data.\n",
    "        hand_shape_data (dict): Dictionary of hand shape data.\n",
    "        phoneme_files (dict): Dictionary of phoneme file paths.\n",
    "        feature_files (dict): Dictionary of feature file paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing combined features, probabilities, and phoneme indices.\n",
    "    \"\"\"\n",
    "    all_videos_data = {}\n",
    "    for base_name in hand_position_data:\n",
    "        if base_name in phoneme_files and base_name in feature_files:\n",
    "            position_df = hand_position_data[base_name]\n",
    "            shape_df = hand_shape_data[base_name]\n",
    "            phoneme_file = phoneme_files[base_name]\n",
    "            feature_df = feature_files[base_name]\n",
    "\n",
    "            # Ensure the feature DataFrame has a 'frame_number' column\n",
    "            if 'frame_number' not in feature_df.columns:\n",
    "                raise ValueError(f\"Feature file for {base_name} does not contain 'frame_number' column.\")\n",
    "\n",
    "            # Get the frame numbers from the feature DataFrame\n",
    "            frame_numbers = feature_df['frame_number'].values\n",
    "\n",
    "            # Initialize arrays for probabilities and predicted classes\n",
    "            num_frames = len(frame_numbers)\n",
    "            num_position_probs = 5  # Number of position probability columns\n",
    "            num_shape_probs = 8  # Number of shape probability columns\n",
    "\n",
    "            # Create arrays to store probabilities and predicted classes\n",
    "            #position_probs = np.zeros((num_frames, num_position_probs))\n",
    "            #shape_probs = np.zeros((num_frames, num_shape_probs))\n",
    "            #predicted_classes = np.full(num_frames, -1)  # Default to -1 for unpredicted frames\n",
    "#\n",
    "            ## Fill in probabilities and predicted classes for frames that have predictions\n",
    "            #for i, frame in enumerate(frame_numbers):\n",
    "            #    if frame in position_df['frame_number'].values:\n",
    "            #        position_probs[i] = position_df.loc[position_df['frame_number'] == frame, [\n",
    "            #            'p_class_1', 'p_class_2', 'p_class_3', 'p_class_4', 'p_class_5'\n",
    "            #        ]].values[0]\n",
    "            #        shape_probs[i] = shape_df.loc[shape_df['frame_number'] == frame, [\n",
    "            #            'p_class_1', 'p_class_2', 'p_class_3', 'p_class_4', 'p_class_5',\n",
    "            #            'p_class_6', 'p_class_7', 'p_class_8'\n",
    "            #        ]].values[0]\n",
    "            #        predicted_classes[i] = position_df.loc[position_df['frame_number'] == frame, 'predicted_class'].values[0]\n",
    "#\n",
    "            #combined_probs = np.hstack((position_probs, shape_probs))\n",
    "#\n",
    "            ## Drop 'fn_video' and 'frame_number' columns from the feature DataFrame\n",
    "            feature_df_filtered = feature_df.drop(columns=['fn_video', 'frame_number'])\n",
    "#\n",
    "            ## Combine filtered features with probabilities\n",
    "            #combined_df = pd.DataFrame(\n",
    "            #    np.hstack((feature_df_filtered.values, combined_probs)),\n",
    "            #    columns=list(feature_df_filtered.columns) + [f'prob_{i}' for i in range(combined_probs.shape[1])]\n",
    "            #)\n",
    "\n",
    "            # Read phoneme sequences\n",
    "            with open(phoneme_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f)\n",
    "                phoneme_sequence = [row[0].split(' ')[0].split('_')[0] for row in reader]\n",
    "\n",
    "            # Convert phoneme sequence to indices\n",
    "            try:\n",
    "                phoneme_indices = [\n",
    "                    phoneme_to_index[phoneme] for phoneme in phoneme_sequence\n",
    "                ]\n",
    "            except KeyError:\n",
    "                print(f\"Unknown phoneme in {base_name}: {phoneme_sequence}\")\n",
    "                phoneme_indices = [\n",
    "                    phoneme_to_index.get(phoneme, -1) for phoneme in phoneme_sequence\n",
    "                ]\n",
    "\n",
    "            # Store the data\n",
    "            all_videos_data[base_name] = {\n",
    "                \"X\": feature_df_filtered.to_numpy(),  # Combined features and probabilities for all frames\n",
    "                \"y\": phoneme_indices,  # Phoneme indices\n",
    "            }\n",
    "    return all_videos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_16':     frame_number  predicted_class  p_class_1  p_class_2  p_class_3  p_class_4  \\\n",
       " 0             17              1.0       0.16       0.03       0.73       0.08   \n",
       " 1             22              1.0       0.87       0.00       0.01       0.00   \n",
       " 2             26              1.0       0.00       0.00       0.00       0.00   \n",
       " 3             31              1.0       0.00       0.00       0.00       0.00   \n",
       " 4             37              1.0       0.00       0.00       0.00       0.00   \n",
       " 5             45              1.0       0.00       0.00       0.00       0.00   \n",
       " 6             55              3.0       0.00       0.00       0.00       0.00   \n",
       " 7             60              3.0       0.00       0.00       0.00       0.00   \n",
       " 8             67              3.0       0.00       0.00       0.00       0.00   \n",
       " 9             77              5.0       0.00       0.00       0.00       0.00   \n",
       " 10            97              1.0       0.00       0.00       0.00       0.00   \n",
       " 11           104              1.0       0.00       0.00       0.00       0.00   \n",
       " 12           111              4.0       0.00       0.00       0.00       0.00   \n",
       " 13           118              1.0       0.00       0.00       0.00       0.00   \n",
       " 14           127              1.0       0.00       0.00       0.00       0.00   \n",
       " 15           144              3.0       0.00       0.00       0.00       0.00   \n",
       " 16           153              3.0       0.00       0.00       0.00       0.00   \n",
       " 17           167              1.0       0.00       0.00       0.00       0.00   \n",
       " 18           178              0.0       0.00       0.00       0.00       0.00   \n",
       " 19           185              5.0       0.00       0.00       0.00       0.00   \n",
       " 20           201              5.0       0.00       0.00       0.00       0.00   \n",
       " 21           223              1.0       0.00       0.00       0.00       0.00   \n",
       " \n",
       "     p_class_5  \n",
       " 0        0.00  \n",
       " 1        0.12  \n",
       " 2        0.00  \n",
       " 3        0.00  \n",
       " 4        0.00  \n",
       " 5        0.00  \n",
       " 6        0.00  \n",
       " 7        0.00  \n",
       " 8        0.00  \n",
       " 9        0.00  \n",
       " 10       0.00  \n",
       " 11       0.00  \n",
       " 12       0.00  \n",
       " 13       0.00  \n",
       " 14       0.00  \n",
       " 15       0.00  \n",
       " 16       0.00  \n",
       " 17       0.00  \n",
       " 18       0.00  \n",
       " 19       0.00  \n",
       " 20       0.00  \n",
       " 21       0.00  }"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of position files: 95\n",
      "Number of shape files: 95\n",
      "Number of phoneme files: 95\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Main Script\n",
    "# ==========================================================\n",
    "\n",
    "# Directories\n",
    "data_dir = r'/scratch2/bsow/Documents/ACSR/output/predictions'\n",
    "phoneme_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/lpc'\n",
    "\n",
    "# Load position and shape data\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "hand_shape_data = load_csv_files(data_dir, 'predictions_rf_shape', type='shape')\n",
    "\n",
    "# Find phoneme files\n",
    "base_names = list(hand_position_data.keys())\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n",
    "\n",
    "# Load feature\n",
    "feature_dir = r'/scratch2/bsow/Documents/ACSR/output/extracted_features'\n",
    "feature_files = load_features(feature_dir, 'features')\n",
    "# Print the number of files found\n",
    "print(f\"Number of position files: {len(hand_position_data)}\")\n",
    "print(f\"Number of shape files: {len(hand_shape_data)}\")\n",
    "print(f\"Number of phoneme files: {len(phoneme_files)}\")\n",
    "\n",
    "hand_position_data = {\n",
    "    key: hand_position_data[key] for key in list(hand_position_data.keys())\n",
    "}\n",
    "hand_shape_data = {\n",
    "    key: hand_shape_data[key] for key in list(hand_shape_data.keys())\n",
    "}\n",
    "phoneme_files = {key: phoneme_files[key] for key in list(phoneme_files.keys())}\n",
    "\n",
    "# Prepare data\n",
    "all_videos_data = prepare_data_for_videos_no_sliding_windows(\n",
    "    hand_position_data, hand_shape_data, phoneme_files, feature_files\n",
    ")\n",
    "X_combined, y_combined = combine_sequences_with_padding(all_videos_data)\n",
    "\n",
    "# Convert phoneme sequences to tensors\n",
    "y_tensors = [\n",
    "    torch.tensor([index for index in video_data[\"y\"]], dtype=torch.long)\n",
    "    for video_data in all_videos_data.values()\n",
    "]\n",
    "all_videos_data = {\n",
    "    key: {\"X\": video_data[\"X\"], \"y\": y_tensors[i]}\n",
    "    for i, (key, video_data) in enumerate(all_videos_data.items())\n",
    "}\n",
    "\n",
    "\n",
    "# Ensure X_combined contains only numeric data\n",
    "try:\n",
    "    X_combined_numeric = np.array(X_combined, dtype=np.float32)  # Force conversion to float32\n",
    "except ValueError as e:\n",
    "    print(f\"Error converting X_combined to numeric: {e}\")\n",
    "    # Identify problematic elements\n",
    "    for i, x in enumerate(X_combined):\n",
    "        if not isinstance(x, (np.ndarray, list, float, int)):\n",
    "            print(f\"Non-numeric element at index {i}: {x}\")\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_combined_tensor = torch.tensor(X_combined_numeric, dtype=torch.float32)\n",
    "y_combined_tensor = torch.tensor(y_combined, dtype=torch.long)\n",
    "\n",
    "# Final organized data\n",
    "all_videos_data = {\"X\": X_combined_tensor, \"y\": y_combined_tensor}\n",
    "# Combine all data into tensors\n",
    "X_combined = torch.tensor(np.array(X_combined), dtype=torch.float32) \n",
    "y_combined = torch.tensor(y_combined, dtype=torch.long)\n",
    "\n",
    "# Final organized data\n",
    "all_videos_data = {\"X\": X_combined, \"y\": y_combined}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train dataset 95\n",
      "Len of val dataset 0\n",
      "Batch X shape: torch.Size([4, 327, 28])\n",
      "Batch y shape: torch.Size([4, 327])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to split data into training and validation sets\n",
    "def train_val_split(data, train_ratio=1):\n",
    "    num_samples = len(data['X'])\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    # randomize the data\n",
    "    indices = torch.randperm(num_samples)\n",
    "    data['X'] = data['X'][indices]\n",
    "    data['y'] = data['y'][indices]\n",
    "    \n",
    "    train_data = {\n",
    "        'X': data['X'][:split_idx],\n",
    "        'y': data['y'][:split_idx]\n",
    "    }\n",
    "    val_data = {\n",
    "        'X': data['X'][split_idx:],\n",
    "        'y': data['y'][split_idx:]\n",
    "    }\n",
    "    return train_data, val_data\n",
    "\n",
    "# Convert data to DataLoader format\n",
    "def data_to_dataloader(data, batch_size=4, shuffle=True):\n",
    "    X_tensors = data['X']\n",
    "    y_tensors = data['y']\n",
    "    \n",
    "    # Create a TensorDataset with both inputs and labels\n",
    "    dataset = TensorDataset(X_tensors, y_tensors)\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_val_split(all_videos_data)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = data_to_dataloader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = data_to_dataloader(val_data, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Len of train dataset\", len(train_data['X']))\n",
    "print(\"Len of val dataset\", len(val_data['X']))\n",
    "\n",
    "# Check the DataLoader output\n",
    "for batch_X, batch_y in train_loader:\n",
    "    print(\"Batch X shape:\", batch_X.shape)\n",
    "    print(\"Batch y shape:\", batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch 1/300, Loss: 29.39581588904063\n",
      "Epoch 2/300, Loss: 6.098719934622447\n",
      "Epoch 3/300, Loss: 5.1515012582143145\n",
      "Epoch 4/300, Loss: 5.084985037644704\n",
      "Epoch 5/300, Loss: 5.057485441366832\n",
      "Epoch 6/300, Loss: 5.109637816747029\n",
      "Epoch 7/300, Loss: 5.14490020275116\n",
      "Epoch 8/300, Loss: 5.057457288106282\n",
      "Epoch 9/300, Loss: 5.065799653530121\n",
      "Epoch 10/300, Loss: 5.0809794664382935\n",
      "Epoch 11/300, Loss: 5.042302429676056\n",
      "Epoch 12/300, Loss: 5.108588079611461\n",
      "Epoch 13/300, Loss: 5.063876291116078\n",
      "Epoch 14/300, Loss: 5.0930026570955915\n",
      "Epoch 15/300, Loss: 5.098359783490499\n",
      "Epoch 16/300, Loss: 5.063571810722351\n",
      "Epoch 17/300, Loss: 5.12009859085083\n",
      "Epoch 18/300, Loss: 5.06608259677887\n",
      "Epoch 19/300, Loss: 5.050159712632497\n",
      "Epoch 20/300, Loss: 5.053357303142548\n",
      "Epoch 21/300, Loss: 5.009483675161998\n",
      "Epoch 22/300, Loss: 5.0445794860521955\n",
      "Epoch 23/300, Loss: 4.984004735946655\n",
      "Epoch 24/300, Loss: 5.106099168459575\n",
      "Epoch 25/300, Loss: 5.104331791400909\n",
      "Epoch 26/300, Loss: 4.943386197090149\n",
      "Epoch 27/300, Loss: 4.871554613113403\n",
      "Epoch 28/300, Loss: 4.886423230171204\n",
      "Epoch 29/300, Loss: 4.910451352596283\n",
      "Epoch 30/300, Loss: 4.900171120961507\n",
      "Epoch 31/300, Loss: 4.843673189481099\n",
      "Epoch 32/300, Loss: 4.884590268135071\n",
      "Epoch 33/300, Loss: 4.810021340847015\n",
      "Epoch 34/300, Loss: 4.839190264542897\n",
      "Epoch 35/300, Loss: 4.788866301377614\n",
      "Epoch 36/300, Loss: 4.778794964154561\n",
      "Epoch 37/300, Loss: 4.775268296400706\n",
      "Epoch 38/300, Loss: 4.752802272637685\n",
      "Epoch 39/300, Loss: 4.73894327878952\n",
      "Epoch 40/300, Loss: 4.72352268298467\n",
      "Epoch 41/300, Loss: 4.726897180080414\n",
      "Epoch 42/300, Loss: 4.706658720970154\n",
      "Epoch 43/300, Loss: 4.688500980536143\n",
      "Epoch 44/300, Loss: 4.651799142360687\n",
      "Epoch 45/300, Loss: 4.6355390548706055\n",
      "Epoch 46/300, Loss: 4.625977794329326\n",
      "Epoch 47/300, Loss: 4.620302240053813\n",
      "Epoch 48/300, Loss: 4.602903783321381\n",
      "Epoch 49/300, Loss: 4.679975966612498\n",
      "Epoch 50/300, Loss: 4.640975991884868\n",
      "Epoch 51/300, Loss: 4.598369816939036\n",
      "Epoch 52/300, Loss: 4.570240994294484\n",
      "Epoch 53/300, Loss: 4.55376793940862\n",
      "Epoch 54/300, Loss: 4.534517049789429\n",
      "Epoch 55/300, Loss: 4.522670825322469\n",
      "Epoch 56/300, Loss: 4.525016089280446\n",
      "Epoch 57/300, Loss: 4.583777527014415\n",
      "Epoch 58/300, Loss: 4.536925832430522\n",
      "Epoch 59/300, Loss: 4.581718007723491\n",
      "Epoch 60/300, Loss: 4.557373444239299\n",
      "Epoch 61/300, Loss: 4.4996727506319685\n",
      "Epoch 62/300, Loss: 4.4892507791519165\n",
      "Epoch 63/300, Loss: 4.4692727128664655\n",
      "Epoch 64/300, Loss: 4.458310008049011\n",
      "Epoch 65/300, Loss: 4.4330681165059405\n",
      "Epoch 66/300, Loss: 4.404375781615575\n",
      "Epoch 67/300, Loss: 4.389251679182053\n",
      "Epoch 68/300, Loss: 4.367199798425038\n",
      "Epoch 69/300, Loss: 4.367304513851802\n",
      "Epoch 70/300, Loss: 4.3478516936302185\n",
      "Epoch 71/300, Loss: 4.332657217979431\n",
      "Epoch 72/300, Loss: 4.3328234652678175\n",
      "Epoch 73/300, Loss: 4.327981263399124\n",
      "Epoch 74/300, Loss: 4.325386802355449\n",
      "Epoch 75/300, Loss: 4.302828202644984\n",
      "Epoch 76/300, Loss: 4.279608488082886\n",
      "Epoch 77/300, Loss: 4.262297997872035\n",
      "Epoch 78/300, Loss: 4.236314684152603\n",
      "Epoch 79/300, Loss: 4.220947682857513\n",
      "Epoch 80/300, Loss: 4.211636920770009\n",
      "Epoch 81/300, Loss: 4.1738976041475935\n",
      "Epoch 82/300, Loss: 4.146091868480046\n",
      "Epoch 83/300, Loss: 4.155143400033315\n",
      "Epoch 84/300, Loss: 4.135243058204651\n",
      "Epoch 85/300, Loss: 4.113212664922078\n",
      "Epoch 86/300, Loss: 4.0865479211012525\n",
      "Epoch 87/300, Loss: 4.0652784903844195\n",
      "Epoch 88/300, Loss: 4.04498149951299\n",
      "Epoch 89/300, Loss: 4.023266871770223\n",
      "Epoch 90/300, Loss: 3.997539003690084\n",
      "Epoch 91/300, Loss: 3.968932797511419\n",
      "Epoch 92/300, Loss: 3.9467202921708426\n",
      "Epoch 93/300, Loss: 3.938092211882273\n",
      "Epoch 94/300, Loss: 3.9208616415659585\n",
      "Epoch 95/300, Loss: 3.889721761147181\n",
      "Epoch 96/300, Loss: 3.874755452076594\n",
      "Epoch 97/300, Loss: 3.851146479447683\n",
      "Epoch 98/300, Loss: 3.8180802265803018\n",
      "Epoch 99/300, Loss: 3.7805957595507302\n",
      "Epoch 100/300, Loss: 3.7739397585392\n",
      "Epoch 101/300, Loss: 3.7705125411351523\n",
      "Epoch 102/300, Loss: 3.735371301571528\n",
      "Epoch 103/300, Loss: 3.7321186860402427\n",
      "Epoch 104/300, Loss: 3.702238768339157\n",
      "Epoch 105/300, Loss: 3.675968199968338\n",
      "Epoch 106/300, Loss: 3.6552456319332123\n",
      "Epoch 107/300, Loss: 3.607890099287033\n",
      "Epoch 108/300, Loss: 3.5836480061213174\n",
      "Epoch 109/300, Loss: 3.540152460336685\n",
      "Epoch 110/300, Loss: 3.513206700483958\n",
      "Epoch 111/300, Loss: 3.497091213862101\n",
      "Epoch 112/300, Loss: 3.4758381148179374\n",
      "Epoch 113/300, Loss: 3.4721050759156546\n",
      "Epoch 114/300, Loss: 3.4403305053710938\n",
      "Epoch 115/300, Loss: 3.403589755296707\n",
      "Epoch 116/300, Loss: 3.3925783137480416\n",
      "Epoch 117/300, Loss: 3.3427268465360007\n",
      "Epoch 118/300, Loss: 3.3279216587543488\n",
      "Epoch 119/300, Loss: 3.3103676438331604\n",
      "Epoch 120/300, Loss: 3.2824739615122476\n",
      "Epoch 121/300, Loss: 3.26828404267629\n",
      "Epoch 122/300, Loss: 3.214747687180837\n",
      "Epoch 123/300, Loss: 3.1860491881767907\n",
      "Epoch 124/300, Loss: 3.1691560546557107\n",
      "Epoch 125/300, Loss: 3.134571095307668\n",
      "Epoch 126/300, Loss: 3.12895534435908\n",
      "Epoch 127/300, Loss: 3.1045170625050864\n",
      "Epoch 128/300, Loss: 3.099079062541326\n",
      "Epoch 129/300, Loss: 3.077720989783605\n",
      "Epoch 130/300, Loss: 3.0463025172551474\n",
      "Epoch 131/300, Loss: 2.9984092116355896\n",
      "Epoch 132/300, Loss: 2.9645257691542306\n",
      "Epoch 133/300, Loss: 2.939554731051127\n",
      "Epoch 134/300, Loss: 2.9139713247617087\n",
      "Epoch 135/300, Loss: 2.8845750987529755\n",
      "Epoch 136/300, Loss: 2.8936356753110886\n",
      "Epoch 137/300, Loss: 2.878239537278811\n",
      "Epoch 138/300, Loss: 2.862592796484629\n",
      "Epoch 139/300, Loss: 2.827513257662455\n",
      "Epoch 140/300, Loss: 2.7937046587467194\n",
      "Epoch 141/300, Loss: 2.777224143346151\n",
      "Epoch 142/300, Loss: 2.7178831497828164\n",
      "Epoch 143/300, Loss: 2.7007436404625573\n",
      "Epoch 144/300, Loss: 2.6702323953310647\n",
      "Epoch 145/300, Loss: 2.6572363674640656\n",
      "Epoch 146/300, Loss: 2.6236169735590615\n",
      "Epoch 147/300, Loss: 2.6242092698812485\n",
      "Epoch 148/300, Loss: 2.6012791891892753\n",
      "Epoch 149/300, Loss: 2.5801435758670173\n",
      "Epoch 150/300, Loss: 2.536168043812116\n",
      "Epoch 151/300, Loss: 2.5415087143580117\n",
      "Epoch 152/300, Loss: 2.555972362558047\n",
      "Epoch 153/300, Loss: 2.5108672529459\n",
      "Epoch 154/300, Loss: 2.482525830467542\n",
      "Epoch 155/300, Loss: 2.471474086244901\n",
      "Epoch 156/300, Loss: 2.47182127336661\n",
      "Epoch 157/300, Loss: 2.4613920698563256\n",
      "Epoch 158/300, Loss: 2.393083224693934\n",
      "Epoch 159/300, Loss: 2.6648091276486716\n",
      "Epoch 160/300, Loss: 2.5135349184274673\n",
      "Epoch 161/300, Loss: 2.4595824827750525\n",
      "Epoch 162/300, Loss: 2.378651355703672\n",
      "Epoch 163/300, Loss: 2.3525465379158654\n",
      "Epoch 164/300, Loss: 2.3187915235757828\n",
      "Epoch 165/300, Loss: 2.297069619099299\n",
      "Epoch 166/300, Loss: 2.2482468684514365\n",
      "Epoch 167/300, Loss: 2.215501000483831\n",
      "Epoch 168/300, Loss: 2.211974730094274\n",
      "Epoch 169/300, Loss: 2.1873936156431832\n",
      "Epoch 170/300, Loss: 2.1471861203511557\n",
      "Epoch 171/300, Loss: 2.149473175406456\n",
      "Epoch 172/300, Loss: 2.097219631075859\n",
      "Epoch 173/300, Loss: 2.094142114122709\n",
      "Epoch 174/300, Loss: 2.0673440595467887\n",
      "Epoch 175/300, Loss: 2.039633587002754\n",
      "Epoch 176/300, Loss: 2.0257305006186166\n",
      "Epoch 177/300, Loss: 2.0222298055887222\n",
      "Epoch 178/300, Loss: 2.0394073873758316\n",
      "Epoch 179/300, Loss: 2.0785057991743088\n",
      "Epoch 180/300, Loss: 2.179179017742475\n",
      "Epoch 181/300, Loss: 2.157841220498085\n",
      "Epoch 182/300, Loss: 2.1033386290073395\n",
      "Epoch 183/300, Loss: 2.009781206647555\n",
      "Epoch 184/300, Loss: 2.000512426098188\n",
      "Epoch 185/300, Loss: 1.9712640692790349\n",
      "Epoch 186/300, Loss: 1.919212246934573\n",
      "Epoch 187/300, Loss: 1.9050878485043843\n",
      "Epoch 188/300, Loss: 1.8742586895823479\n",
      "Epoch 189/300, Loss: 1.8998831808567047\n",
      "Epoch 190/300, Loss: 1.8876151765386264\n",
      "Epoch 191/300, Loss: 1.864774649341901\n",
      "Epoch 192/300, Loss: 1.8673624296983082\n",
      "Epoch 193/300, Loss: 1.8191237052281697\n",
      "Epoch 194/300, Loss: 1.7807139481107395\n",
      "Epoch 195/300, Loss: 1.7631659309069316\n",
      "Epoch 196/300, Loss: 1.7470101887981098\n",
      "Epoch 197/300, Loss: 1.708702211578687\n",
      "Epoch 198/300, Loss: 1.6892996629079182\n",
      "Epoch 199/300, Loss: 1.6630089928706486\n",
      "Epoch 200/300, Loss: 1.6791933973630269\n",
      "Epoch 201/300, Loss: 1.6766777535279591\n",
      "Epoch 202/300, Loss: 1.6953863526384036\n",
      "Epoch 203/300, Loss: 1.6793572480479877\n",
      "Epoch 204/300, Loss: 1.7009534239768982\n",
      "Epoch 205/300, Loss: 1.7114164903759956\n",
      "Epoch 206/300, Loss: 1.6915767788887024\n",
      "Epoch 207/300, Loss: 1.6541651338338852\n",
      "Epoch 208/300, Loss: 1.6220327218373616\n",
      "Epoch 209/300, Loss: 1.6800650134682655\n",
      "Epoch 210/300, Loss: 1.6381639738877614\n",
      "Epoch 211/300, Loss: 1.6369323531786601\n",
      "Epoch 212/300, Loss: 1.5922486931085587\n",
      "Epoch 213/300, Loss: 1.6754334966341655\n",
      "Epoch 214/300, Loss: 1.6479149510463078\n",
      "Epoch 215/300, Loss: 1.606260081132253\n",
      "Epoch 216/300, Loss: 1.5844051390886307\n",
      "Epoch 217/300, Loss: 1.5307502175370853\n",
      "Epoch 218/300, Loss: 1.4989249656597774\n",
      "Epoch 219/300, Loss: 1.4992086564501126\n",
      "Epoch 220/300, Loss: 1.4605365246534348\n",
      "Epoch 221/300, Loss: 1.487133135398229\n",
      "Epoch 222/300, Loss: 1.446550164371729\n",
      "Epoch 223/300, Loss: 1.4635108237465222\n",
      "Epoch 224/300, Loss: 1.4146488060553868\n",
      "Epoch 225/300, Loss: 1.394215613603592\n",
      "Epoch 226/300, Loss: 1.3862135335803032\n",
      "Epoch 227/300, Loss: 1.38299011439085\n",
      "Epoch 228/300, Loss: 1.344331403573354\n",
      "Epoch 229/300, Loss: 1.367937684059143\n",
      "Epoch 230/300, Loss: 1.3336236029863358\n",
      "Epoch 231/300, Loss: 1.3130131686727207\n",
      "Epoch 232/300, Loss: 1.3198999340335529\n",
      "Epoch 233/300, Loss: 1.3163149108489354\n",
      "Epoch 234/300, Loss: 1.2864098623394966\n",
      "Epoch 235/300, Loss: 1.2868143022060394\n",
      "Epoch 236/300, Loss: 1.290023423731327\n",
      "Epoch 237/300, Loss: 1.3090869970619678\n",
      "Epoch 238/300, Loss: 1.2689052323500316\n",
      "Epoch 239/300, Loss: 1.2508585328857105\n",
      "Epoch 240/300, Loss: 1.2485923692584038\n",
      "Epoch 241/300, Loss: 1.3009783041973908\n",
      "Epoch 242/300, Loss: 1.376331386466821\n",
      "Epoch 243/300, Loss: 1.3412817281981309\n",
      "Epoch 244/300, Loss: 1.2554214956859748\n",
      "Epoch 245/300, Loss: 1.2887451810141404\n",
      "Epoch 246/300, Loss: 1.3225470781326294\n",
      "Epoch 247/300, Loss: 1.2885237087806065\n",
      "Epoch 248/300, Loss: 1.254599077006181\n",
      "Epoch 249/300, Loss: 1.2911854113141696\n",
      "Epoch 250/300, Loss: 1.2796202301979065\n",
      "Epoch 251/300, Loss: 1.2574661076068878\n",
      "Epoch 252/300, Loss: 1.296250684807698\n",
      "Epoch 253/300, Loss: 1.3454705079396565\n",
      "Epoch 254/300, Loss: 1.2605520536502202\n",
      "Epoch 255/300, Loss: 1.2217486165463924\n",
      "Epoch 256/300, Loss: 1.1994930629928906\n",
      "Epoch 257/300, Loss: 1.2225732058286667\n",
      "Epoch 258/300, Loss: 1.1852686790128548\n",
      "Epoch 259/300, Loss: 1.1866531024376552\n",
      "Epoch 260/300, Loss: 1.1622452760736148\n",
      "Epoch 261/300, Loss: 1.1571307393411796\n",
      "Epoch 262/300, Loss: 1.227621388932069\n",
      "Epoch 263/300, Loss: 1.2377563168605168\n",
      "Epoch 264/300, Loss: 1.234164168437322\n",
      "Epoch 265/300, Loss: 1.1723910731573899\n",
      "Epoch 266/300, Loss: 1.1669721379876137\n",
      "Epoch 267/300, Loss: 1.1310239384571712\n",
      "Epoch 268/300, Loss: 1.078317708025376\n",
      "Epoch 269/300, Loss: 1.0832637002070744\n",
      "Epoch 270/300, Loss: 1.0606077089905739\n",
      "Epoch 271/300, Loss: 1.0536366881181796\n",
      "Epoch 272/300, Loss: 1.0489691731830437\n",
      "Epoch 273/300, Loss: 1.044127778460582\n",
      "Epoch 274/300, Loss: 1.122306950390339\n",
      "Epoch 275/300, Loss: 1.0979855520029862\n",
      "Epoch 276/300, Loss: 1.0748655088245869\n",
      "Epoch 277/300, Loss: 1.092651803046465\n",
      "Epoch 278/300, Loss: 1.1026265397667885\n",
      "Epoch 279/300, Loss: 1.043578849484523\n",
      "Epoch 280/300, Loss: 1.0191356154779594\n",
      "Epoch 281/300, Loss: 1.0063211576392252\n",
      "Epoch 282/300, Loss: 0.9809623224039873\n",
      "Epoch 283/300, Loss: 0.97830395710965\n",
      "Epoch 284/300, Loss: 0.9808759763836861\n",
      "Epoch 285/300, Loss: 0.9951427641014258\n",
      "Epoch 286/300, Loss: 0.9873332704106966\n",
      "Epoch 287/300, Loss: 0.9966537021100521\n",
      "Epoch 288/300, Loss: 0.9768932225803534\n",
      "Epoch 289/300, Loss: 0.9378814697265625\n",
      "Epoch 290/300, Loss: 0.9217184868951639\n",
      "Epoch 291/300, Loss: 0.9539552840093771\n",
      "Epoch 292/300, Loss: 0.9602721072733402\n",
      "Epoch 293/300, Loss: 0.9485300543407599\n",
      "Epoch 294/300, Loss: 1.031084728737672\n",
      "Epoch 295/300, Loss: 0.9792050955196222\n",
      "Epoch 296/300, Loss: 0.9473046871523062\n",
      "Epoch 297/300, Loss: 0.9209346224864324\n",
      "Epoch 298/300, Loss: 0.9076923225075006\n",
      "Epoch 299/300, Loss: 0.908341613287727\n",
      "Epoch 300/300, Loss: 0.9462707862257957\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m train_model(model, train_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 61\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_loader, criterion)\u001b[0m\n\u001b[1;32m     58\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), y_batch, input_lengths, target_lengths)\n\u001b[1;32m     59\u001b[0m         total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtotal_val_loss\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Model Definition\n",
    "class CuedSpeechRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=48, num_layers=2):\n",
    "        super(CuedSpeechRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim + 1)  # +1 for the CTC blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # Ensure X_batch is 3D: (batch_size, sequence_length, feature_dimension)\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((X_batch.size(0),), outputs.size(1), dtype=torch.long)  # Sequence length for each batch element\n",
    "            target_lengths = torch.tensor([len(y[y != phoneme_to_index[' ']]) for y in y_batch], dtype=torch.long)  # Target sequence length ignoring padding\n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(outputs.transpose(0, 1), y_batch, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((X_batch.size(0),), outputs.size(1), dtype=torch.long)  # Sequence length for each batch element\n",
    "            target_lengths = torch.tensor([len(y[y != phoneme_to_index[' ']]) for y in y_batch], dtype=torch.long)  # Target sequence length ignoring padding\n",
    "\n",
    "            val_loss = criterion(outputs.transpose(0, 1), y_batch, input_lengths, target_lengths)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_val_loss/len(val_loader)}\")\n",
    "\n",
    "# Instantiate and Train Model\n",
    "input_dim = X_combined.shape[-1]\n",
    "output_dim = len(phoneme_to_index)\n",
    "model = CuedSpeechRNN(input_dim, output_dim)\n",
    "criterion = nn.CTCLoss(blank=len(phoneme_to_index))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=300)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.295373439788818\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded training phoneme sequences: [['la', 'p', 'ɥi', 'ʁ', 'wɑ', 'ɑ̃', 't'], ['pu', 'v', 'zɛ̃', 'ɛ', 'ʁə', 'pɛ̃', 'sa', 't'], ['mɔ̃', 'p', 'ʁɔ', 'ʁ', 't'], ['a', 't'], ['bɛ', 't', 'ʁ', 'pɛ̃', 'ɑ̃', 't'], ['dɛ', 't'], ['le', 'la', 'ka', 'do', 'pu', 'a', 't'], ['t'], ['sə', 'fɛ', 'lø', 'lø', 't'], ['la', 'p', 'lø', 'tɛ', 'də', 't'], ['i', 'p', 't'], ['pu', 'ɡo', 'n', 't', 't'], ['t'], ['i', 't'], ['pu', 'ʁa', 'ɡ', 'za', 'mɛ̃', 'ɛ', 'ma', 'nə', 't'], ['i', 't'], ['ʒə', 'a', 't'], ['t'], ['le', 'p', 'tɛ', 'kœ', 'ji', 'ʁɛ̃', 'də', 't'], ['a', 't'], ['ʁ'], ['le', 'nɔ', 'z', 'sɑ̃', 'lɛ', 'ze', 't'], ['dɛ', 't'], ['i', 't', 'ʁə', 'ze', 'na', 'tɛ', 'e'], ['la', 's', 'sɔ', 's', 't', 'ʁ', 't'], ['la', 'ya', 'sɔ', 'a', 't'], ['pu', 't'], ['le', 'ʁ'], ['lø', 't'], ['i', 'la', 'fɑ̃', 'də', 'sœ', 'ʁɔ̃', 'də', 't'], ['pu', 'zɔ̃', 'lɛ̃', 't'], ['la', 'də', 'sɔ̃', 'a', 'ti', 'a', 't'], ['la', 'də', 'sə', 't'], ['t'], ['a', 'p', 'də', 't'], ['i', 'a', 'i', 'di', 'fi', 't'], ['le', 'k', 't'], ['bɛ', 'kœ', 'ji', 't'], ['la', 'p', 't'], ['le', 'tɛ', 't'], ['a', 't'], ['la', 'ʁ', 't'], ['dɛ', 't'], ['la', 't', 'də', 'ʁ', 't'], ['dɛ', 't'], ['le', 'ɑ̃', 'p', 'də', 'lɔ̃', 't'], ['o', 'p', 'lø', 'ʒa', 'dɛ̃', 't'], ['la', 'p', 't'], ['pu', 'd', 'ʁ', 't'], ['lø', 'ʁ', 'b', 'də', 'lø', 'də', 't'], ['pu', 'bɛ', 'də', 'də', 't'], ['a', 'n', 'fu', 't'], ['pu', 'tɔ', 'mɑ', 'a', 'də', 't'], ['pu', 'se', 't'], ['pu', 'tɛ', 'də', 'vo', 'pa', 't'], ['le', 't'], ['lø', 'n', 'fu', 't'], ['i', 'də', 'tɛ', 't'], ['a', 'ʁ'], ['a', 't'], ['lø', 'ʃœ', 't'], ['i', 't'], ['le', 't'], ['ʒe', 'ɑ̃', 't'], ['ʁ', 'd', 'ye', 'nə', 'sa', 't'], ['la', 'də', 'lœ', 't'], ['dɛ', 't'], ['i', 'a', 't'], ['pu', 'ʁ', 's', 'lɑ̃', 't'], ['le', 'mɔ̃', 'ji', 'lø', 't'], ['a', 't'], ['la', 'p', 'la', 't'], ['la', 'də', 't'], ['a', 'mɑ̃', 'ʒe', 't'], ['le', 'o', 'ʃɔ', 'ce', 't', 'ʃo', 'ze', 't', 'ʒa', 't'], ['i', 't'], ['ʁi', 'z', 'kə', 'sɔ̃', 't'], ['jɔ̃', 'nə', 'tu', 'ʃe', 't'], ['pu', 'nə', 'pa', 'v', 'wa', 'ʁ', 'fɛ̃', 'i', 'lo', 't'], ['dɛ', 'ʁ'], ['o', 'də', 'ʁɔ', 'zi', 'ke', 't'], ['pu', 'kə', 'e', 'ci', 'də', 't'], ['le', 't'], ['na', 'tɑ̃', 'se', 'ʃe', 't'], ['dɛ', 't']]\n",
      "True training phoneme sequences: [['də', 'p', 'ɥi', 't', 'ʁ', 'wɑ', 'ɑ̃', 'ɛ', 'l', 't', 'ʁa', 'va', 'ja', 'jɛ', 'dɑ̃', 'la', 'ʁ', 'm'], ['kɔ', 'm', 'se', 'v', 'wa', 'zɛ̃', 'ɛ', 'la', 'ɛ', 'l', 'mɛ', 'm', 'ʁə', 'pɛ̃', 'sa', 'v', 'wa', 't', 'y', 'ʁ'], ['mɔ̃', 'p', 'ʁɔ', 'fɛ', 'sœ', 'ʁ', 'ma', 'di', 'ci', 'la', 'vɛ', 'tu', 'ʒu', 'ʁ', 'y', 'pœ', 'ʁ', 'də', 'la', 'po', 'v', 'ʁə', 'te'], ['di', 'na', 'a', 'e', 'te', 's', 'y', 'ʁ', 'p', 'ʁi', 'z', 'da', 'p', 'ʁɑ̃', 'd', 'ʁ', 'kə', 'sɔ̃', 'mɛ', 'jœ', 'ʁa', 'mi', 'e', 'tɛ', 'mɔ', 'ʁ'], ['sa', 'nu', 'vɛ', 'l', 'ʃɑ̃', 'b', 'ʁa', 'ɛ̃', 'm', 'y', 'ʁɑ̃', 't', 'jɛ', 'ʁ', 'mɑ̃', 'pɛ̃', 'ɑ̃', 'ʁu', 'ʒ'], ['dɛ', 'sɔ̃', 'a', 'ʁi', 've', 'dɑ̃', 'lø', 'ɡ', 'ʁɑ̃', 'ma', 'ɡa', 'zɛ̃', 'ɛ', 'l', 'sɛ', 'a', 'ʃ', 'te', 'ɛ̃', 'pa', 'ʁa', 'p', 'l', 'ɥi'], ['la', 'fi', 'jɛ', 'ta', 'ʁə', 's', 'y', 'sə', 'ka', 'do', 'pu', 'ʁ', 'sɔ̃', 'a', 'i', 'vɛ', 'ʁ', 'sɛ', 'ʁ'], ['ɑ̃', 'ʁɑ̃', 't', 'ʁɑ̃', 'le', 'a', 't', 'ʁu', 'va', 's', 'y', 'ʁ', 'la', 'ta', 'b', 'l', 'sɔ̃', 'v', 'jø', 'ku', 'to'], ['ɔ̃', 'fɛ', 'lø', 'a', 'vɛ', 'k', 'lø', 'ʒ', 'y', 'de', 'pɔ', 'm'], ['a', 'p', 'ʁɛ', 'a', 'v', 'wa', 'ʁe', 'te', 'la', 've', 'lø', 'kɔ', 'l', 'ne', 'tɛ', 'p', 'l', 'y', 'ta', 'ʃe', 'də', 'b', 'wa'], ['i', 'la', 'p', 'ʁe', 'si', 'la', 'ʁi', 'ʃɛ', 's', 'ka', 'ʁi', 'lɑ', 'ɡ', 'ʁɑ̃', 'di', 'dɑ̃', 'la', 'po', 'v', 'ʁə', 'te'], ['y', 'ɡo', 'vi', 'vɛ', 'dɑ̃', 'y', 'n', 'ka', 'ba', 'n', 'ci', 'la', 'vɛ', 'l', 'ɥi', 'mɛ', 'm', 'kɔ̃', 's', 't', 'ʁ', 'ɥi', 't'], ['ɛ̃', 't', 'ʁɛ', 'ʒœ', 'n', 'ʃ', 'jɛ̃', 'sa', 'pɛ', 'lɛ̃', 'ʃ', 'jo'], ['i', 'lɛ', 'ɑ̃', 't', 'ʁe', 'dɑ̃', 'sɛ', 't', 'ʃɑ̃', 'b', 'ʁe', 'ɛ', 'mɛ̃', 't', 'nɑ̃', 'ma', 'la', 'd'], ['pu', 'ʁ', 'ʁe', 'y', 'si', 'ʁa', 'lɛ', 'ɡ', 'za', 'mɛ̃', 'ɛ', 'ma', 'nə', 'mɑ̃', 'cɛ', 'pa', 'də', 'fɔ', 'ʁ', 's'], ['dɑ̃', 'sɛ', 't', 'b', 'wa', 't', 'sə', 'ka', 'ʃɛ̃', 'ʃ', 'jo'], ['ʒə', 's', 'ɥi', 'a', 'le', 'a', 'ʃ', 'te', 'ɛ̃', 'ʃa'], ['a', 'vɛ', 'k', 'lø', 'p', 'wa', 'sɔ̃', 'la', 'so', 's', 'ne', 'tɛ', 'pa', 't', 'ʁɛ', 'fɔ', 'ʁ', 't'], ['i', 'za', 'bɛ', 'le', 'tɛ', 'sɔ', 'ʁ', 'ti', 'kœ', 'ji', 'ʁɛ̃', 'bu', 'cɛ', 'də', 'mɑ̃', 't'], ['le', 'dø', 'v', 'wa', 'zɛ̃', 'ɔ̃', 'di', 's', 'k', 'y', 'te', 'e', 'ɔ̃', 'kɔ̃', 'p', 'ʁi', 'ci', 'le', 'tɛ', 'a', 'mu', 'ʁø'], ['ma', 'nɔ̃', 'a', 'e', 'te', 't', 'ʁi', 's', 't', 'də', 'de', 'ku', 'v', 'ʁi', 'ʁ', 'kə', 'sɔ̃', 'p', 'wa', 'sɔ̃', 'ʁu', 'ʒe', 'tɛ', 'mɔ', 'ʁ'], ['le', 'nɔ', 'ʁ', 'm', 'va', 'i', 'z', 'sɑ̃', 'b', 'lɛ', 'pə', 'ze', 'y', 'n', 'tɔ', 'n'], ['a', 'p', 'ʁɛ', 'dø', 'sə', 'mɛ', 'ni', 'i', 'a', 'vɛ', 'p', 'lɛ̃', 'də', 'vɛ', 't', 'mɑ̃', 'sa', 'la', 'la', 've'], ['i', 'l', 'vu', 'lɛ', 't', 'ʁa', 'va', 'je', 'pu', 'ʁ', 'y', 'nɑ̃', 't', 'ʁə', 'p', 'ʁi', 'ze', 'na', 'pa', 'e', 'zi', 'te', 'a', 'si', 'e', 'lø', 'kɔ̃', 't', 'ʁa'], ['la', 'nɔ̃', 's', 'də', 'sɔ', 'l', 'd', 't', 'ʁɛ', 'ɛ̃', 'pɔ', 'ʁ', 'tɑ̃', 'ta', 'a', 'ti', 'ʁe', 'y', 'ni', 'mɑ̃', 's', 'fu', 'l'], ['la', 'ku', 'lœ', 'ʁ', 'la', 'p', 'l', 'ya', 'sɔ', 's', 'je', 'a', 'la', 'kɔ', 'lɛ', 'ʁɛ', 'lø', 'ʁu', 'ʒ'], ['pu', 'ʁa', 'ʁi', 've', 'a', 'tɑ̃', 'i', 'lo', 'ʁɛ', 'd', 'y', 'ma', 'ʁ', 'ʃe', 'p', 'l', 'y', 'vi', 't'], ['pu', 'ʁɔ', 'nɔ', 'ʁe', 'la', 'mɔ', 'ʁ', 'də', 'sɔ̃', 'kɔ', 'lɛ', 'ɡ', 'ci', 'ma', 'a', 'si', 's', 'te', 'a', 'sɔ̃', 'ɑ̃', 'tɛ', 'ʁ', 'mɑ̃'], ['lø', 'ʒ', 'y', 'də', 'ɛ', 't', 'ʁɛ', 'a', 'si', 'd'], ['pɑ̃', 'dɑ̃', 'la', 'ʁe', 'y', 'ɔ̃', 'le', 'ɑ̃', 'fɑ̃', 'də', 'sa', 'sœ', 'ʁɔ̃', 'de', 'si', 'de', 'də', 'fɛ', 'ʁɛ̃', 'pɑ'], ['pu', 'ʁ', 'ʁɑ̃', 't', 'ʁe', 'dɑ̃', 'la', 'mɛ', 'zɔ̃', 'lɛ̃', 's', 'pɛ', 'k', 'tœ', 'ʁa', 'd', 'y', 'mɔ̃', 't', 'ʁe', 'sɔ̃', 'ba'], ['ɛ', 'la', 's', 'y', 'ɡ', 'ʒe', 'ʁe', 'a', 'sɔ̃', 'pə', 'ti', 'a', 'mi', 'ba', 'ʁ', 'b', 'y', 'də', 'sə', 'ʁa', 'ze'], ['i', 'la', 'di', 'a', 'sɔ̃', 'f', 'ʁɛ', 'ʁ', 'ci', 'l', 'də', 'vɛ', 'v', 'ʁɛ', 'mɑ̃', 'sə', 'ʁa', 'ze'], ['sɛ', 'si', 'd'], ['a', 'p', 'ʁɛ', 'sɛ', 't', 'ɡ', 'ʁo', 's', 'be', 'ti', 'zi', 'l', 'ʁu', 'ʒi', 'də', 'ɔ̃', 't'], ['mɔ̃', 'f', 'ʁɛ', 'ʁ', 'pɔ', 'se', 'dɛ', 'de', 'a', 'i', 'mo', 'di', 'fi', 'si', 'la', 'nu', 'ʁi', 'ʁ'], ['i', 'la', 'vɛ', 'pœ', 'ʁ', 'de', 'mi', 'k', 'ʁɔ', 'be', 'k', 'ʁɛ', 'ɛ', 'də', 'tɔ̃', 'be', 'ma', 'la', 'd'], ['bɛ', 'ʁ', 'na', 'ʁ', 'la', 'vɛ', 'a', 'kœ', 'ji', 'a', 'vɛ', 'kɛ̃', 'te', 'a', 'la', 'mɑ̃', 't'], ['ɑ̃', 'sa', 'p', 'ʁɔ', 'ʃɑ̃', 'd', 'y', 'n', 'ʁ', 'y', 'ʃɛ', 'la', 'e', 'te', 'pi', 'ce', 'pa', 'ʁ', 'y', 'na', 'bɛ', 'j'], ['le', 'pɔ', 'i', 's', 'je', 'na', 'vɛ', 'pa', 'd', 'y', 'tu', 'kɔ̃', 'p', 'ʁi', 'cɛ', 'l', 'se', 'tɛ', 'ʁɑ̃', 'kɔ̃', 't', 'ʁe'], ['ɑ̃', 'l', 'y', 'k', 'sɛ', 'ku', 'pe', 'lø', 'd', 'wa', 'a', 'vɛ', 'kɛ̃', 'ku', 'to'], ['la', 'mɛ', 'ʁɛ̃', 'cɛ', 'ta', 'vɛ', 'p', 'l', 'y', 'z', 'jœ', 'ʁ', 'bu', 'ʃa', 'nu', 'ʁi', 'ʁ'], ['dɛ', 'lœ', 'ʁ', 'ʁə', 'tu', 'ʁa', 'la', 'mɛ', 'zɔ̃', 'va', 'lɑ̃', 'tɛ̃', 'l', 'ɥi', 'a', 'dɔ', 'ne', 'y', 'n', 'sɛ', 'ʁ', 'v', 'jɛ', 't'], ['la', 'pə', 'ti', 't', 'fi', 'je', 'tɛ', 'kɔ̃', 'tɑ̃', 't', 'də', 'pɛ', 'ʁ', 'd', 'ʁ', 'sa', 'p', 'ʁə', 'ɛ', 'ʁ', 'dɑ̃'], ['le', 'ma', 'ʁ', 'ʃɑ̃', 'də', 'le', 'ɡ', 'y', 'm', 'nɔ̃', 'pa', 'la', 'bi', 't', 'y', 'd', 'də', 'fɛ', 'ʁ', 'la', 'ɛ', 'ʁ'], ['le', 'ci', 'p', 'sɛ', 'ɑ̃', 't', 'ʁɛ', 'ne', 'də', 't', 'ʁɛ', 'lɔ̃', 'ɡ', 'sə', 'mɛ', 'n', 's', 'y', 'ʁ', 'sɔ̃', 'ba', 'to'], ['o', 'p', 'ʁɛ̃', 'tɑ̃', 'dɑ̃', 'lø', 'ʒa', 'ʁ', 'dɛ̃', 'sɔ̃', 'pɛ', 'ʁ', 'tɔ̃', 'bɛ', 'la', 'pə', 'lu', 'z'], ['la', 'p', 'ʁɛ', 'mi', 'di', 'i', 'l', 's', 'y', 'pu', 'ʁ', 'k', 'wa', 'i', 'la', 'vɛ', 'ɔ̃', 't'], ['pu', 'ʁa', 'me', 'ɔ', 'ʁe', 'sɛ', 't', 'ʁə', 'sɛ', 'ti', 'l', 'fo', 'd', 'ʁɛ', 'a', 'ʒu', 'te', 'ɛ̃', 'pø', 'də', 's', 'y', 'k', 'ʁ'], ['də', 'lœ', 'ʁ', 'ʃɑ̃', 'b', 'ʁ', 'le', 'ɑ̃', 'fɑ̃', 'ɑ̃', 'tɑ̃', 'dɛ', 'b', 'jɛ̃', 'lø', 'b', 'ʁ', 'ɥi', 'də', 'la', 'fɛ', 't'], ['ɑ̃', 'ʁə', 'ɡa', 'ʁ', 'dɑ̃', 'sɔ̃', 'i', 'v', 'ʁi', 'l', 'də', 'vi', 'ne', 'lɛ', 's', 'pɛ', 's', 'də', 'la', 'bɛ', 'j'], ['la', 'v', 'jɛ', 'j', 'fa', 'm', 'mɛ', 'dɑ̃', 'sɔ̃', 'sa', 'kɛ̃', 'pə', 'ti', 'b', 'ʁi', 'cɛ', 'e', 'y', 'n', 'fu', 'ʁ', 'ʃɛ', 't'], ['pu', 'ʁu', 'v', 'ʁi', 'ʁ', 'la', 'pɔ', 'ʁ', 't', 'tɔ', 'mɑ', 'a', 'ti', 'ʁe', 'də', 'tu', 't', 'se', 'fɔ', 'ʁ', 's'], ['dɛ', 'sa', 'ma', 'ʒɔ', 'ʁi', 'te', 'i', 'l', 'se', 'tɛ', 'ɑ̃', 'ɡa', 'ʒe', 'dɑ̃', 'la', 'ʁ', 'me'], ['ʒ', 'y', 'a', 'e', 'tɛ', 'də', 'nu', 'vo', 'pa', 'ʁ', 'ti', 'ɑ̃', 'va', 'kɑ̃', 'sa', 'vɛ', 'k', 'sɔ̃', 'a', 'mi'], ['ɛ', 'l', 'nɛ', 'ɑ̃', 'kɔ', 'ʁ', 'ʒa', 'mɛ', 'pɑ', 'se', 'də', 'vɑ̃', 'la', 'bu', 'ʁ', 's'], ['lø', 'pə', 'ti', 'ɡa', 'ʁ', 'sɔ̃', 'mɑ̃', 'ʒ', 'sɔ̃', 's', 'tɛ', 'ka', 'vɛ', 'kɛ̃', 'ku', 'to', 'e', 'y', 'n', 'fu', 'ʁ', 'ʃɛ', 't'], ['e', 'mi', 'i', 'la', 'vɛ', 'de', 'tɛ', 's', 'te', 'dɛ', 'lø', 'mɔ', 'mɑ̃', 'u', 'ɛ', 'l', 'se', 'tɛ', 'ʁɑ̃', 'kɔ̃', 't', 'ʁe'], ['sa', 'a', 'di', 'mi', 'n', 'ɥe', 'e', 'ɛ', 'l', 'd', 'wa', 'a', 'ʃ', 'te', 'y', 'n', 'nu', 'vɛ', 'l', 'pɛ', 'ʁ', 'də', 'l', 'y', 'nɛ', 't'], ['sɔ̃', 'pɛ', 'ʁa', 'vɛ', 'dɑ̃', 'lø', 'tɑ̃', 'y', 'nɛ̃', 'k', 'ʁ', 'wa', 'ja', 'b', 'l', 'kɔ', 'lɛ', 'k', 's', 'jɔ̃', 'də', 'fɔ', 'tɔ', 's'], ['lø', 'pɛ', 'ʃœ', 'ʁɛ', 'mɛ', 'pa', 'se', 'la', 'ʒu', 'ʁ', 'ne', 'ɑ̃', 'mɛ', 'ʁ', 's', 'y', 'ʁ', 'sɔ̃', 'ba', 'to'], ['a', 'p', 'ʁɛ', 'y', 'n', 'ba', 'la', 'd', 'dɑ̃', 'le', 'ʃɑ̃', 'me', 'bɔ', 'te', 'tɛ', 'ku', 'vɛ', 'ʁ', 't', 'də', 'bu'], ['le', 'tɑ̃', 's', 'jɔ̃', 'ɑ̃', 't', 'ʁə', 'le', 'pɛ', 'i', 'pu', 'ʁɛ', 'a', 'bu', 'ti', 'ʁa', 'la', 'ɛ', 'ʁ'], ['ʒe', 'ɑ̃', 'tɑ̃', 'd', 'y', 'v', 'jɔ', 'le', 'ɛ̃', 'ʃa'], ['ɡa', 's', 'pa', 'ʁe', 'tɛ', 'pɛ', 'ʁ', 'd', 'ye', 'nə', 'sa', 'vɛ', 'pa', 'k', 'wa', 'fɛ', 'ʁ'], ['ɛ', 'la', 'a', 'ʃ', 'te', 'ɛ̃', 'po', 'də', 'f', 'lœ', 'ʁ', 'pu', 'ʁ', 'sɔ̃', 'a', 'i', 'vɛ', 'ʁ', 'sɛ', 'ʁ'], ['le', 'fo', 'ʒ', 'y', 'mo', 'e', 'tɛ', 'o', 'si', 'di', 'fe', 'ʁɑ̃', 'kə', 'lø', 'ʒu', 'ʁe', 'la', 'n', 'ɥi'], ['a', 'p', 'ʁɛ', 'sa', 'pa', 'nɛ̃', 'pa', 'sɑ̃', 'la', 'ɛ', 'de', 'a', 'pu', 'se', 'sa', 'v', 'wa', 't', 'y', 'ʁ'], ['pu', 'ʁa', 'v', 'wa', 'ʁ', 'pa', 'ʁ', 'ti', 'si', 'pe', 'a', 'la', 'ku', 'ʁ', 's', 'lɑ̃', 'fɑ̃', 'a', 'ʁə', 's', 'yɛ̃', 'ba'], ['le', 'ɔ', 'mɔ̃', 'fa', 'ji', 'tɔ̃', 'be', 'dɑ̃', 'lø', 't', 'ʁu', 'ci', 'la', 'vɛ', 'k', 'ʁø', 'ze'], ['i', 'l', 'va', 'a', 'ʃ', 'te', 'de', 'a', 'k', 's', 'jɔ̃', 'kɔ', 'te', 'ɑ̃', 'bu', 'ʁ', 's'], ['sə', 'p', 'la', 'ɛ', 'bo', 'ku', 't', 'ʁo', 'a', 'si', 'd'], ['la', 'bu', 'ti', 'k', 'da', 'k', 'sɛ', 's', 'wa', 'ʁe', 'tɛ', 'dɑ̃', 'ɛ̃', 'ka', 'ʁ', 't', 'je', 'e', 'l', 'wa', 'n', 'je', 'də', 'la', 'pu', 'l'], ['sɔ̃', 'a', 'mi', 'tu', 'ʒu', 'ʁa', 'dɔ', 'ʁe', 'mɑ̃', 'ʒe', 'de', 'pɔ', 'm'], ['le', 'o', 'e', 'tɛ', 'ʃɔ', 'ce', 'də', 'vɑ̃', 'sɛ', 't', 'ʃo', 'ze', 't', 'ʁɑ̃', 'ʒa', 'fɛ', 'ʁ'], ['i', 'l', 'l', 'ɥi', 'a', 'mi', 'ɛ̃', 'ku', 'to', 'su', 'la', 'ɡɔ', 'ʁ', 'ʒ'], ['ka', 'mi', 'je', 'tɛ', 't', 'ʁɛ', 's', 'y', 'ʁ', 'p', 'ʁi', 'z', 'kə', 'sɔ̃', 'ma', 'ʁi', 'nɛ', 'pɑ', 'ɡa', 'e'], ['fɛ', 'ta', 'tɑ̃', 's', 'jɔ̃', 'a', 'nə', 'pa', 'tu', 'ʃe', 'la', 'p', 'wa', 'l', 'kɑ̃', 'tɛ', 'lɛ', 'ʃo', 'd'], ['pu', 'ʁ', 'nə', 'pa', 'a', 'v', 'wa', 'ʁ', 'fɛ̃', 'i', 'lo', 'ʁɛ', 'd', 'y', 'mɑ̃', 'ʒe', 'p', 'l', 'y', 'vi', 't'], ['ɛ', 'la', 'ɑ̃', 'pɔ', 'ʁ', 'te', 'sɔ̃', 'nu', 'vɛ', 'la', 'pa', 'ʁɛ', 'j', 'pu', 'ʁ', 'p', 'ʁɑ̃', 'd', 'ʁ', 'də', 'bɛ', 'l', 'fɔ', 'tɔ', 's'], ['o', 'kɔ̃', 'sɛ', 'ʁ', 'də', 'ʁɔ', 'k', 'la', 'm', 'y', 'zi', 'ke', 'tɛ', 't', 'ʁo', 'fɔ', 'ʁ', 't'], ['ce', 'vi', 'ne', 'tɛ', 'de', 's', 'y', 'kə', 'sɔ̃', 'e', 'ci', 'p', 'də', 'ba', 's', 'cɛ', 't', 'nɛ', 'pa', 'ɡa', 'e'], ['a', 'p', 'ʁɛ', 'cɛ', 'l', 'k', 'm', 'wa', 'i', 'lɔ̃', 'ɑ̃', 'ɡa', 'ʒe', 'ɛ̃', 'ɔ', 'm', 'pu', 'ʁ', 'tu', 'la', 've'], ['a', 'p', 'ʁɛ', 'sɛ', 't', 'ʁ', 'la', 've', 'le', 'mɛ̃', 'na', 'tɑ̃', 'le', 'se', 'ʃe', 'a', 'vɛ', 'k', 'y', 'n', 'sɛ', 'ʁ', 'vo'], ['dɛ', 'la', 'p', 'ʁə', 'ɛ', 'ʁ', 'ɡu', 't', 'də', 'p', 'l', 'ɥi', 'ɛ', 'la', 'u', 'vɛ', 'ʁ', 'sɔ̃', 'pa', 'ʁa', 'p', 'li']]\n",
      "Decoded validation phoneme sequences: [['pu', 't'], ['la', 'də', 't'], ['le', 't'], ['pu', 'də', 'də', 't'], ['dɛ', 't'], ['la', 't', 'tɛ', 'a', 'kə', 'də', 'lø', 'kœ', 'nə', 'ji', 't'], ['a', 'ʁ'], ['wa', 'jɔ̃', 'nə', 'ʃe', 't', 'də', 'ti', 't'], ['la', 'tɛ', 'pa', 'nə', 't'], ['i', 't']]\n",
      "True validation phoneme sequences: [['pu', 'ʁ', 'fɛ', 'te', 'sɔ̃', 'de', 'pa', 'ʁ', 'se', 'a', 'mi', 'l', 'ɥi', 'ɔ̃', 'ɔ', 'ʁ', 'ɡa', 'i', 'ze', 'y', 'n', 'fɛ', 't'], ['lø', 'po', 'v', 'ʁɔ', 'm', 'sɛ̃', 'ce', 'tɛ', 'tu', 'ʒu', 'ʁ', 'də', 'pɛ', 'ʁ', 'd', 'ʁ', 'y', 'n', 'nu', 'vɛ', 'l', 'dɑ̃'], ['ɛ', 'lɛ', 'mɛ', 'bo', 'ku', 'la', 'i', 'bɛ', 'ʁ', 'te', 'kə', 'lœ', 'ʁɔ', 'f', 'ʁɛ', 'la', 'n', 'ɥi'], ['pu', 'ʁ', 'ʁɑ̃', 'd', 'ʁə', 'sɔ̃', 'ka', 'fe', 'm', 'wɛ̃', 'a', 'mɛ', 'ʁɛ', 'l', 'mi', 'ɛ̃', 'mɔ', 'ʁ', 'so', 'də', 's', 'y', 'k', 'ʁ'], ['le', 'dø', 'e', 't', 'y', 'd', 'jɑ̃', 'sə', 'sɔ̃', 'ʁɑ̃', 'kɔ̃', 't', 'ʁe', 'e', 'sɔ̃', 'ʁa', 'pi', 'd', 'mɑ̃', 'tɔ̃', 'be', 'a', 'mu', 'ʁø'], ['pɑ̃', 'dɑ̃', 'la', 'ʁɑ̃', 'dɔ', 'ne', 'le', 'p', 'je', 'də', 'i', 'la', 'l', 'ɥi', 'fə', 'zɛ', 'p', 'l', 'y', 'ma', 'la', 'ʃa', 'k', 'pɑ'], ['i', 'lɛ', 'm', 'lø', 'kɔ', 'mɛ', 'ʁ', 'se', 'a', 'de', 'si', 'de', 'du', 'v', 'ʁi', 'ʁɛ̃', 'ma', 'ɡa', 'zɛ̃', 'də', 'l', 'y', 'nɛ', 't'], ['k', 'ʁi', 'di', 'kə', 'sɔ̃', 'ʃ', 'jɛ̃', 'a', 'vɛ', 'tu', 'ʒu', 'ʁe', 'te', 'sɔ̃', 'mɛ', 'jœ', 'ʁa', 'mi'], ['a', 'tɑ̃', 'dɔ̃', 'nu', 'a', 'sə', 'kə', 'sɛ', 't', 'mɛ', 'zɔ̃', 'nə', 's', 'wa', 'pa', 'v', 'ʁɛ', 'mɑ̃', 'ʃo', 'd'], ['ɛ', 'l', 'tu', 'ʒu', 'ʁ', 'y', 'ma', 'l']]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(output, blank):\n",
    "    \"\"\"\n",
    "    Decode model outputs using a greedy decoder.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model outputs of shape (batch_size, sequence_length, num_classes).\n",
    "        blank (int): Index of the blank token.\n",
    "\n",
    "    Returns:\n",
    "        list: List of decoded sequences.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)  # Get the most likely class for each time step\n",
    "    decodes = []\n",
    "    for args in arg_maxes:\n",
    "        decode = []\n",
    "        previous_idx = None\n",
    "        for index in args:\n",
    "            if index != blank and (previous_idx is None or index != previous_idx):\n",
    "                decode.append(index.item())  # Append non-blank and non-repeated tokens\n",
    "            previous_idx = index\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "\n",
    "def decode_loader(model, loader, blank, index_to_phoneme):\n",
    "    \"\"\"\n",
    "    Decode outputs for all batches in a DataLoader and return both decoded and true sequences.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        loader (torch.utils.data.DataLoader): DataLoader containing input data and labels.\n",
    "        blank (int): Index of the blank token.\n",
    "        index_to_phoneme (dict): Mapping from indices to phonemes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (decoded_sequences, true_sequences), where:\n",
    "            - decoded_sequences: List of decoded phoneme sequences.\n",
    "            - true_sequences: List of true phoneme sequences.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_decoded_sequences = []\n",
    "    all_true_sequences = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, y_batch in loader:  # Iterate over batches (X_batch: inputs, y_batch: labels)\n",
    "            X_batch = X_batch.to(device)  # Move inputs to device\n",
    "            y_batch = y_batch.to(device)  # Move labels to device\n",
    "            outputs = model(X_batch)  # Get model predictions\n",
    "            decoded_phoneme_sequences = greedy_decoder(outputs, blank=blank)  # Decode outputs\n",
    "            decoded_phonemes = [[index_to_phoneme[idx] for idx in sequence] for sequence in decoded_phoneme_sequences]  # Convert indices to phonemes\n",
    "            all_decoded_sequences.extend(decoded_phonemes)  # Add to the list of decoded sequences\n",
    "\n",
    "            # Convert true labels to phoneme sequences\n",
    "            true_phoneme_sequences = [[index_to_phoneme[idx.item()] for idx in sequence if idx != blank and \n",
    "                                       index_to_phoneme[idx.item()] != \" \"] for sequence in y_batch]\n",
    "            all_true_sequences.extend(true_phoneme_sequences)  # Add to the list of true sequences\n",
    "\n",
    "    return all_decoded_sequences, all_true_sequences\n",
    "\n",
    "\n",
    "# Example usage\n",
    "blank_token = len(phoneme_to_index)  # Index of the blank token\n",
    "decoded_train_sequences, true_train_sequences = decode_loader(model, train_loader, blank_token, index_to_phoneme)\n",
    "decoded_val_sequences, true_val_sequences = decode_loader(model, val_loader, blank_token, index_to_phoneme)\n",
    "\n",
    "# Print results\n",
    "print(\"Decoded training phoneme sequences:\", decoded_train_sequences)\n",
    "print(\"True training phoneme sequences:\", true_train_sequences)\n",
    "print(\"Decoded validation phoneme sequences:\", decoded_val_sequences)\n",
    "print(\"True validation phoneme sequences:\", true_val_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PER (jiwer): 0.84232868405094 1 - PER:  0.15767131594906003\n",
      "Validation PER (jiwer): 0.9512195121951219 1 - PER:  0.04878048780487809\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_per_with_jiwer(decoded_sequences, true_sequences):\n",
    "    \"\"\"\n",
    "    Calculate the Phoneme Error Rate (PER) using jiwer.\n",
    "\n",
    "    Args:\n",
    "        decoded_sequences (list): List of decoded phoneme sequences.\n",
    "        true_sequences (list): List of true phoneme sequences.\n",
    "\n",
    "    Returns:\n",
    "        float: Phoneme Error Rate (PER).\n",
    "    \"\"\"\n",
    "    # Convert phoneme sequences to space-separated strings\n",
    "    decoded_str = [\" \".join(seq) for seq in decoded_sequences]\n",
    "    true_str = [\" \".join(seq) for seq in true_sequences]\n",
    "\n",
    "    # Calculate PER using jiwer\n",
    "    per = jiwer.wer(true_str, decoded_str)\n",
    "    return per\n",
    "\n",
    "# Example usage\n",
    "train_per = calculate_per_with_jiwer(decoded_train_sequences, true_train_sequences)\n",
    "val_per = calculate_per_with_jiwer(decoded_val_sequences, true_val_sequences)\n",
    "\n",
    "print(\"Training PER (jiwer):\", train_per, \"1 - PER: \", 1 - train_per)\n",
    "print(\"Validation PER (jiwer):\", val_per, \"1 - PER: \", 1 - val_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-jnt0UJEK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
