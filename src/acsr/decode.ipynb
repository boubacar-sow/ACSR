{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load CSV files from a directory based on a filename pattern\n",
    "def load_csv_files(directory, filename_pattern, type=\"position\"):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            df.fillna(0, inplace=True)\n",
    "            base_name = filename.split(f'_{type}_')[1].split('.csv')[0]\n",
    "            files_data[base_name] = df\n",
    "    return files_data\n",
    "\n",
    "# Find corresponding phoneme files based on the base names of position filenames\n",
    "def find_phoneme_files(directory, base_names):\n",
    "    phoneme_files = {}\n",
    "    for base_name in base_names:\n",
    "        phoneme_file = os.path.join(directory, f'{base_name}.csv')\n",
    "        if os.path.exists(phoneme_file):\n",
    "            phoneme_files[base_name] = phoneme_file\n",
    "    return phoneme_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================================\n",
    "# Helper Functions\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of sequences to pad.\n",
    "        max_length (int): Maximum length to pad to.\n",
    "        pad_value (int): Value to use for padding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded sequences.\n",
    "    \"\"\"\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.full((max_length - len(seq), seq.shape[1]), pad_value)\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "\n",
    "def extract_probabilities(data, columns):\n",
    "    \"\"\"\n",
    "    Extract and concatenate probabilities from multiple DataFrames.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of DataFrames.\n",
    "        columns (list): Columns to extract probabilities from.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Concatenated probabilities.\n",
    "    \"\"\"\n",
    "    data = [df.fillna(0) for df in data]\n",
    "    probs_list = [df[columns].to_numpy() for df in data]\n",
    "    return np.concatenate(probs_list, axis=0)\n",
    "\n",
    "\n",
    "def apply_phonotactic_rules(combinations):\n",
    "    \"\"\"\n",
    "    Apply phonotactic rules to filter invalid combinations.\n",
    "\n",
    "    Args:\n",
    "        combinations (list): List of phoneme combinations.\n",
    "\n",
    "    Returns:\n",
    "        list: Valid phoneme combinations.\n",
    "    \"\"\"\n",
    "    valid_combinations = []\n",
    "    for combination in combinations:\n",
    "        # Example phonotactic rules:\n",
    "        # 1. No consecutive vowels (e.g., \"ae\")\n",
    "        # 2. Certain consonant clusters are invalid (e.g., \"tl\")\n",
    "        if re.search(r\"[aeiouy]{2}\", combination):\n",
    "            continue  # Skip invalid combinations with consecutive vowels\n",
    "        if re.search(\n",
    "            r\"([s])\\1\", combination\n",
    "        ):  # Skip invalid combinations with double consonants\n",
    "            continue\n",
    "        valid_combinations.append(combination)\n",
    "    return valid_combinations\n",
    "\n",
    "\n",
    "def combine_sequences_with_padding(video_data):\n",
    "    \"\"\"\n",
    "    Combine sequences with padding to ensure uniform length.\n",
    "\n",
    "    Args:\n",
    "        video_data (dict): Dictionary containing video data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded input sequences (X) and padded labels (y).\n",
    "    \"\"\"\n",
    "    max_length = max(len(video_data[video][\"X\"]) for video in video_data)\n",
    "    X_padded = [\n",
    "        pad_sequences([video_data[video][\"X\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    y_padded = [\n",
    "        video_data[video][\"y\"]\n",
    "        + [phoneme_to_index[\" \"]] * (max_length - len(video_data[video][\"y\"]))\n",
    "        for video in video_data\n",
    "    ]\n",
    "    return X_padded, y_padded\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Data Preparation Functions\n",
    "# ==========================================================\n",
    "\n",
    "# Load phoneme-to-index mapping\n",
    "with open(\n",
    "    r\"C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\data\\phonelist.csv\", \"r\"\n",
    ") as file:\n",
    "    reader = csv.reader(file)\n",
    "    vocabulary_list = [row[0] for row in reader]\n",
    "\n",
    "\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(vocabulary_list)}\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "phoneme_to_index[\" \"] = len(phoneme_to_index)\n",
    "index_to_phoneme[len(index_to_phoneme)] = \" \"\n",
    "\n",
    "\n",
    "def prepare_data_for_videos_no_sliding_windows(\n",
    "    hand_position_data, hand_shape_data, phoneme_files\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data for all videos without sliding windows.\n",
    "\n",
    "    Args:\n",
    "        hand_position_data (dict): Dictionary of hand position data.\n",
    "        hand_shape_data (dict): Dictionary of hand shape data.\n",
    "        phoneme_files (dict): Dictionary of phoneme file paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing combined probabilities and phoneme indices.\n",
    "    \"\"\"\n",
    "    all_videos_data = {}\n",
    "    for base_name in hand_position_data:\n",
    "        if base_name in phoneme_files:\n",
    "            position_df = hand_position_data[base_name]\n",
    "            shape_df = hand_shape_data[base_name]\n",
    "            phoneme_file = phoneme_files[base_name]\n",
    "\n",
    "            # Extract probabilities\n",
    "            hand_position_probs = extract_probabilities(\n",
    "                [position_df],\n",
    "                [\"p_class_1\", \"p_class_2\", \"p_class_3\", \"p_class_4\", \"p_class_5\"],\n",
    "            )\n",
    "            hand_shape_probs = extract_probabilities(\n",
    "                [shape_df],\n",
    "                [\n",
    "                    \"p_class_1\",\n",
    "                    \"p_class_2\",\n",
    "                    \"p_class_3\",\n",
    "                    \"p_class_4\",\n",
    "                    \"p_class_5\",\n",
    "                    \"p_class_6\",\n",
    "                    \"p_class_7\",\n",
    "                    \"p_class_8\",\n",
    "                ],\n",
    "            )\n",
    "            combined_probs = np.concatenate(\n",
    "                (hand_position_probs, hand_shape_probs), axis=1\n",
    "            )\n",
    "\n",
    "            # Read phoneme sequences\n",
    "            with open(phoneme_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f)\n",
    "                phoneme_sequence = [row[0] for row in reader]\n",
    "\n",
    "            # Convert phoneme sequence to indices\n",
    "            phoneme_indices = [\n",
    "                phoneme_to_index[phoneme] for phoneme in phoneme_sequence\n",
    "            ]\n",
    "            all_videos_data[base_name] = {\"X\": combined_probs, \"y\": phoneme_indices}\n",
    "    return all_videos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of position files: 2\n",
      "Number of shape files: 2\n",
      "Number of phoneme files: 2\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Main Script\n",
    "# ==========================================================\n",
    "\n",
    "# Directories\n",
    "data_dir = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR\\output\\predictions'\n",
    "phoneme_dir = r'C:\\Users\\bouba\\Downloads\\CSF22\\CSF22\\CSF22_train\\train_labels'\n",
    "\n",
    "# Load position and shape data\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "hand_shape_data = load_csv_files(data_dir, 'predictions_rf_shape', type='shape')\n",
    "\n",
    "# Find phoneme files\n",
    "base_names = hand_position_data.keys()\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n",
    "\n",
    "# Print the number of files found\n",
    "print(f\"Number of position files: {len(hand_position_data)}\")\n",
    "print(f\"Number of shape files: {len(hand_shape_data)}\")\n",
    "print(f\"Number of phoneme files: {len(phoneme_files)}\")\n",
    "\n",
    "# Take only the first 5 videos for demonstration\n",
    "hand_position_data = {\n",
    "    key: hand_position_data[key] for key in list(hand_position_data.keys())[:5]\n",
    "}\n",
    "hand_shape_data = {\n",
    "    key: hand_shape_data[key] for key in list(hand_shape_data.keys())[:5]\n",
    "}\n",
    "phoneme_files = {key: phoneme_files[key] for key in list(phoneme_files.keys())[:5]}\n",
    "\n",
    "# Prepare data\n",
    "all_videos_data = prepare_data_for_videos_no_sliding_windows(\n",
    "    hand_position_data, hand_shape_data, phoneme_files\n",
    ")\n",
    "X_combined, y_combined = combine_sequences_with_padding(all_videos_data)\n",
    "\n",
    "# Convert phoneme sequences to tensors\n",
    "y_tensors = [\n",
    "    torch.tensor([index for index in video_data[\"y\"]], dtype=torch.long)\n",
    "    for video_data in all_videos_data.values()\n",
    "]\n",
    "all_videos_data = {\n",
    "    key: {\"X\": video_data[\"X\"], \"y\": y_tensors[i]}\n",
    "    for i, (key, video_data) in enumerate(all_videos_data.items())\n",
    "}\n",
    "\n",
    "# Combine all data into tensors\n",
    "X_combined = torch.tensor(np.array(X_combined), dtype=torch.float32) \n",
    "y_combined = torch.tensor(y_combined, dtype=torch.long)\n",
    "\n",
    "# Normalize data (optional)\n",
    "# X_combined = (X_combined - X_combined.mean()) / X_combined.std()\n",
    "\n",
    "# Final organized data\n",
    "all_videos_data = {\"X\": X_combined, \"y\": y_combined}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([1, 257, 13])\n",
      "Batch y shape: torch.Size([1, 257])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to split data into training and validation sets\n",
    "def train_val_split(data, train_ratio=0.8):\n",
    "    num_samples = len(data['X'])\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    \n",
    "    train_data = {\n",
    "        'X': data['X'][:split_idx],\n",
    "        'y': data['y'][:split_idx]\n",
    "    }\n",
    "    val_data = {\n",
    "        'X': data['X'][split_idx:],\n",
    "        'y': data['y'][split_idx:]\n",
    "    }\n",
    "    return train_data, val_data\n",
    "\n",
    "# Convert data to DataLoader format\n",
    "def data_to_dataloader(data, batch_size=4, shuffle=True):\n",
    "    X_tensors = data['X']\n",
    "    y_tensors = data['y']\n",
    "    \n",
    "    # Create a TensorDataset with both inputs and labels\n",
    "    dataset = TensorDataset(X_tensors, y_tensors)\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_val_split(all_videos_data)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = data_to_dataloader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = data_to_dataloader(val_data, batch_size=4, shuffle=False)\n",
    "\n",
    "# Check the DataLoader output\n",
    "for batch_X, batch_y in train_loader:\n",
    "    print(\"Batch X shape:\", batch_X.shape)\n",
    "    print(\"Batch y shape:\", batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 60.435726165771484\n",
      "Epoch 2/1000, Loss: 59.404659271240234\n",
      "Epoch 3/1000, Loss: 58.17195510864258\n",
      "Epoch 4/1000, Loss: 56.66895294189453\n",
      "Epoch 5/1000, Loss: 54.7713737487793\n",
      "Epoch 6/1000, Loss: 52.272186279296875\n",
      "Epoch 7/1000, Loss: 48.819602966308594\n",
      "Epoch 8/1000, Loss: 43.8242301940918\n",
      "Epoch 9/1000, Loss: 36.3961181640625\n",
      "Epoch 10/1000, Loss: 25.830293655395508\n",
      "Epoch 11/1000, Loss: 14.042709350585938\n",
      "Epoch 12/1000, Loss: 6.298023700714111\n",
      "Epoch 13/1000, Loss: 3.7357428073883057\n",
      "Epoch 14/1000, Loss: 3.502570867538452\n",
      "Epoch 15/1000, Loss: 4.014628887176514\n",
      "Epoch 16/1000, Loss: 4.529301166534424\n",
      "Epoch 17/1000, Loss: 4.8412604331970215\n",
      "Epoch 18/1000, Loss: 4.997861385345459\n",
      "Epoch 19/1000, Loss: 5.043270111083984\n",
      "Epoch 20/1000, Loss: 5.0059027671813965\n",
      "Epoch 21/1000, Loss: 4.905983924865723\n",
      "Epoch 22/1000, Loss: 4.758982181549072\n",
      "Epoch 23/1000, Loss: 4.577328681945801\n",
      "Epoch 24/1000, Loss: 4.371266841888428\n",
      "Epoch 25/1000, Loss: 4.149406909942627\n",
      "Epoch 26/1000, Loss: 3.91947603225708\n",
      "Epoch 27/1000, Loss: 3.688966989517212\n",
      "Epoch 28/1000, Loss: 3.465670347213745\n",
      "Epoch 29/1000, Loss: 3.2580487728118896\n",
      "Epoch 30/1000, Loss: 3.0753161907196045\n",
      "Epoch 31/1000, Loss: 2.9268951416015625\n",
      "Epoch 32/1000, Loss: 2.821013927459717\n",
      "Epoch 33/1000, Loss: 2.7621707916259766\n",
      "Epoch 34/1000, Loss: 2.7480833530426025\n",
      "Epoch 35/1000, Loss: 2.7679378986358643\n",
      "Epoch 36/1000, Loss: 2.8046114444732666\n",
      "Epoch 37/1000, Loss: 2.8402655124664307\n",
      "Epoch 38/1000, Loss: 2.861691951751709\n",
      "Epoch 39/1000, Loss: 2.8625190258026123\n",
      "Epoch 40/1000, Loss: 2.8429505825042725\n",
      "Epoch 41/1000, Loss: 2.808032274246216\n",
      "Epoch 42/1000, Loss: 2.765218734741211\n",
      "Epoch 43/1000, Loss: 2.7221508026123047\n",
      "Epoch 44/1000, Loss: 2.6849477291107178\n",
      "Epoch 45/1000, Loss: 2.6573379039764404\n",
      "Epoch 46/1000, Loss: 2.640545129776001\n",
      "Epoch 47/1000, Loss: 2.633812427520752\n",
      "Epoch 48/1000, Loss: 2.6350765228271484\n",
      "Epoch 49/1000, Loss: 2.6416478157043457\n",
      "Epoch 50/1000, Loss: 2.6508216857910156\n",
      "Epoch 51/1000, Loss: 2.6601812839508057\n",
      "Epoch 52/1000, Loss: 2.6678967475891113\n",
      "Epoch 53/1000, Loss: 2.6726949214935303\n",
      "Epoch 54/1000, Loss: 2.6739399433135986\n",
      "Epoch 55/1000, Loss: 2.671513795852661\n",
      "Epoch 56/1000, Loss: 2.6657676696777344\n",
      "Epoch 57/1000, Loss: 2.6573729515075684\n",
      "Epoch 58/1000, Loss: 2.6472456455230713\n",
      "Epoch 59/1000, Loss: 2.6364052295684814\n",
      "Epoch 60/1000, Loss: 2.6258678436279297\n",
      "Epoch 61/1000, Loss: 2.616520404815674\n",
      "Epoch 62/1000, Loss: 2.6090567111968994\n",
      "Epoch 63/1000, Loss: 2.603821277618408\n",
      "Epoch 64/1000, Loss: 2.600874662399292\n",
      "Epoch 65/1000, Loss: 2.5999367237091064\n",
      "Epoch 66/1000, Loss: 2.600466251373291\n",
      "Epoch 67/1000, Loss: 2.6017768383026123\n",
      "Epoch 68/1000, Loss: 2.6031718254089355\n",
      "Epoch 69/1000, Loss: 2.6040573120117188\n",
      "Epoch 70/1000, Loss: 2.60406494140625\n",
      "Epoch 71/1000, Loss: 2.6030690670013428\n",
      "Epoch 72/1000, Loss: 2.601175546646118\n",
      "Epoch 73/1000, Loss: 2.5986711978912354\n",
      "Epoch 74/1000, Loss: 2.5959227085113525\n",
      "Epoch 75/1000, Loss: 2.5932509899139404\n",
      "Epoch 76/1000, Loss: 2.590923309326172\n",
      "Epoch 77/1000, Loss: 2.5890533924102783\n",
      "Epoch 78/1000, Loss: 2.5876500606536865\n",
      "Epoch 79/1000, Loss: 2.5866410732269287\n",
      "Epoch 80/1000, Loss: 2.5858638286590576\n",
      "Epoch 81/1000, Loss: 2.585167169570923\n",
      "Epoch 82/1000, Loss: 2.5844004154205322\n",
      "Epoch 83/1000, Loss: 2.583465814590454\n",
      "Epoch 84/1000, Loss: 2.5823304653167725\n",
      "Epoch 85/1000, Loss: 2.580993413925171\n",
      "Epoch 86/1000, Loss: 2.5795207023620605\n",
      "Epoch 87/1000, Loss: 2.5779800415039062\n",
      "Epoch 88/1000, Loss: 2.5764503479003906\n",
      "Epoch 89/1000, Loss: 2.575000762939453\n",
      "Epoch 90/1000, Loss: 2.5736587047576904\n",
      "Epoch 91/1000, Loss: 2.5724446773529053\n",
      "Epoch 92/1000, Loss: 2.5713210105895996\n",
      "Epoch 93/1000, Loss: 2.5702528953552246\n",
      "Epoch 94/1000, Loss: 2.56919527053833\n",
      "Epoch 95/1000, Loss: 2.568082332611084\n",
      "Epoch 96/1000, Loss: 2.5668790340423584\n",
      "Epoch 97/1000, Loss: 2.5655770301818848\n",
      "Epoch 98/1000, Loss: 2.5641682147979736\n",
      "Epoch 99/1000, Loss: 2.5626883506774902\n",
      "Epoch 100/1000, Loss: 2.5611584186553955\n",
      "Epoch 101/1000, Loss: 2.5596089363098145\n",
      "Epoch 102/1000, Loss: 2.5580639839172363\n",
      "Epoch 103/1000, Loss: 2.556536912918091\n",
      "Epoch 104/1000, Loss: 2.5550193786621094\n",
      "Epoch 105/1000, Loss: 2.553520917892456\n",
      "Epoch 106/1000, Loss: 2.552006244659424\n",
      "Epoch 107/1000, Loss: 2.5504679679870605\n",
      "Epoch 108/1000, Loss: 2.548874616622925\n",
      "Epoch 109/1000, Loss: 2.5472331047058105\n",
      "Epoch 110/1000, Loss: 2.545515537261963\n",
      "Epoch 111/1000, Loss: 2.5437400341033936\n",
      "Epoch 112/1000, Loss: 2.5418930053710938\n",
      "Epoch 113/1000, Loss: 2.5399882793426514\n",
      "Epoch 114/1000, Loss: 2.5380358695983887\n",
      "Epoch 115/1000, Loss: 2.536039113998413\n",
      "Epoch 116/1000, Loss: 2.533985137939453\n",
      "Epoch 117/1000, Loss: 2.531898260116577\n",
      "Epoch 118/1000, Loss: 2.5297584533691406\n",
      "Epoch 119/1000, Loss: 2.5275537967681885\n",
      "Epoch 120/1000, Loss: 2.52528715133667\n",
      "Epoch 121/1000, Loss: 2.5229480266571045\n",
      "Epoch 122/1000, Loss: 2.5205252170562744\n",
      "Epoch 123/1000, Loss: 2.518019914627075\n",
      "Epoch 124/1000, Loss: 2.5154192447662354\n",
      "Epoch 125/1000, Loss: 2.512747287750244\n",
      "Epoch 126/1000, Loss: 2.509976863861084\n",
      "Epoch 127/1000, Loss: 2.5071237087249756\n",
      "Epoch 128/1000, Loss: 2.5041730403900146\n",
      "Epoch 129/1000, Loss: 2.501134157180786\n",
      "Epoch 130/1000, Loss: 2.497997760772705\n",
      "Epoch 131/1000, Loss: 2.4947705268859863\n",
      "Epoch 132/1000, Loss: 2.491436719894409\n",
      "Epoch 133/1000, Loss: 2.487996816635132\n",
      "Epoch 134/1000, Loss: 2.4844510555267334\n",
      "Epoch 135/1000, Loss: 2.4807870388031006\n",
      "Epoch 136/1000, Loss: 2.4770119190216064\n",
      "Epoch 137/1000, Loss: 2.473127603530884\n",
      "Epoch 138/1000, Loss: 2.4691147804260254\n",
      "Epoch 139/1000, Loss: 2.4649930000305176\n",
      "Epoch 140/1000, Loss: 2.460744619369507\n",
      "Epoch 141/1000, Loss: 2.4563865661621094\n",
      "Epoch 142/1000, Loss: 2.4519193172454834\n",
      "Epoch 143/1000, Loss: 2.4473204612731934\n",
      "Epoch 144/1000, Loss: 2.4426090717315674\n",
      "Epoch 145/1000, Loss: 2.4377810955047607\n",
      "Epoch 146/1000, Loss: 2.4328417778015137\n",
      "Epoch 147/1000, Loss: 2.427774429321289\n",
      "Epoch 148/1000, Loss: 2.4225940704345703\n",
      "Epoch 149/1000, Loss: 2.417307138442993\n",
      "Epoch 150/1000, Loss: 2.411909341812134\n",
      "Epoch 151/1000, Loss: 2.4064126014709473\n",
      "Epoch 152/1000, Loss: 2.4008209705352783\n",
      "Epoch 153/1000, Loss: 2.395132064819336\n",
      "Epoch 154/1000, Loss: 2.389338970184326\n",
      "Epoch 155/1000, Loss: 2.383434534072876\n",
      "Epoch 156/1000, Loss: 2.3774302005767822\n",
      "Epoch 157/1000, Loss: 2.3713412284851074\n",
      "Epoch 158/1000, Loss: 2.365166187286377\n",
      "Epoch 159/1000, Loss: 2.358903169631958\n",
      "Epoch 160/1000, Loss: 2.3525631427764893\n",
      "Epoch 161/1000, Loss: 2.3461241722106934\n",
      "Epoch 162/1000, Loss: 2.3396050930023193\n",
      "Epoch 163/1000, Loss: 2.3330206871032715\n",
      "Epoch 164/1000, Loss: 2.3263638019561768\n",
      "Epoch 165/1000, Loss: 2.3196327686309814\n",
      "Epoch 166/1000, Loss: 2.31282114982605\n",
      "Epoch 167/1000, Loss: 2.305959463119507\n",
      "Epoch 168/1000, Loss: 2.299060344696045\n",
      "Epoch 169/1000, Loss: 2.292128801345825\n",
      "Epoch 170/1000, Loss: 2.285177707672119\n",
      "Epoch 171/1000, Loss: 2.2782533168792725\n",
      "Epoch 172/1000, Loss: 2.2713844776153564\n",
      "Epoch 173/1000, Loss: 2.264585494995117\n",
      "Epoch 174/1000, Loss: 2.257864475250244\n",
      "Epoch 175/1000, Loss: 2.251221179962158\n",
      "Epoch 176/1000, Loss: 2.2446510791778564\n",
      "Epoch 177/1000, Loss: 2.238131284713745\n",
      "Epoch 178/1000, Loss: 2.231666088104248\n",
      "Epoch 179/1000, Loss: 2.2252585887908936\n",
      "Epoch 180/1000, Loss: 2.2189157009124756\n",
      "Epoch 181/1000, Loss: 2.2126481533050537\n",
      "Epoch 182/1000, Loss: 2.2064731121063232\n",
      "Epoch 183/1000, Loss: 2.200378894805908\n",
      "Epoch 184/1000, Loss: 2.1943624019622803\n",
      "Epoch 185/1000, Loss: 2.1883718967437744\n",
      "Epoch 186/1000, Loss: 2.1822962760925293\n",
      "Epoch 187/1000, Loss: 2.1758816242218018\n",
      "Epoch 188/1000, Loss: 2.169733762741089\n",
      "Epoch 189/1000, Loss: 2.181178331375122\n",
      "Epoch 190/1000, Loss: 2.15433669090271\n",
      "Epoch 191/1000, Loss: 2.1593711376190186\n",
      "Epoch 192/1000, Loss: 2.1611716747283936\n",
      "Epoch 193/1000, Loss: 2.1490492820739746\n",
      "Epoch 194/1000, Loss: 2.1392159461975098\n",
      "Epoch 195/1000, Loss: 2.1276941299438477\n",
      "Epoch 196/1000, Loss: 2.110729932785034\n",
      "Epoch 197/1000, Loss: 2.1058146953582764\n",
      "Epoch 198/1000, Loss: 2.0913190841674805\n",
      "Epoch 199/1000, Loss: 2.086672782897949\n",
      "Epoch 200/1000, Loss: 2.0760855674743652\n",
      "Epoch 201/1000, Loss: 2.0708727836608887\n",
      "Epoch 202/1000, Loss: 2.0641062259674072\n",
      "Epoch 203/1000, Loss: 2.0556111335754395\n",
      "Epoch 204/1000, Loss: 2.0475246906280518\n",
      "Epoch 205/1000, Loss: 2.0410053730010986\n",
      "Epoch 206/1000, Loss: 2.033519983291626\n",
      "Epoch 207/1000, Loss: 2.0247833728790283\n",
      "Epoch 208/1000, Loss: 2.0187489986419678\n",
      "Epoch 209/1000, Loss: 2.0140810012817383\n",
      "Epoch 210/1000, Loss: 2.006528377532959\n",
      "Epoch 211/1000, Loss: 2.0003249645233154\n",
      "Epoch 212/1000, Loss: 1.993552327156067\n",
      "Epoch 213/1000, Loss: 1.9871848821640015\n",
      "Epoch 214/1000, Loss: 1.980366587638855\n",
      "Epoch 215/1000, Loss: 1.9732277393341064\n",
      "Epoch 216/1000, Loss: 1.9664489030838013\n",
      "Epoch 217/1000, Loss: 1.9589319229125977\n",
      "Epoch 218/1000, Loss: 1.9518908262252808\n",
      "Epoch 219/1000, Loss: 1.9446557760238647\n",
      "Epoch 220/1000, Loss: 1.9377353191375732\n",
      "Epoch 221/1000, Loss: 1.9469631910324097\n",
      "Epoch 222/1000, Loss: 2.0162386894226074\n",
      "Epoch 223/1000, Loss: 2.006300210952759\n",
      "Epoch 224/1000, Loss: 1.981157660484314\n",
      "Epoch 225/1000, Loss: 1.9625952243804932\n",
      "Epoch 226/1000, Loss: 1.962020754814148\n",
      "Epoch 227/1000, Loss: 1.9559262990951538\n",
      "Epoch 228/1000, Loss: 1.9311436414718628\n",
      "Epoch 229/1000, Loss: 1.9139760732650757\n",
      "Epoch 230/1000, Loss: 1.8998963832855225\n",
      "Epoch 231/1000, Loss: 1.8891249895095825\n",
      "Epoch 232/1000, Loss: 1.8800657987594604\n",
      "Epoch 233/1000, Loss: 1.8907932043075562\n",
      "Epoch 234/1000, Loss: 1.8859838247299194\n",
      "Epoch 235/1000, Loss: 1.8816258907318115\n",
      "Epoch 236/1000, Loss: 1.8715541362762451\n",
      "Epoch 237/1000, Loss: 1.85980224609375\n",
      "Epoch 238/1000, Loss: 1.8515247106552124\n",
      "Epoch 239/1000, Loss: 1.8450132608413696\n",
      "Epoch 240/1000, Loss: 1.8393161296844482\n",
      "Epoch 241/1000, Loss: 1.8337113857269287\n",
      "Epoch 242/1000, Loss: 1.8275935649871826\n",
      "Epoch 243/1000, Loss: 1.8206382989883423\n",
      "Epoch 244/1000, Loss: 1.8130028247833252\n",
      "Epoch 245/1000, Loss: 1.8053953647613525\n",
      "Epoch 246/1000, Loss: 1.7984479665756226\n",
      "Epoch 247/1000, Loss: 1.7911081314086914\n",
      "Epoch 248/1000, Loss: 1.7831510305404663\n",
      "Epoch 249/1000, Loss: 1.7759020328521729\n",
      "Epoch 250/1000, Loss: 1.7693010568618774\n",
      "Epoch 251/1000, Loss: 1.7628519535064697\n",
      "Epoch 252/1000, Loss: 1.7563093900680542\n",
      "Epoch 253/1000, Loss: 1.7496389150619507\n",
      "Epoch 254/1000, Loss: 1.7429343461990356\n",
      "Epoch 255/1000, Loss: 1.736385703086853\n",
      "Epoch 256/1000, Loss: 1.7301934957504272\n",
      "Epoch 257/1000, Loss: 1.7243438959121704\n",
      "Epoch 258/1000, Loss: 1.7184356451034546\n",
      "Epoch 259/1000, Loss: 1.7122504711151123\n",
      "Epoch 260/1000, Loss: 1.7061223983764648\n",
      "Epoch 261/1000, Loss: 1.700256109237671\n",
      "Epoch 262/1000, Loss: 1.694564938545227\n",
      "Epoch 263/1000, Loss: 1.688936471939087\n",
      "Epoch 264/1000, Loss: 1.683323860168457\n",
      "Epoch 265/1000, Loss: 1.6777842044830322\n",
      "Epoch 266/1000, Loss: 1.6723978519439697\n",
      "Epoch 267/1000, Loss: 1.6669362783432007\n",
      "Epoch 268/1000, Loss: 1.6768313646316528\n",
      "Epoch 269/1000, Loss: 1.6572870016098022\n",
      "Epoch 270/1000, Loss: 1.6532469987869263\n",
      "Epoch 271/1000, Loss: 1.649082899093628\n",
      "Epoch 272/1000, Loss: 1.644750952720642\n",
      "Epoch 273/1000, Loss: 1.6401939392089844\n",
      "Epoch 274/1000, Loss: 1.6355257034301758\n",
      "Epoch 275/1000, Loss: 1.6308833360671997\n",
      "Epoch 276/1000, Loss: 1.6262308359146118\n",
      "Epoch 277/1000, Loss: 1.6215744018554688\n",
      "Epoch 278/1000, Loss: 1.6170570850372314\n",
      "Epoch 279/1000, Loss: 1.6126362085342407\n",
      "Epoch 280/1000, Loss: 1.6081435680389404\n",
      "Epoch 281/1000, Loss: 1.603481650352478\n",
      "Epoch 282/1000, Loss: 1.5987389087677002\n",
      "Epoch 283/1000, Loss: 1.5992223024368286\n",
      "Epoch 284/1000, Loss: 1.590597152709961\n",
      "Epoch 285/1000, Loss: 1.5870325565338135\n",
      "Epoch 286/1000, Loss: 1.5832910537719727\n",
      "Epoch 287/1000, Loss: 1.5792943239212036\n",
      "Epoch 288/1000, Loss: 1.5749951601028442\n",
      "Epoch 289/1000, Loss: 1.5703507661819458\n",
      "Epoch 290/1000, Loss: 1.570740818977356\n",
      "Epoch 291/1000, Loss: 1.5635117292404175\n",
      "Epoch 292/1000, Loss: 1.560577630996704\n",
      "Epoch 293/1000, Loss: 1.5565845966339111\n",
      "Epoch 294/1000, Loss: 1.5534754991531372\n",
      "Epoch 295/1000, Loss: 1.5511207580566406\n",
      "Epoch 296/1000, Loss: 1.5476739406585693\n",
      "Epoch 297/1000, Loss: 1.5434690713882446\n",
      "Epoch 298/1000, Loss: 1.5385912656784058\n",
      "Epoch 299/1000, Loss: 1.5356968641281128\n",
      "Epoch 300/1000, Loss: 1.5317895412445068\n",
      "Epoch 301/1000, Loss: 1.5260303020477295\n",
      "Epoch 302/1000, Loss: 1.5215457677841187\n",
      "Epoch 303/1000, Loss: 1.5175281763076782\n",
      "Epoch 304/1000, Loss: 1.5143814086914062\n",
      "Epoch 305/1000, Loss: 1.5099849700927734\n",
      "Epoch 306/1000, Loss: 1.5060579776763916\n",
      "Epoch 307/1000, Loss: 1.5006544589996338\n",
      "Epoch 308/1000, Loss: 1.4954359531402588\n",
      "Epoch 309/1000, Loss: 1.4892956018447876\n",
      "Epoch 310/1000, Loss: 1.4845333099365234\n",
      "Epoch 311/1000, Loss: 1.4794203042984009\n",
      "Epoch 312/1000, Loss: 1.4759817123413086\n",
      "Epoch 313/1000, Loss: 1.4713366031646729\n",
      "Epoch 314/1000, Loss: 1.4683525562286377\n",
      "Epoch 315/1000, Loss: 1.4635207653045654\n",
      "Epoch 316/1000, Loss: 1.4575881958007812\n",
      "Epoch 317/1000, Loss: 1.4504280090332031\n",
      "Epoch 318/1000, Loss: 1.4489048719406128\n",
      "Epoch 319/1000, Loss: 1.4413092136383057\n",
      "Epoch 320/1000, Loss: 1.4394077062606812\n",
      "Epoch 321/1000, Loss: 1.4339218139648438\n",
      "Epoch 322/1000, Loss: 1.426349401473999\n",
      "Epoch 323/1000, Loss: 1.4199715852737427\n",
      "Epoch 324/1000, Loss: 1.4151227474212646\n",
      "Epoch 325/1000, Loss: 1.4075504541397095\n",
      "Epoch 326/1000, Loss: 1.4008709192276\n",
      "Epoch 327/1000, Loss: 1.3959218263626099\n",
      "Epoch 328/1000, Loss: 1.3887245655059814\n",
      "Epoch 329/1000, Loss: 1.3863351345062256\n",
      "Epoch 330/1000, Loss: 1.377515196800232\n",
      "Epoch 331/1000, Loss: 1.3725649118423462\n",
      "Epoch 332/1000, Loss: 1.3685237169265747\n",
      "Epoch 333/1000, Loss: 1.3605890274047852\n",
      "Epoch 334/1000, Loss: 1.3533331155776978\n",
      "Epoch 335/1000, Loss: 1.347802996635437\n",
      "Epoch 336/1000, Loss: 1.3435529470443726\n",
      "Epoch 337/1000, Loss: 1.335928201675415\n",
      "Epoch 338/1000, Loss: 1.3288145065307617\n",
      "Epoch 339/1000, Loss: 1.3257195949554443\n",
      "Epoch 340/1000, Loss: 1.3158597946166992\n",
      "Epoch 341/1000, Loss: 1.315259575843811\n",
      "Epoch 342/1000, Loss: 1.3155937194824219\n",
      "Epoch 343/1000, Loss: 1.3079534769058228\n",
      "Epoch 344/1000, Loss: 1.314693808555603\n",
      "Epoch 345/1000, Loss: 1.305947184562683\n",
      "Epoch 346/1000, Loss: 1.3040446043014526\n",
      "Epoch 347/1000, Loss: 1.3030261993408203\n",
      "Epoch 348/1000, Loss: 1.2853144407272339\n",
      "Epoch 349/1000, Loss: 1.281530737876892\n",
      "Epoch 350/1000, Loss: 1.2800430059432983\n",
      "Epoch 351/1000, Loss: 1.2761648893356323\n",
      "Epoch 352/1000, Loss: 1.2690918445587158\n",
      "Epoch 353/1000, Loss: 1.2587941884994507\n",
      "Epoch 354/1000, Loss: 1.2509605884552002\n",
      "Epoch 355/1000, Loss: 1.2431570291519165\n",
      "Epoch 356/1000, Loss: 1.2378278970718384\n",
      "Epoch 357/1000, Loss: 1.2332046031951904\n",
      "Epoch 358/1000, Loss: 1.2230936288833618\n",
      "Epoch 359/1000, Loss: 1.2143151760101318\n",
      "Epoch 360/1000, Loss: 1.2094801664352417\n",
      "Epoch 361/1000, Loss: 1.2062283754348755\n",
      "Epoch 362/1000, Loss: 1.1985728740692139\n",
      "Epoch 363/1000, Loss: 1.1908224821090698\n",
      "Epoch 364/1000, Loss: 1.1846809387207031\n",
      "Epoch 365/1000, Loss: 1.17731511592865\n",
      "Epoch 366/1000, Loss: 1.1728686094284058\n",
      "Epoch 367/1000, Loss: 1.1644798517227173\n",
      "Epoch 368/1000, Loss: 1.1571727991104126\n",
      "Epoch 369/1000, Loss: 1.152936577796936\n",
      "Epoch 370/1000, Loss: 1.1475846767425537\n",
      "Epoch 371/1000, Loss: 1.1399407386779785\n",
      "Epoch 372/1000, Loss: 1.1331031322479248\n",
      "Epoch 373/1000, Loss: 1.1280869245529175\n",
      "Epoch 374/1000, Loss: 1.1201978921890259\n",
      "Epoch 375/1000, Loss: 1.1137374639511108\n",
      "Epoch 376/1000, Loss: 1.1091786623001099\n",
      "Epoch 377/1000, Loss: 1.1027981042861938\n",
      "Epoch 378/1000, Loss: 1.0957551002502441\n",
      "Epoch 379/1000, Loss: 1.0903311967849731\n",
      "Epoch 380/1000, Loss: 1.0834940671920776\n",
      "Epoch 381/1000, Loss: 1.0756075382232666\n",
      "Epoch 382/1000, Loss: 1.0685166120529175\n",
      "Epoch 383/1000, Loss: 1.0609830617904663\n",
      "Epoch 384/1000, Loss: 1.056822657585144\n",
      "Epoch 385/1000, Loss: 1.0457316637039185\n",
      "Epoch 386/1000, Loss: 1.0618618726730347\n",
      "Epoch 387/1000, Loss: 1.0735728740692139\n",
      "Epoch 388/1000, Loss: 1.0855973958969116\n",
      "Epoch 389/1000, Loss: 1.0827696323394775\n",
      "Epoch 390/1000, Loss: 1.0651390552520752\n",
      "Epoch 391/1000, Loss: 1.0558497905731201\n",
      "Epoch 392/1000, Loss: 1.0459450483322144\n",
      "Epoch 393/1000, Loss: 1.0502926111221313\n",
      "Epoch 394/1000, Loss: 1.0509607791900635\n",
      "Epoch 395/1000, Loss: 1.0462216138839722\n",
      "Epoch 396/1000, Loss: 1.0380375385284424\n",
      "Epoch 397/1000, Loss: 1.0302770137786865\n",
      "Epoch 398/1000, Loss: 1.024572491645813\n",
      "Epoch 399/1000, Loss: 1.018583059310913\n",
      "Epoch 400/1000, Loss: 1.012692928314209\n",
      "Epoch 401/1000, Loss: 1.0048291683197021\n",
      "Epoch 402/1000, Loss: 0.9959278106689453\n",
      "Epoch 403/1000, Loss: 0.9864775538444519\n",
      "Epoch 404/1000, Loss: 0.979642927646637\n",
      "Epoch 405/1000, Loss: 0.9770411849021912\n",
      "Epoch 406/1000, Loss: 0.9658752679824829\n",
      "Epoch 407/1000, Loss: 0.9594786763191223\n",
      "Epoch 408/1000, Loss: 0.9548860788345337\n",
      "Epoch 409/1000, Loss: 0.9469688534736633\n",
      "Epoch 410/1000, Loss: 0.946597158908844\n",
      "Epoch 411/1000, Loss: 0.9381187558174133\n",
      "Epoch 412/1000, Loss: 0.936389148235321\n",
      "Epoch 413/1000, Loss: 0.9306074380874634\n",
      "Epoch 414/1000, Loss: 0.9256482124328613\n",
      "Epoch 415/1000, Loss: 0.9197079539299011\n",
      "Epoch 416/1000, Loss: 0.9144792556762695\n",
      "Epoch 417/1000, Loss: 0.91377192735672\n",
      "Epoch 418/1000, Loss: 0.9060195684432983\n",
      "Epoch 419/1000, Loss: 0.9026294350624084\n",
      "Epoch 420/1000, Loss: 0.8936341404914856\n",
      "Epoch 421/1000, Loss: 0.8910151124000549\n",
      "Epoch 422/1000, Loss: 0.8862454295158386\n",
      "Epoch 423/1000, Loss: 0.8896069526672363\n",
      "Epoch 424/1000, Loss: 0.8799792528152466\n",
      "Epoch 425/1000, Loss: 0.9076058268547058\n",
      "Epoch 426/1000, Loss: 0.9044427275657654\n",
      "Epoch 427/1000, Loss: 0.9108419418334961\n",
      "Epoch 428/1000, Loss: 0.8886989951133728\n",
      "Epoch 429/1000, Loss: 0.8768151998519897\n",
      "Epoch 430/1000, Loss: 0.8986361622810364\n",
      "Epoch 431/1000, Loss: 0.8799495100975037\n",
      "Epoch 432/1000, Loss: 0.8678655028343201\n",
      "Epoch 433/1000, Loss: 0.8712615966796875\n",
      "Epoch 434/1000, Loss: 0.8720183968544006\n",
      "Epoch 435/1000, Loss: 0.8691335916519165\n",
      "Epoch 436/1000, Loss: 0.8626218438148499\n",
      "Epoch 437/1000, Loss: 0.8544732928276062\n",
      "Epoch 438/1000, Loss: 0.8517268896102905\n",
      "Epoch 439/1000, Loss: 0.8528604507446289\n",
      "Epoch 440/1000, Loss: 0.8468021154403687\n",
      "Epoch 441/1000, Loss: 0.8490158319473267\n",
      "Epoch 442/1000, Loss: 0.8426286578178406\n",
      "Epoch 443/1000, Loss: 0.8418775796890259\n",
      "Epoch 444/1000, Loss: 0.826177179813385\n",
      "Epoch 445/1000, Loss: 0.8304525017738342\n",
      "Epoch 446/1000, Loss: 0.8148846626281738\n",
      "Epoch 447/1000, Loss: 0.8132826685905457\n",
      "Epoch 448/1000, Loss: 0.809687614440918\n",
      "Epoch 449/1000, Loss: 0.8032838702201843\n",
      "Epoch 450/1000, Loss: 0.803029477596283\n",
      "Epoch 451/1000, Loss: 0.7915081977844238\n",
      "Epoch 452/1000, Loss: 0.7911637425422668\n",
      "Epoch 453/1000, Loss: 0.7866042256355286\n",
      "Epoch 454/1000, Loss: 0.7796599268913269\n",
      "Epoch 455/1000, Loss: 0.7770875692367554\n",
      "Epoch 456/1000, Loss: 0.7717249989509583\n",
      "Epoch 457/1000, Loss: 0.7680511474609375\n",
      "Epoch 458/1000, Loss: 0.7641671299934387\n",
      "Epoch 459/1000, Loss: 0.7597325444221497\n",
      "Epoch 460/1000, Loss: 0.7554124593734741\n",
      "Epoch 461/1000, Loss: 0.7601805329322815\n",
      "Epoch 462/1000, Loss: 0.7672648429870605\n",
      "Epoch 463/1000, Loss: 0.7697979211807251\n",
      "Epoch 464/1000, Loss: 0.7557158470153809\n",
      "Epoch 465/1000, Loss: 0.747917115688324\n",
      "Epoch 466/1000, Loss: 0.755846381187439\n",
      "Epoch 467/1000, Loss: 0.7542200088500977\n",
      "Epoch 468/1000, Loss: 0.7439740896224976\n",
      "Epoch 469/1000, Loss: 0.7410096526145935\n",
      "Epoch 470/1000, Loss: 0.7423112988471985\n",
      "Epoch 471/1000, Loss: 0.7407943606376648\n",
      "Epoch 472/1000, Loss: 0.7338036894798279\n",
      "Epoch 473/1000, Loss: 0.7384244203567505\n",
      "Epoch 474/1000, Loss: 0.7473160624504089\n",
      "Epoch 475/1000, Loss: 0.745699942111969\n",
      "Epoch 476/1000, Loss: 0.7215278744697571\n",
      "Epoch 477/1000, Loss: 0.784628689289093\n",
      "Epoch 478/1000, Loss: 0.785882830619812\n",
      "Epoch 479/1000, Loss: 0.8446101546287537\n",
      "Epoch 480/1000, Loss: 0.8432833552360535\n",
      "Epoch 481/1000, Loss: 0.8221161961555481\n",
      "Epoch 482/1000, Loss: 0.8049455881118774\n",
      "Epoch 483/1000, Loss: 0.7985916137695312\n",
      "Epoch 484/1000, Loss: 0.7982200980186462\n",
      "Epoch 485/1000, Loss: 0.7993177175521851\n",
      "Epoch 486/1000, Loss: 0.8001978993415833\n",
      "Epoch 487/1000, Loss: 0.8036225438117981\n",
      "Epoch 488/1000, Loss: 0.799988865852356\n",
      "Epoch 489/1000, Loss: 0.7942175269126892\n",
      "Epoch 490/1000, Loss: 0.7831916213035583\n",
      "Epoch 491/1000, Loss: 0.7707024216651917\n",
      "Epoch 492/1000, Loss: 0.7580079436302185\n",
      "Epoch 493/1000, Loss: 0.7470751404762268\n",
      "Epoch 494/1000, Loss: 0.7405520081520081\n",
      "Epoch 495/1000, Loss: 0.7381278276443481\n",
      "Epoch 496/1000, Loss: 0.7357253432273865\n",
      "Epoch 497/1000, Loss: 0.7306721806526184\n",
      "Epoch 498/1000, Loss: 0.7229713797569275\n",
      "Epoch 499/1000, Loss: 0.7127628922462463\n",
      "Epoch 500/1000, Loss: 0.7022940516471863\n",
      "Epoch 501/1000, Loss: 0.707223117351532\n",
      "Epoch 502/1000, Loss: 0.694312572479248\n",
      "Epoch 503/1000, Loss: 0.6906377673149109\n",
      "Epoch 504/1000, Loss: 0.6885578036308289\n",
      "Epoch 505/1000, Loss: 0.685203492641449\n",
      "Epoch 506/1000, Loss: 0.6809740662574768\n",
      "Epoch 507/1000, Loss: 0.6765902638435364\n",
      "Epoch 508/1000, Loss: 0.6724377274513245\n",
      "Epoch 509/1000, Loss: 0.6690348982810974\n",
      "Epoch 510/1000, Loss: 0.6664826273918152\n",
      "Epoch 511/1000, Loss: 0.6633316874504089\n",
      "Epoch 512/1000, Loss: 0.659162163734436\n",
      "Epoch 513/1000, Loss: 0.6553906202316284\n",
      "Epoch 514/1000, Loss: 0.6529024839401245\n",
      "Epoch 515/1000, Loss: 0.6508067846298218\n",
      "Epoch 516/1000, Loss: 0.6495227813720703\n",
      "Epoch 517/1000, Loss: 0.6459857821464539\n",
      "Epoch 518/1000, Loss: 0.6429089307785034\n",
      "Epoch 519/1000, Loss: 0.6390942335128784\n",
      "Epoch 520/1000, Loss: 0.6359424591064453\n",
      "Epoch 521/1000, Loss: 0.6346184015274048\n",
      "Epoch 522/1000, Loss: 0.6331399083137512\n",
      "Epoch 523/1000, Loss: 0.6304387450218201\n",
      "Epoch 524/1000, Loss: 0.6274213194847107\n",
      "Epoch 525/1000, Loss: 0.6245258450508118\n",
      "Epoch 526/1000, Loss: 0.6216115355491638\n",
      "Epoch 527/1000, Loss: 0.6190077662467957\n",
      "Epoch 528/1000, Loss: 0.6164097189903259\n",
      "Epoch 529/1000, Loss: 0.6137169599533081\n",
      "Epoch 530/1000, Loss: 0.6110616326332092\n",
      "Epoch 531/1000, Loss: 0.6082601547241211\n",
      "Epoch 532/1000, Loss: 0.605491042137146\n",
      "Epoch 533/1000, Loss: 0.6027155518531799\n",
      "Epoch 534/1000, Loss: 0.5999501943588257\n",
      "Epoch 535/1000, Loss: 0.597349226474762\n",
      "Epoch 536/1000, Loss: 0.5947585105895996\n",
      "Epoch 537/1000, Loss: 0.5922070741653442\n",
      "Epoch 538/1000, Loss: 0.5895026326179504\n",
      "Epoch 539/1000, Loss: 0.5867149233818054\n",
      "Epoch 540/1000, Loss: 0.5839675664901733\n",
      "Epoch 541/1000, Loss: 0.5813306570053101\n",
      "Epoch 542/1000, Loss: 0.5787555575370789\n",
      "Epoch 543/1000, Loss: 0.5761331915855408\n",
      "Epoch 544/1000, Loss: 0.5733858346939087\n",
      "Epoch 545/1000, Loss: 0.5768814086914062\n",
      "Epoch 546/1000, Loss: 0.6221325993537903\n",
      "Epoch 547/1000, Loss: 0.627895712852478\n",
      "Epoch 548/1000, Loss: 0.6212596297264099\n",
      "Epoch 549/1000, Loss: 0.6068445444107056\n",
      "Epoch 550/1000, Loss: 0.5874919295310974\n",
      "Epoch 551/1000, Loss: 0.5864723324775696\n",
      "Epoch 552/1000, Loss: 0.5863075852394104\n",
      "Epoch 553/1000, Loss: 0.584905207157135\n",
      "Epoch 554/1000, Loss: 0.5865339636802673\n",
      "Epoch 555/1000, Loss: 0.5858674645423889\n",
      "Epoch 556/1000, Loss: 0.5785145163536072\n",
      "Epoch 557/1000, Loss: 0.5718971490859985\n",
      "Epoch 558/1000, Loss: 0.5735608339309692\n",
      "Epoch 559/1000, Loss: 0.5791415572166443\n",
      "Epoch 560/1000, Loss: 0.5740901827812195\n",
      "Epoch 561/1000, Loss: 0.562896728515625\n",
      "Epoch 562/1000, Loss: 0.570544958114624\n",
      "Epoch 563/1000, Loss: 0.57123202085495\n",
      "Epoch 564/1000, Loss: 0.5591331124305725\n",
      "Epoch 565/1000, Loss: 0.5626530051231384\n",
      "Epoch 566/1000, Loss: 0.5624724626541138\n",
      "Epoch 567/1000, Loss: 0.5535165667533875\n",
      "Epoch 568/1000, Loss: 0.549503743648529\n",
      "Epoch 569/1000, Loss: 0.5520570278167725\n",
      "Epoch 570/1000, Loss: 0.5502992868423462\n",
      "Epoch 571/1000, Loss: 0.5499056577682495\n",
      "Epoch 572/1000, Loss: 0.5518774390220642\n",
      "Epoch 573/1000, Loss: 0.5410997271537781\n",
      "Epoch 574/1000, Loss: 0.5582672357559204\n",
      "Epoch 575/1000, Loss: 0.5660707354545593\n",
      "Epoch 576/1000, Loss: 0.5853029489517212\n",
      "Epoch 577/1000, Loss: 0.573818564414978\n",
      "Epoch 578/1000, Loss: 0.5628629326820374\n",
      "Epoch 579/1000, Loss: 0.5799602270126343\n",
      "Epoch 580/1000, Loss: 0.5734055638313293\n",
      "Epoch 581/1000, Loss: 0.5586039423942566\n",
      "Epoch 582/1000, Loss: 0.5596413016319275\n",
      "Epoch 583/1000, Loss: 0.5585949420928955\n",
      "Epoch 584/1000, Loss: 0.5557233691215515\n",
      "Epoch 585/1000, Loss: 0.5559908151626587\n",
      "Epoch 586/1000, Loss: 0.5550305247306824\n",
      "Epoch 587/1000, Loss: 0.5521067380905151\n",
      "Epoch 588/1000, Loss: 0.5499551892280579\n",
      "Epoch 589/1000, Loss: 0.5538408160209656\n",
      "Epoch 590/1000, Loss: 0.5487212538719177\n",
      "Epoch 591/1000, Loss: 0.5531963109970093\n",
      "Epoch 592/1000, Loss: 0.54978346824646\n",
      "Epoch 593/1000, Loss: 0.5426234602928162\n",
      "Epoch 594/1000, Loss: 0.5409532189369202\n",
      "Epoch 595/1000, Loss: 0.5419217348098755\n",
      "Epoch 596/1000, Loss: 0.5387253165245056\n",
      "Epoch 597/1000, Loss: 0.5332468152046204\n",
      "Epoch 598/1000, Loss: 0.5305296182632446\n",
      "Epoch 599/1000, Loss: 0.5289772152900696\n",
      "Epoch 600/1000, Loss: 0.5251272320747375\n",
      "Epoch 601/1000, Loss: 0.5202071666717529\n",
      "Epoch 602/1000, Loss: 0.5188486576080322\n",
      "Epoch 603/1000, Loss: 2.1491575241088867\n",
      "Epoch 604/1000, Loss: 0.5939904451370239\n",
      "Epoch 605/1000, Loss: 3.4120357036590576\n",
      "Epoch 606/1000, Loss: 3.2263388633728027\n",
      "Epoch 607/1000, Loss: 2.9794278144836426\n",
      "Epoch 608/1000, Loss: 2.7633354663848877\n",
      "Epoch 609/1000, Loss: 2.564441204071045\n",
      "Epoch 610/1000, Loss: 2.325714588165283\n",
      "Epoch 611/1000, Loss: 2.6395628452301025\n",
      "Epoch 612/1000, Loss: 2.263080596923828\n",
      "Epoch 613/1000, Loss: 1.9045860767364502\n",
      "Epoch 614/1000, Loss: 1.67862868309021\n",
      "Epoch 615/1000, Loss: 1.4491777420043945\n",
      "Epoch 616/1000, Loss: 1.3557738065719604\n",
      "Epoch 617/1000, Loss: 1.3914002180099487\n",
      "Epoch 618/1000, Loss: 1.4086310863494873\n",
      "Epoch 619/1000, Loss: 1.351765513420105\n",
      "Epoch 620/1000, Loss: 1.2561992406845093\n",
      "Epoch 621/1000, Loss: 1.171670913696289\n",
      "Epoch 622/1000, Loss: 1.1198030710220337\n",
      "Epoch 623/1000, Loss: 1.0727951526641846\n",
      "Epoch 624/1000, Loss: 1.026128888130188\n",
      "Epoch 625/1000, Loss: 0.9978175759315491\n",
      "Epoch 626/1000, Loss: 0.9805406928062439\n",
      "Epoch 627/1000, Loss: 0.9646257162094116\n",
      "Epoch 628/1000, Loss: 0.9450897574424744\n",
      "Epoch 629/1000, Loss: 0.9216827154159546\n",
      "Epoch 630/1000, Loss: 0.894167423248291\n",
      "Epoch 631/1000, Loss: 0.862917959690094\n",
      "Epoch 632/1000, Loss: 0.8246632814407349\n",
      "Epoch 633/1000, Loss: 0.784855306148529\n",
      "Epoch 634/1000, Loss: 0.829991340637207\n",
      "Epoch 635/1000, Loss: 0.7578748464584351\n",
      "Epoch 636/1000, Loss: 0.7585797905921936\n",
      "Epoch 637/1000, Loss: 0.7529991269111633\n",
      "Epoch 638/1000, Loss: 0.7362698316574097\n",
      "Epoch 639/1000, Loss: 0.7409198880195618\n",
      "Epoch 640/1000, Loss: 0.7167788743972778\n",
      "Epoch 641/1000, Loss: 0.7150973677635193\n",
      "Epoch 642/1000, Loss: 0.7084258794784546\n",
      "Epoch 643/1000, Loss: 0.6927184462547302\n",
      "Epoch 644/1000, Loss: 0.6866971254348755\n",
      "Epoch 645/1000, Loss: 0.6893370747566223\n",
      "Epoch 646/1000, Loss: 0.6732629537582397\n",
      "Epoch 647/1000, Loss: 0.66925448179245\n",
      "Epoch 648/1000, Loss: 0.6676438450813293\n",
      "Epoch 649/1000, Loss: 0.6584252715110779\n",
      "Epoch 650/1000, Loss: 0.6553563475608826\n",
      "Epoch 651/1000, Loss: 0.6531467437744141\n",
      "Epoch 652/1000, Loss: 0.6452963948249817\n",
      "Epoch 653/1000, Loss: 0.6389777064323425\n",
      "Epoch 654/1000, Loss: 0.6374607682228088\n",
      "Epoch 655/1000, Loss: 0.6315454244613647\n",
      "Epoch 656/1000, Loss: 0.6283931136131287\n",
      "Epoch 657/1000, Loss: 0.6267104148864746\n",
      "Epoch 658/1000, Loss: 0.6215518116950989\n",
      "Epoch 659/1000, Loss: 0.6156081557273865\n",
      "Epoch 660/1000, Loss: 0.6136912703514099\n",
      "Epoch 661/1000, Loss: 0.6093970537185669\n",
      "Epoch 662/1000, Loss: 0.6043310761451721\n",
      "Epoch 663/1000, Loss: 0.6026429533958435\n",
      "Epoch 664/1000, Loss: 0.5983087420463562\n",
      "Epoch 665/1000, Loss: 0.5948655009269714\n",
      "Epoch 666/1000, Loss: 0.5923276543617249\n",
      "Epoch 667/1000, Loss: 0.5892185568809509\n",
      "Epoch 668/1000, Loss: 0.5858200192451477\n",
      "Epoch 669/1000, Loss: 0.5828548073768616\n",
      "Epoch 670/1000, Loss: 0.5805703401565552\n",
      "Epoch 671/1000, Loss: 0.5780006647109985\n",
      "Epoch 672/1000, Loss: 0.5750064849853516\n",
      "Epoch 673/1000, Loss: 0.5724042057991028\n",
      "Epoch 674/1000, Loss: 0.5699816346168518\n",
      "Epoch 675/1000, Loss: 0.5673423409461975\n",
      "Epoch 676/1000, Loss: 0.564469575881958\n",
      "Epoch 677/1000, Loss: 0.5617111921310425\n",
      "Epoch 678/1000, Loss: 0.5593301057815552\n",
      "Epoch 679/1000, Loss: 0.5569996237754822\n",
      "Epoch 680/1000, Loss: 0.5544207096099854\n",
      "Epoch 681/1000, Loss: 0.5519877076148987\n",
      "Epoch 682/1000, Loss: 0.5497657656669617\n",
      "Epoch 683/1000, Loss: 0.5474867224693298\n",
      "Epoch 684/1000, Loss: 0.5450719594955444\n",
      "Epoch 685/1000, Loss: 0.5427337288856506\n",
      "Epoch 686/1000, Loss: 0.5405896306037903\n",
      "Epoch 687/1000, Loss: 0.5383881330490112\n",
      "Epoch 688/1000, Loss: 0.5361157059669495\n",
      "Epoch 689/1000, Loss: 0.5339990854263306\n",
      "Epoch 690/1000, Loss: 0.5319600701332092\n",
      "Epoch 691/1000, Loss: 0.5298768281936646\n",
      "Epoch 692/1000, Loss: 0.5278379917144775\n",
      "Epoch 693/1000, Loss: 0.5259208679199219\n",
      "Epoch 694/1000, Loss: 0.5239804983139038\n",
      "Epoch 695/1000, Loss: 0.5220293998718262\n",
      "Epoch 696/1000, Loss: 0.5201776623725891\n",
      "Epoch 697/1000, Loss: 0.5183466076850891\n",
      "Epoch 698/1000, Loss: 0.5164867043495178\n",
      "Epoch 699/1000, Loss: 0.5146628618240356\n",
      "Epoch 700/1000, Loss: 0.512881338596344\n",
      "Epoch 701/1000, Loss: 0.5110698938369751\n",
      "Epoch 702/1000, Loss: 0.5092706680297852\n",
      "Epoch 703/1000, Loss: 0.5075123906135559\n",
      "Epoch 704/1000, Loss: 0.5057366490364075\n",
      "Epoch 705/1000, Loss: 0.5039487481117249\n",
      "Epoch 706/1000, Loss: 0.5021806359291077\n",
      "Epoch 707/1000, Loss: 0.5003854036331177\n",
      "Epoch 708/1000, Loss: 0.49856171011924744\n",
      "Epoch 709/1000, Loss: 0.4967350363731384\n",
      "Epoch 710/1000, Loss: 0.49486929178237915\n",
      "Epoch 711/1000, Loss: 0.4929630160331726\n",
      "Epoch 712/1000, Loss: 0.49103429913520813\n",
      "Epoch 713/1000, Loss: 0.48905402421951294\n",
      "Epoch 714/1000, Loss: 0.48702380061149597\n",
      "Epoch 715/1000, Loss: 0.484957754611969\n",
      "Epoch 716/1000, Loss: 0.48283934593200684\n",
      "Epoch 717/1000, Loss: 0.48068687319755554\n",
      "Epoch 718/1000, Loss: 0.47852280735969543\n",
      "Epoch 719/1000, Loss: 0.4763438105583191\n",
      "Epoch 720/1000, Loss: 0.4741716682910919\n",
      "Epoch 721/1000, Loss: 0.47200703620910645\n",
      "Epoch 722/1000, Loss: 0.4698427617549896\n",
      "Epoch 723/1000, Loss: 0.46768873929977417\n",
      "Epoch 724/1000, Loss: 0.46553927659988403\n",
      "Epoch 725/1000, Loss: 0.46339139342308044\n",
      "Epoch 726/1000, Loss: 0.46125417947769165\n",
      "Epoch 727/1000, Loss: 0.4591237008571625\n",
      "Epoch 728/1000, Loss: 0.4570066034793854\n",
      "Epoch 729/1000, Loss: 0.4549148678779602\n",
      "Epoch 730/1000, Loss: 0.4528530538082123\n",
      "Epoch 731/1000, Loss: 0.4508357644081116\n",
      "Epoch 732/1000, Loss: 0.4488738179206848\n",
      "Epoch 733/1000, Loss: 0.44696614146232605\n",
      "Epoch 734/1000, Loss: 0.44510939717292786\n",
      "Epoch 735/1000, Loss: 0.44329556822776794\n",
      "Epoch 736/1000, Loss: 0.44151562452316284\n",
      "Epoch 737/1000, Loss: 0.4397663176059723\n",
      "Epoch 738/1000, Loss: 0.43804946541786194\n",
      "Epoch 739/1000, Loss: 0.4363675117492676\n",
      "Epoch 740/1000, Loss: 0.43472573161125183\n",
      "Epoch 741/1000, Loss: 0.4331277906894684\n",
      "Epoch 742/1000, Loss: 0.4315740764141083\n",
      "Epoch 743/1000, Loss: 0.4300597310066223\n",
      "Epoch 744/1000, Loss: 0.42857626080513\n",
      "Epoch 745/1000, Loss: 0.42711228132247925\n",
      "Epoch 746/1000, Loss: 0.42565983533859253\n",
      "Epoch 747/1000, Loss: 0.4242207109928131\n",
      "Epoch 748/1000, Loss: 0.42280200123786926\n",
      "Epoch 749/1000, Loss: 0.42141079902648926\n",
      "Epoch 750/1000, Loss: 0.4200522303581238\n",
      "Epoch 751/1000, Loss: 0.41872304677963257\n",
      "Epoch 752/1000, Loss: 0.41741570830345154\n",
      "Epoch 753/1000, Loss: 0.41612282395362854\n",
      "Epoch 754/1000, Loss: 0.4148389995098114\n",
      "Epoch 755/1000, Loss: 0.41356682777404785\n",
      "Epoch 756/1000, Loss: 0.41231003403663635\n",
      "Epoch 757/1000, Loss: 0.4110715985298157\n",
      "Epoch 758/1000, Loss: 0.4098507761955261\n",
      "Epoch 759/1000, Loss: 0.40864279866218567\n",
      "Epoch 760/1000, Loss: 0.4074432849884033\n",
      "Epoch 761/1000, Loss: 0.4062526226043701\n",
      "Epoch 762/1000, Loss: 0.40507206320762634\n",
      "Epoch 763/1000, Loss: 0.4039033055305481\n",
      "Epoch 764/1000, Loss: 0.40274566411972046\n",
      "Epoch 765/1000, Loss: 0.40159541368484497\n",
      "Epoch 766/1000, Loss: 0.4004494249820709\n",
      "Epoch 767/1000, Loss: 0.3993041217327118\n",
      "Epoch 768/1000, Loss: 0.39815905690193176\n",
      "Epoch 769/1000, Loss: 0.3970133662223816\n",
      "Epoch 770/1000, Loss: 0.39586731791496277\n",
      "Epoch 771/1000, Loss: 0.39472028613090515\n",
      "Epoch 772/1000, Loss: 0.39357390999794006\n",
      "Epoch 773/1000, Loss: 0.3924318253993988\n",
      "Epoch 774/1000, Loss: 0.39129990339279175\n",
      "Epoch 775/1000, Loss: 0.39018532633781433\n",
      "Epoch 776/1000, Loss: 0.38909319043159485\n",
      "Epoch 777/1000, Loss: 0.3880276381969452\n",
      "Epoch 778/1000, Loss: 0.38698962330818176\n",
      "Epoch 779/1000, Loss: 0.3859768807888031\n",
      "Epoch 780/1000, Loss: 0.3849831521511078\n",
      "Epoch 781/1000, Loss: 0.38400301337242126\n",
      "Epoch 782/1000, Loss: 0.3830288052558899\n",
      "Epoch 783/1000, Loss: 0.3820578455924988\n",
      "Epoch 784/1000, Loss: 0.3810892105102539\n",
      "Epoch 785/1000, Loss: 0.38012561202049255\n",
      "Epoch 786/1000, Loss: 0.37917089462280273\n",
      "Epoch 787/1000, Loss: 0.37822824716567993\n",
      "Epoch 788/1000, Loss: 0.3773002326488495\n",
      "Epoch 789/1000, Loss: 0.3763880431652069\n",
      "Epoch 790/1000, Loss: 0.3754923939704895\n",
      "Epoch 791/1000, Loss: 0.37461188435554504\n",
      "Epoch 792/1000, Loss: 0.3737456798553467\n",
      "Epoch 793/1000, Loss: 0.37289151549339294\n",
      "Epoch 794/1000, Loss: 0.3720484673976898\n",
      "Epoch 795/1000, Loss: 0.3712146580219269\n",
      "Epoch 796/1000, Loss: 0.37039056420326233\n",
      "Epoch 797/1000, Loss: 0.369575560092926\n",
      "Epoch 798/1000, Loss: 0.3687693476676941\n",
      "Epoch 799/1000, Loss: 0.3679720461368561\n",
      "Epoch 800/1000, Loss: 0.36718329787254333\n",
      "Epoch 801/1000, Loss: 0.36640357971191406\n",
      "Epoch 802/1000, Loss: 0.3656332194805145\n",
      "Epoch 803/1000, Loss: 0.3648715317249298\n",
      "Epoch 804/1000, Loss: 0.3641190230846405\n",
      "Epoch 805/1000, Loss: 0.36337512731552124\n",
      "Epoch 806/1000, Loss: 0.362638920545578\n",
      "Epoch 807/1000, Loss: 0.3619104325771332\n",
      "Epoch 808/1000, Loss: 0.3611888885498047\n",
      "Epoch 809/1000, Loss: 0.3604734241962433\n",
      "Epoch 810/1000, Loss: 0.3597639203071594\n",
      "Epoch 811/1000, Loss: 0.35906001925468445\n",
      "Epoch 812/1000, Loss: 0.3583604693412781\n",
      "Epoch 813/1000, Loss: 0.3576657474040985\n",
      "Epoch 814/1000, Loss: 0.35697516798973083\n",
      "Epoch 815/1000, Loss: 0.35628825426101685\n",
      "Epoch 816/1000, Loss: 0.35560494661331177\n",
      "Epoch 817/1000, Loss: 0.3549249470233917\n",
      "Epoch 818/1000, Loss: 0.3542481064796448\n",
      "Epoch 819/1000, Loss: 0.353576123714447\n",
      "Epoch 820/1000, Loss: 0.3529174029827118\n",
      "Epoch 821/1000, Loss: 0.35234764218330383\n",
      "Epoch 822/1000, Loss: 0.35256049036979675\n",
      "Epoch 823/1000, Loss: 0.352735310792923\n",
      "Epoch 824/1000, Loss: 0.3586244285106659\n",
      "Epoch 825/1000, Loss: 0.354175329208374\n",
      "Epoch 826/1000, Loss: 0.3561360239982605\n",
      "Epoch 827/1000, Loss: 0.3532433807849884\n",
      "Epoch 828/1000, Loss: 0.35633960366249084\n",
      "Epoch 829/1000, Loss: 0.3505888283252716\n",
      "Epoch 830/1000, Loss: 0.3502499461174011\n",
      "Epoch 831/1000, Loss: 0.35267773270606995\n",
      "Epoch 832/1000, Loss: 0.34800025820732117\n",
      "Epoch 833/1000, Loss: 0.35045763850212097\n",
      "Epoch 834/1000, Loss: 0.3503018021583557\n",
      "Epoch 835/1000, Loss: 0.3470369279384613\n",
      "Epoch 836/1000, Loss: 0.34595581889152527\n",
      "Epoch 837/1000, Loss: 0.34759417176246643\n",
      "Epoch 838/1000, Loss: 0.3442761301994324\n",
      "Epoch 839/1000, Loss: 0.34361234307289124\n",
      "Epoch 840/1000, Loss: 0.34422531723976135\n",
      "Epoch 841/1000, Loss: 0.3424516022205353\n",
      "Epoch 842/1000, Loss: 0.3406926691532135\n",
      "Epoch 843/1000, Loss: 0.34201496839523315\n",
      "Epoch 844/1000, Loss: 0.33929207921028137\n",
      "Epoch 845/1000, Loss: 0.3400859832763672\n",
      "Epoch 846/1000, Loss: 0.3390377461910248\n",
      "Epoch 847/1000, Loss: 0.3372196853160858\n",
      "Epoch 848/1000, Loss: 0.338332861661911\n",
      "Epoch 849/1000, Loss: 0.3359479010105133\n",
      "Epoch 850/1000, Loss: 0.3366043269634247\n",
      "Epoch 851/1000, Loss: 0.33471551537513733\n",
      "Epoch 852/1000, Loss: 0.3342345356941223\n",
      "Epoch 853/1000, Loss: 0.3330226242542267\n",
      "Epoch 854/1000, Loss: 0.33232593536376953\n",
      "Epoch 855/1000, Loss: 0.3318067193031311\n",
      "Epoch 856/1000, Loss: 0.330089807510376\n",
      "Epoch 857/1000, Loss: 0.33011600375175476\n",
      "Epoch 858/1000, Loss: 0.3282882869243622\n",
      "Epoch 859/1000, Loss: 0.3279534876346588\n",
      "Epoch 860/1000, Loss: 0.32612186670303345\n",
      "Epoch 861/1000, Loss: 0.32619449496269226\n",
      "Epoch 862/1000, Loss: 0.32456788420677185\n",
      "Epoch 863/1000, Loss: 0.3241143226623535\n",
      "Epoch 864/1000, Loss: 0.32241368293762207\n",
      "Epoch 865/1000, Loss: 0.322427362203598\n",
      "Epoch 866/1000, Loss: 0.3216751515865326\n",
      "Epoch 867/1000, Loss: 0.3209604322910309\n",
      "Epoch 868/1000, Loss: 0.319585382938385\n",
      "Epoch 869/1000, Loss: 0.31858906149864197\n",
      "Epoch 870/1000, Loss: 0.31833550333976746\n",
      "Epoch 871/1000, Loss: 0.31710705161094666\n",
      "Epoch 872/1000, Loss: 0.3171573281288147\n",
      "Epoch 873/1000, Loss: 0.3159235417842865\n",
      "Epoch 874/1000, Loss: 0.3151901066303253\n",
      "Epoch 875/1000, Loss: 0.3146381080150604\n",
      "Epoch 876/1000, Loss: 0.31326478719711304\n",
      "Epoch 877/1000, Loss: 0.3127214014530182\n",
      "Epoch 878/1000, Loss: 0.31205886602401733\n",
      "Epoch 879/1000, Loss: 0.3110562264919281\n",
      "Epoch 880/1000, Loss: 0.31041237711906433\n",
      "Epoch 881/1000, Loss: 0.3098762035369873\n",
      "Epoch 882/1000, Loss: 0.3091416656970978\n",
      "Epoch 883/1000, Loss: 0.3082471787929535\n",
      "Epoch 884/1000, Loss: 0.30788323283195496\n",
      "Epoch 885/1000, Loss: 0.3077114224433899\n",
      "Epoch 886/1000, Loss: 0.3062450885772705\n",
      "Epoch 887/1000, Loss: 0.3061475157737732\n",
      "Epoch 888/1000, Loss: 0.30727535486221313\n",
      "Epoch 889/1000, Loss: 0.3042888641357422\n",
      "Epoch 890/1000, Loss: 0.30855968594551086\n",
      "Epoch 891/1000, Loss: 0.32408109307289124\n",
      "Epoch 892/1000, Loss: 0.3246299922466278\n",
      "Epoch 893/1000, Loss: 0.3046402931213379\n",
      "Epoch 894/1000, Loss: 0.3528362214565277\n",
      "Epoch 895/1000, Loss: 0.33970704674720764\n",
      "Epoch 896/1000, Loss: 0.37998679280281067\n",
      "Epoch 897/1000, Loss: 0.38116732239723206\n",
      "Epoch 898/1000, Loss: 0.36261844635009766\n",
      "Epoch 899/1000, Loss: 0.34476563334465027\n",
      "Epoch 900/1000, Loss: 0.33490487933158875\n",
      "Epoch 901/1000, Loss: 0.33156952261924744\n",
      "Epoch 902/1000, Loss: 0.3332386016845703\n",
      "Epoch 903/1000, Loss: 0.3383881747722626\n",
      "Epoch 904/1000, Loss: 0.34330180287361145\n",
      "Epoch 905/1000, Loss: 0.34425732493400574\n",
      "Epoch 906/1000, Loss: 0.34009233117103577\n",
      "Epoch 907/1000, Loss: 0.3325416147708893\n",
      "Epoch 908/1000, Loss: 0.324827641248703\n",
      "Epoch 909/1000, Loss: 0.31952381134033203\n",
      "Epoch 910/1000, Loss: 0.31743454933166504\n",
      "Epoch 911/1000, Loss: 0.317672461271286\n",
      "Epoch 912/1000, Loss: 0.3184967339038849\n",
      "Epoch 913/1000, Loss: 0.3184632956981659\n",
      "Epoch 914/1000, Loss: 0.31703367829322815\n",
      "Epoch 915/1000, Loss: 0.3145184814929962\n",
      "Epoch 916/1000, Loss: 0.3116810619831085\n",
      "Epoch 917/1000, Loss: 0.30926135182380676\n",
      "Epoch 918/1000, Loss: 0.30757173895835876\n",
      "Epoch 919/1000, Loss: 0.30636611580848694\n",
      "Epoch 920/1000, Loss: 0.3051600754261017\n",
      "Epoch 921/1000, Loss: 0.3036549985408783\n",
      "Epoch 922/1000, Loss: 0.3018588721752167\n",
      "Epoch 923/1000, Loss: 0.29994910955429077\n",
      "Epoch 924/1000, Loss: 0.29813164472579956\n",
      "Epoch 925/1000, Loss: 0.29653120040893555\n",
      "Epoch 926/1000, Loss: 0.29513174295425415\n",
      "Epoch 927/1000, Loss: 0.29381734132766724\n",
      "Epoch 928/1000, Loss: 0.29247310757637024\n",
      "Epoch 929/1000, Loss: 0.2910187840461731\n",
      "Epoch 930/1000, Loss: 0.28945431113243103\n",
      "Epoch 931/1000, Loss: 0.28790467977523804\n",
      "Epoch 932/1000, Loss: 0.28649234771728516\n",
      "Epoch 933/1000, Loss: 0.28515613079071045\n",
      "Epoch 934/1000, Loss: 0.2837373614311218\n",
      "Epoch 935/1000, Loss: 0.2821919918060303\n",
      "Epoch 936/1000, Loss: 0.2805732488632202\n",
      "Epoch 937/1000, Loss: 0.2789241373538971\n",
      "Epoch 938/1000, Loss: 0.2772878408432007\n",
      "Epoch 939/1000, Loss: 0.27571889758110046\n",
      "Epoch 940/1000, Loss: 0.27423804998397827\n",
      "Epoch 941/1000, Loss: 0.2728261947631836\n",
      "Epoch 942/1000, Loss: 0.2714451253414154\n",
      "Epoch 943/1000, Loss: 0.27005526423454285\n",
      "Epoch 944/1000, Loss: 0.2686377167701721\n",
      "Epoch 945/1000, Loss: 0.267192542552948\n",
      "Epoch 946/1000, Loss: 0.265735387802124\n",
      "Epoch 947/1000, Loss: 0.26429012417793274\n",
      "Epoch 948/1000, Loss: 0.2628781795501709\n",
      "Epoch 949/1000, Loss: 0.26150137186050415\n",
      "Epoch 950/1000, Loss: 0.26013946533203125\n",
      "Epoch 951/1000, Loss: 0.2587665915489197\n",
      "Epoch 952/1000, Loss: 0.25736719369888306\n",
      "Epoch 953/1000, Loss: 0.25594809651374817\n",
      "Epoch 954/1000, Loss: 0.2545391619205475\n",
      "Epoch 955/1000, Loss: 0.2531612515449524\n",
      "Epoch 956/1000, Loss: 0.2518129050731659\n",
      "Epoch 957/1000, Loss: 0.2504867911338806\n",
      "Epoch 958/1000, Loss: 0.2491752654314041\n",
      "Epoch 959/1000, Loss: 0.2478787750005722\n",
      "Epoch 960/1000, Loss: 0.24659791588783264\n",
      "Epoch 961/1000, Loss: 0.2453320324420929\n",
      "Epoch 962/1000, Loss: 0.24407710134983063\n",
      "Epoch 963/1000, Loss: 0.242826908826828\n",
      "Epoch 964/1000, Loss: 0.24157848954200745\n",
      "Epoch 965/1000, Loss: 0.24033458530902863\n",
      "Epoch 966/1000, Loss: 0.23910199105739594\n",
      "Epoch 967/1000, Loss: 0.23789027333259583\n",
      "Epoch 968/1000, Loss: 0.2367052137851715\n",
      "Epoch 969/1000, Loss: 0.2355486899614334\n",
      "Epoch 970/1000, Loss: 0.23441669344902039\n",
      "Epoch 971/1000, Loss: 0.23330798745155334\n",
      "Epoch 972/1000, Loss: 0.2322242259979248\n",
      "Epoch 973/1000, Loss: 0.23116804659366608\n",
      "Epoch 974/1000, Loss: 0.23013947904109955\n",
      "Epoch 975/1000, Loss: 0.22913725674152374\n",
      "Epoch 976/1000, Loss: 0.22815854847431183\n",
      "Epoch 977/1000, Loss: 0.2272043526172638\n",
      "Epoch 978/1000, Loss: 0.2262762039899826\n",
      "Epoch 979/1000, Loss: 0.2253749668598175\n",
      "Epoch 980/1000, Loss: 0.2244981825351715\n",
      "Epoch 981/1000, Loss: 0.22364209592342377\n",
      "Epoch 982/1000, Loss: 0.22280320525169373\n",
      "Epoch 983/1000, Loss: 0.22198237478733063\n",
      "Epoch 984/1000, Loss: 0.22118164598941803\n",
      "Epoch 985/1000, Loss: 0.22040249407291412\n",
      "Epoch 986/1000, Loss: 0.21964578330516815\n",
      "Epoch 987/1000, Loss: 0.21890988945960999\n",
      "Epoch 988/1000, Loss: 0.21819405257701874\n",
      "Epoch 989/1000, Loss: 0.21749739348888397\n",
      "Epoch 990/1000, Loss: 0.2168194204568863\n",
      "Epoch 991/1000, Loss: 0.21615810692310333\n",
      "Epoch 992/1000, Loss: 0.21551163494586945\n",
      "Epoch 993/1000, Loss: 0.21487846970558167\n",
      "Epoch 994/1000, Loss: 0.2142568826675415\n",
      "Epoch 995/1000, Loss: 0.21364612877368927\n",
      "Epoch 996/1000, Loss: 0.21304646134376526\n",
      "Epoch 997/1000, Loss: 0.2124580591917038\n",
      "Epoch 998/1000, Loss: 0.21188080310821533\n",
      "Epoch 999/1000, Loss: 0.21131350100040436\n",
      "Epoch 1000/1000, Loss: 0.21075616776943207\n",
      "Validation Loss: 10.531716346740723\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Model Definition\n",
    "class CuedSpeechRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_layers=2):\n",
    "        super(CuedSpeechRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim + 1)  # +1 for the CTC blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Ensure X_batch is 3D: (batch_size, sequence_length, feature_dimension)\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((X_batch.size(0),), outputs.size(1), dtype=torch.long)  # Sequence length for each batch element\n",
    "            target_lengths = torch.tensor([len(y[y != phoneme_to_index[' ']]) for y in y_batch], dtype=torch.long)  # Target sequence length ignoring padding\n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(outputs.transpose(0, 1), y_batch, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((X_batch.size(0),), outputs.size(1), dtype=torch.long)  # Sequence length for each batch element\n",
    "            target_lengths = torch.tensor([len(y[y != phoneme_to_index[' ']]) for y in y_batch], dtype=torch.long)  # Target sequence length ignoring padding\n",
    "\n",
    "            val_loss = criterion(outputs.transpose(0, 1), y_batch, input_lengths, target_lengths)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_val_loss/len(val_loader)}\")\n",
    "\n",
    "\n",
    "# Instantiate and Train Model\n",
    "input_dim = X_combined.shape[-1]\n",
    "output_dim = len(phoneme_to_index)\n",
    "model = CuedSpeechRNN(input_dim, output_dim)\n",
    "criterion = nn.CTCLoss(blank=len(phoneme_to_index))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1000)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded training phoneme sequences: [['<start>', 'm', 'a', 's^', 'x', 'm', 'i', 'r', 'u', 's', 'i', '<end>']]\n",
      "True training phoneme sequences: [['<start>', 'm', 'a', 's^', 'x', 'm', 'i', 'z', 'e^', 'r', 'u', 's', 'i', '<end>']]\n",
      "Decoded validation phoneme sequences: [['<start>', 'm', 'a', 's^', 'x', 'm', 'i', 'r', 'u', 's', 'i', '<end>']]\n",
      "True validation phoneme sequences: [['<start>', 'i', 'l', 's', 'x', 'g', 'a', 'r', 'a~', 't', 'i', 'r', 'a', 'd', 'y', 'f', 'r', 'w', 'a', 'a', 'v', 'e^', 'k', 's', 'x', 'b', 'o~', 'k', 'a', 'p', 'y', 's^', 'o~', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(output, blank):\n",
    "    \"\"\"\n",
    "    Decode model outputs using a greedy decoder.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model outputs of shape (batch_size, sequence_length, num_classes).\n",
    "        blank (int): Index of the blank token.\n",
    "\n",
    "    Returns:\n",
    "        list: List of decoded sequences.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)  # Get the most likely class for each time step\n",
    "    decodes = []\n",
    "    for args in arg_maxes:\n",
    "        decode = []\n",
    "        previous_idx = None\n",
    "        for index in args:\n",
    "            if index != blank and (previous_idx is None or index != previous_idx):\n",
    "                decode.append(index.item())  # Append non-blank and non-repeated tokens\n",
    "            previous_idx = index\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "\n",
    "def decode_loader(model, loader, blank, index_to_phoneme):\n",
    "    \"\"\"\n",
    "    Decode outputs for all batches in a DataLoader and return both decoded and true sequences.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        loader (torch.utils.data.DataLoader): DataLoader containing input data and labels.\n",
    "        blank (int): Index of the blank token.\n",
    "        index_to_phoneme (dict): Mapping from indices to phonemes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (decoded_sequences, true_sequences), where:\n",
    "            - decoded_sequences: List of decoded phoneme sequences.\n",
    "            - true_sequences: List of true phoneme sequences.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_decoded_sequences = []\n",
    "    all_true_sequences = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, y_batch in loader:  # Iterate over batches (X_batch: inputs, y_batch: labels)\n",
    "            outputs = model(X_batch)  # Get model predictions\n",
    "            decoded_phoneme_sequences = greedy_decoder(outputs, blank=blank)  # Decode outputs\n",
    "            decoded_phonemes = [[index_to_phoneme[idx] for idx in sequence] for sequence in decoded_phoneme_sequences]  # Convert indices to phonemes\n",
    "            all_decoded_sequences.extend(decoded_phonemes)  # Add to the list of decoded sequences\n",
    "\n",
    "            # Convert true labels to phoneme sequences\n",
    "            true_phoneme_sequences = [[index_to_phoneme[idx.item()] for idx in sequence if idx != blank and \n",
    "                                       index_to_phoneme[idx.item()] != \" \"] for sequence in y_batch]\n",
    "            all_true_sequences.extend(true_phoneme_sequences)  # Add to the list of true sequences\n",
    "\n",
    "    return all_decoded_sequences, all_true_sequences\n",
    "\n",
    "\n",
    "# Example usage\n",
    "blank_token = len(phoneme_to_index)  # Index of the blank token\n",
    "decoded_train_sequences, true_train_sequences = decode_loader(model, train_loader, blank_token, index_to_phoneme)\n",
    "decoded_val_sequences, true_val_sequences = decode_loader(model, val_loader, blank_token, index_to_phoneme)\n",
    "\n",
    "# Print results\n",
    "print(\"Decoded training phoneme sequences:\", decoded_train_sequences)\n",
    "print(\"True training phoneme sequences:\", true_train_sequences)\n",
    "print(\"Decoded validation phoneme sequences:\", decoded_val_sequences)\n",
    "print(\"True validation phoneme sequences:\", true_val_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-MgaKDfGw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
