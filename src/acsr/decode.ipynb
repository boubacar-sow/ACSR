{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# cued_speech_sentences_1\n",
    "cued_speech_sentences_1 = [\n",
    "    \"al bɛr a u b li e la ra di o kə sa mɛ ra vɛ kɔ̃ s t r ɥ i t\",\n",
    "    \"le zɔ mə zɔ̃ fa i tɔ̃ be dɑ̃ lə t ru ki la vɛ k rø ze\",\n",
    "    \"le zɔ l da pa s tu ʒu r  pa r lə ʃə mɛ ki lɔ̃ k rø ze\",\n",
    "    \"e mi li la vɛ de te s te dɛ lə mɔ mɑ̃  u ɛl sɛ tɛ̃ rɔ̃ kɔ̃ t re\",\n",
    "    \"le po li si e na vɛ pã dɥ tu kɔ̃ p ri kɛ l sɛ tɛ̃ rɔ̃ kɔ̃ t re\",\n",
    "    \"i l a p re si la r i ʃɛ s ka r i l a g rã di dɑ̃ la po v re te\",\n",
    "    \"mo p ro fɛ s œr ma di ki la vɛ t u ʒu rə pœ r d lə po v re te\",\n",
    "    \"ɔ kɔ̃ sɛ r de ro k la my zi k e tɛ t ro fɔ r t\",\n",
    "    \"a vɛ k lə p wa sɔ̃ la so s ne te pa t rɛ fɔ r t\",\n",
    "    \"pu r f e t e s ɔ̃ d e pa r t sə z a mi ljɔ̃ ɔ̃ r ga ni z e y n e f e t\",\n",
    "    \"də l e ʃã brə l e z ã fã tɑ̃ d œ̃ b j ɛ̃ l e b ri də l e f e t\",\n",
    "    \"k ri di k ə s ɔ̃ ʃ i ɛ̃ av ɛ t u ʒu re t ɛ s ɔ̃ m ɛ j œ̃ r\",\n",
    "    \"ʒy li ai ta də no vo pa r ti ɑ̃ va kã s a v ɛ k s ɔ̃ m ɛ j\",\n",
    "    \"pu r ɑ̃ t re d ã lə ma z ɔ̃ l ɛ̃ s pɛ k t œ̃ r a dy mõ r s ɔ̃ b a d ʒ\"\n",
    "]\n",
    "\n",
    "# cued_speech_sentences_2\n",
    "cued_speech_sentences_2 = [\n",
    "    \"pu r a v wa r pa r ti si pe a la k ʊ r s lɑ̃ fɑ̃ a rə sy ỹ b a dʒ\",\n",
    "    \"ɡa s pa r de t ɛ p ɛr dy e nə sa v ɛ pa kwa fɛr\",\n",
    "    \"le o e t ɛ ʃ ɔ ke də vã s ɛ t ʃ o z e t r ɑ̃ ʒ a fɛr\",\n",
    "    \"la m ɛ r ɛ̃ k ɥ i ɛ t av ɛ p l ɥ z j œ r bu ʃ a nu rir\",\n",
    "    \"mõ f r ɛr po sɛ d ɛ de z a ni mo di fi si l a nu rir\",\n",
    "    \"b ɛr nar l a v ɛ a k k ɥ i ji a v ɛ k œ̃ te a la mɑ̃ t\",\n",
    "    \"i za b ɛl e t ɛ s ɔr ti k ɥ i ji r ɛ̃ bu ke də mɑ̃ t\",\n",
    "    \"la ku l œ r la p l ɥ s a so si e a la ko l ɛ r e lə ru ʒ\",\n",
    "    \"sa nu v ɛ l ʃɑ̃ br a ɛ̃ m yr ɑ̃ tjɛr mə p ɛ̃ t ɑ̃ ru ʒ\",\n",
    "    \"la pə ti fi j e t ɛ k ɔ̃ t ɑ̃ t də p ɛr drə sa p rə mj ɛr dɑ̃\",\n",
    "    \"lə po v r ɔ̃ m sɛ̃ k ɥ i ɛ t ɛ tu ʒu r də p ɛr drə y nə nu v ɛ l dɑ̃\",\n",
    "    \"d ɛ sa ma ʒɔ ri te i l sɛ t ɛ ɑ̃ ɡ a ʒe dɑ̃ l a r me\",\n",
    "    \"də p ɥ i t r wɑ̃ zɑ̃ ɛ l t r a va j ɛ dɑ̃ l a r me\",\n",
    "    \"lə ʒy də si t r ɔ̃ ɛ t r ɛ a sid\",\n",
    "    \"sə p la ɛ bo ku t r o a sid\",\n",
    "    \"ɛ l a sy ʒe ʁe a s ɔ̃ pə ti a mi baʁ by də sə r ɑ se\"\n",
    "]\n",
    "\n",
    "# cued_speech_sentences_3\n",
    "cued_speech_sentences_3 = [\n",
    "    \"i la di a s ɔ̃ f r ɛr ki l də v ɛ v r ɛ mɑ̃ sə ra ze\",\n",
    "    \"ɑ̃ sa p r ɔ ʃɑ̃ d y n r y ʃɛ l a e te pi ke pa r y n a b ɛ l\",\n",
    "    \"ɑ̃ r ə ɡa r dɑ̃ s ɔ̃ li v r i l a də vi ne l ɛ s p ɛ s də la b ɛ l\",\n",
    "    \"a p r ɛ y n ba la də dɑ̃ le ʃɑ̃ m ɛ b ɔ tə e te k u v ɛ r də bu\",\n",
    "    \"a p r ɛ a v wa r e te la v e lə k ɔ l n ɛ te p l y ta ʃe də bu\",\n",
    "    \"la n ɔ̃ s də s ɔ l d t r ɛ p ɔr tɑ̃ t a a ti re y n ɛ mɑ̃ s f ul\",\n",
    "    \"la bu tik də a k sɛ swa ɛ te dɑ̃ ɛ k a r tje e lwa nje də la f ul\",\n",
    "    \"la fi lj ɛ tə a r ə sy sə ka do pu r s ɔ̃ a ni v ɛr sɛr\",\n",
    "    \"ɛ l a a ʃə te ɛ̃ po də fl œ r pu r s ɔ̃ a ni v ɛr sɛr\",\n",
    "    \"ɑ̃ f ɛ lə si dr a v ɛ k lə ʒy də p ɔ m\",\n",
    "    \"s ɔ̃ a mi a tu ʒu r a d ɔ re mɑ̃ ʒe də p ɔ m\",\n",
    "    \"l ɛ n ɔr mə va l i s sɛ̃ bl ɛ pə ze y n t ɔ̃\",\n",
    "    \"s ɛ t ɛ s pas p ø ɑ̃ sto ke r y n t ɔ̃\",\n",
    "    \"i lɥ i a mi ɛ̃ k u to su la ɡ ɔr j\",\n",
    "    \"ɛ l a tu ʒu r ø mal a la ɡ ɔr j\",\n",
    "    \"a p r ɛ sa pa n y n pa s ɑ̃ la e də a pu se sa v wa ti r\",\n",
    "    \"k ɔm sɛ v wa z ɛ̃ ɛ l a ɛl m ɛ m r ə p ɛ̃ sa v wa ti r\",\n",
    "    \"pu r a r i ve a tɑ̃ z i l ɔ rɛ dy ma r ʃe p l y vit\",\n",
    "    \"pu r nə pa v wa r f ɛ̃ z i l ɔ rɛ dy mɑ̃ ʒe p l y vit\"\n",
    "]\n",
    "\n",
    "all_sentences = cued_speech_sentences_1 + cued_speech_sentences_2 + cued_speech_sentences_3\n",
    "\n",
    "directory = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\stimuli\\sentences\\txt'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i, sentence in enumerate(all_sentences, start=2):\n",
    "    filename = f\"sent_{i:02}.lpc\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load CSV files from a directory based on a filename pattern\n",
    "def load_csv_files(directory, filename_pattern, type=\"position\"):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            df.fillna(-1, inplace=True)\n",
    "            base_name = filename.split(f'_{type}_')[1].split('.mp4.csv')[0]\n",
    "            files_data[base_name] = df\n",
    "    return files_data\n",
    "\n",
    "# Directories\n",
    "data_dir = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\output_new'\n",
    "phoneme_dir = r'C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR-main\\stimuli\\sentences\\txt'\n",
    "\n",
    "# Load position and shape data\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "hand_shape_data = load_csv_files(data_dir, 'predictions_rf_shape', type='shape')\n",
    "\n",
    "\n",
    "# Find corresponding phoneme files based on the base names of position filenames\n",
    "def find_phoneme_files(directory, base_names):\n",
    "    phoneme_files = {}\n",
    "    for base_name in base_names:\n",
    "        phoneme_file = os.path.join(directory, f'{base_name}.lpc')\n",
    "        if os.path.exists(phoneme_file):\n",
    "            phoneme_files[base_name] = phoneme_file\n",
    "    return phoneme_files\n",
    "\n",
    "# Find phoneme files\n",
    "base_names = hand_position_data.keys()\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sliding window for each data frame\n",
    "def create_sliding_window(data, window_size):\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "    return np.array(X)\n",
    "\n",
    "# Function to pad sequences to the maximum length\n",
    "def pad_sequences(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.zeros((max_length - len(seq), seq.shape[1]))\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# Extract and concatenate probabilities from multiple dataframes\n",
    "def extract_probabilities(data, columns):\n",
    "    data = [df.fillna(-1) for df in data]\n",
    "    probs_list = [df[columns].to_numpy() for df in data]\n",
    "    return np.concatenate(probs_list, axis=0)\n",
    "\n",
    "# Prepare data for all videos\n",
    "def prepare_data_for_videos_no_sliding_windows(hand_position_data, hand_shape_data, phoneme_files):\n",
    "    all_videos_data = {}\n",
    "    for base_name in hand_position_data:\n",
    "        if base_name in phoneme_files:\n",
    "            position_df = hand_position_data[base_name]\n",
    "            shape_df = hand_shape_data[base_name]\n",
    "            phoneme_file = phoneme_files[base_name]\n",
    "\n",
    "            # Extract probabilities\n",
    "            hand_position_probs = extract_probabilities([position_df], ['p_class_1', 'p_class_2', 'p_class_3', 'p_class_4', 'p_class_5'])\n",
    "            hand_shape_probs = extract_probabilities([shape_df], ['p_class_1', 'p_class_2', 'p_class_3', 'p_class_4', 'p_class_5', 'p_class_6', 'p_class_7', 'p_class_8'])\n",
    "            combined_probs = np.concatenate((hand_position_probs, hand_shape_probs), axis=1)\n",
    "\n",
    "            # Read phoneme sequences\n",
    "            with open(phoneme_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.read().splitlines()\n",
    "                phoneme_sequence = [phoneme for line in lines for phoneme in line.split()]\n",
    "\n",
    "            # Convert phoneme sequence to indices\n",
    "            phoneme_indices = [phoneme_to_index[phoneme] for phoneme in phoneme_sequence]\n",
    "            all_videos_data[base_name] = {'X': combined_probs, 'y': phoneme_indices}\n",
    "    return all_videos_data\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "hand_shape_data = load_csv_files(data_dir, 'predictions_rf_shape', type='shape')\n",
    "base_names = hand_position_data.keys()d\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n",
    "# take only the first 5 phoneme files for demonstration\n",
    "hand_position_data = {key: hand_position_data[key] for key in list(hand_position_data.keys())[:5]}\n",
    "hand_shape_data = {key: hand_shape_data[key] for key in list(hand_shape_data.keys())[:5]}\n",
    "phoneme_files = {key: phoneme_files[key] for key in list(phoneme_files.keys())[:5]}\n",
    "\n",
    "# Function to apply phonotactic rules\n",
    "def apply_phonotactic_rules(combinations):\n",
    "    valid_combinations = []\n",
    "    for combination in combinations:\n",
    "        # Example phonotactic rules:\n",
    "        # 1. No consecutive vowels (e.g., \"ae\")\n",
    "        # 2. Certain consonant clusters are invalid (e.g., \"tl\")\n",
    "        if re.search(r'[aeiouy]{2}', combination):\n",
    "            continue  # Skip invalid combinations with consecutive vowels\n",
    "        if re.search(r'([s])\\1', combination):  # Skip invalid combinations with double consonants\n",
    "            continue\n",
    "        valid_combinations.append(combination)\n",
    "    return valid_combinations\n",
    "\n",
    "# Phoneme Mapping\n",
    "consonants = ['b', 'ch', 'd', 'f', 'g', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 'sh', 't', 'v', 'w', 'y', 'z']\n",
    "vowels = ['a', 'e', 'ɛ', 'i', 'o', 'ɔ', 'u', 'ø', 'œ']\n",
    "phoneme_combinations = consonants + vowels + [c + v for c in consonants for v in vowels] + [v + c for v in vowels for c in consonants]\n",
    "\n",
    "all_phonemes = []\n",
    "for sentence in all_sentences:  # Assuming all_sentences is a list of sentences already defined\n",
    "    all_phonemes.extend(sentence.split())\n",
    "\n",
    "phoneme_combinations += set(all_phonemes)\n",
    "valid_phoneme_combinations = set(apply_phonotactic_rules(phoneme_combinations))\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(valid_phoneme_combinations)}\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "\n",
    "phoneme_to_index[' '] = len(phoneme_to_index)\n",
    "index_to_phoneme[len(index_to_phoneme)] = ' '\n",
    "\n",
    "\n",
    "# Function to pad sequences to the maximum length\n",
    "def pad_sequences(sequences, max_length, pad_value=-1):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.full((max_length - len(seq), seq.shape[1]), pad_value)\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# Combine sequences with padding\n",
    "def combine_sequences_with_padding(video_data):\n",
    "    max_length = max(len(video_data[video]['X']) for video in video_data)\n",
    "    X_padded = [pad_sequences([video_data[video]['X']], max_length)[0] for video in video_data]\n",
    "    y_padded = [video_data[video]['y'] + [phoneme_to_index[' ']] * (max_length - len(video_data[video]['y'])) for video in video_data]\n",
    "    return X_padded, y_padded\n",
    "\n",
    "\n",
    "# Prepare data for all videos\n",
    "all_videos_data = prepare_data_for_videos_no_sliding_windows(hand_position_data, hand_shape_data, phoneme_files)\n",
    "X_combined, y_combined = combine_sequences_with_padding(all_videos_data)\n",
    "\n",
    "# convert phoneme sequences to indices\n",
    "y_tensors = [torch.tensor([index for index in video_data['y']], dtype=torch.long) for video_data in all_videos_data.values()]\n",
    "all_videos_data = {key: {'X': video_data['X'], 'y': y_tensors[i]} for i, (key, video_data) in enumerate(all_videos_data.items())}\n",
    "\n",
    "# Combine all data\n",
    "X_combined = torch.tensor(X_combined, dtype=torch.float32)\n",
    "y_combined = torch.tensor(y_combined, dtype=torch.long)\n",
    "\n",
    "all_videos_data = {'X': X_combined, 'y': y_combined}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       " 'y': tensor([[ 34, 215,  66,  ..., 449, 449, 449],\n",
       "         [242, 145,  66,  ..., 449, 449, 449],\n",
       "         [181,  67, 344,  ..., 449, 449, 449],\n",
       "         [398, 265,  62,  ..., 449, 449, 449],\n",
       "         [110, 114, 363,  ..., 449, 449, 449]])}"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_videos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into training and validation sets\n",
    "def train_val_split(data, train_ratio=0.8):\n",
    "    video_names = list(data['X'])\n",
    "    split_idx = int(len(video_names) * train_ratio)\n",
    "    train_data = {'X': data['X'][:split_idx], 'y': data['y'][:split_idx]}\n",
    "    val_data = {'X': data['X'][split_idx:], 'y': data['y'][split_idx:]}\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_val_split(all_videos_data)\n",
    "\n",
    "# Convert data to DataLoader format\n",
    "def data_to_dataloader(data):\n",
    "    X_tensors = data['X']\n",
    "    y_tensors = data['y']\n",
    "    \n",
    "    X_dataset = TensorDataset(X_tensors)\n",
    "    y_dataset = y_tensors  # Phoneme sequences are kept separately for each video\n",
    "    \n",
    "    return DataLoader(X_dataset, batch_size=1, shuffle=True), y_dataset\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader, y_train_tensors = data_to_dataloader(train_data)\n",
    "val_loader, y_val_tensors = data_to_dataloader(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n",
      "torch.Size([1, 1166, 13])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 898.406494140625\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model Definition\n",
    "class CuedSpeechRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_layers=4):\n",
    "        super(CuedSpeechRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim + 1)  # +1 for the CTC blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, y_train_tensors, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in zip(train_loader, y_train_tensors):\n",
    "            optimizer.zero_grad()\n",
    "            # Ensure X_batch is 3D: (batch_size, sequence_length, feature_dimension)\n",
    "            X_batch = X_batch[0]  # Extract the tensor from the batch tuple\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)  # Add batch dimension if needed\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.long) # Sequence length for each batch element \n",
    "            target_lengths = torch.tensor([len(y_batch[y_batch != phoneme_to_index[' ']])], dtype=torch.long) # Target sequence length ignoring padding \n",
    "            # Compute CTC loss \n",
    "            loss = criterion(outputs.transpose(0, 1), y_batch.unsqueeze(0), input_lengths, target_lengths) \n",
    "            loss.backward() \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1) \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader, y_val_tensors, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in zip(val_loader, y_val_tensors):\n",
    "            X_batch = X_batch[0]\n",
    "            if X_batch.dim() == 2:\n",
    "                X_batch = X_batch.unsqueeze(0)\n",
    "            outputs = model(X_batch)\n",
    "            input_lengths = torch.full((outputs.size(0),), outputs.size(1), dtype=torch.long) # Sequence length for each batch element \n",
    "            target_lengths = torch.tensor([len(y_batch[y_batch != phoneme_to_index[' ']])], dtype=torch.long) # Target sequence length ignoring padding \n",
    "            val_loss = criterion(outputs.transpose(0, 1), y_batch.unsqueeze(0), input_lengths, target_lengths)\n",
    "            total_val_loss += val_loss.item()\n",
    "    print(f\"Validation Loss: {total_val_loss/len(val_loader)}\")\n",
    "\n",
    "# Decoding Function\n",
    "def greedy_decoder(output, blank):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank:\n",
    "                if j != 0 and index == args[j - 1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "# Instantiate and Train Model\n",
    "input_dim = X_combined.shape[-1]\n",
    "output_dim = len(phoneme_to_index)\n",
    "model = CuedSpeechRNN(input_dim, output_dim)\n",
    "criterion = nn.CTCLoss(blank=len(phoneme_to_index))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, y_train_tensors, criterion, optimizer, num_epochs=340)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, train_loader, y_val_tensors, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[242, 145,  66,  ..., 238, 238, 238],\n",
      "        [242, 145,  66,  ..., 238, 238, 238],\n",
      "        [242, 145,  66,  ..., 238, 238, 238],\n",
      "        [242, 145,  66,  ..., 238, 238, 238],\n",
      "        [242, 145,  66,  ..., 238, 238, 238]])\n",
      "Decoded phoneme sequences: [['le', 'po', 'li', 'si', 'e', 'vɛ', 'te', 's', 'te', 'tu', 'lə', 'p', 'ri', 'kɛ', 'l', 'sɛ', 'tɛ̃', 'rɔ̃', 'kɔ̃', 't', 're'], ['le', 'po', 'li', 'si', 'e', 'vɛ', 'te', 's', 'te', 'tu', 'lə', 'p', 'ri', 'kɛ', 'l', 'sɛ', 'tɛ̃', 'rɔ̃', 'kɔ̃', 't', 're'], ['le', 'po', 'li', 'si', 'e', 'vɛ', 'te', 's', 'te', 'tu', 'lə', 'p', 'ri', 'kɛ', 'l', 'sɛ', 'tɛ̃', 'rɔ̃', 'kɔ̃', 't', 're'], ['le', 'po', 'li', 'si', 'e', 'vɛ', 'te', 's', 'te', 'tu', 'lə', 'p', 'ri', 'kɛ', 'l', 'sɛ', 'tɛ̃', 'rɔ̃', 'kɔ̃', 't', 're'], ['le', 'po', 'li', 'si', 'e', 'vɛ', 'te', 's', 'te', 'tu', 'lə', 'p', 'ri', 'kɛ', 'l', 'sɛ', 'tɛ̃', 'rɔ̃', 'kɔ̃', 't', 're']]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(output, blank):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    print(arg_maxes)\n",
    "    decodes = []\n",
    "    for args in arg_maxes:\n",
    "        decode = []\n",
    "        previous_idx = None\n",
    "        for index in args:\n",
    "            if index != blank and (previous_idx is None or index != previous_idx):\n",
    "                decode.append(index.item())\n",
    "            previous_idx = index\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "# Decoding\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_combined)    \n",
    "    decoded_phoneme_sequences = greedy_decoder(outputs, blank=len(phoneme_to_index))\n",
    "decoded_phonemes = [[index_to_phoneme[idx] for idx in sequence] for sequence in decoded_phoneme_sequences]\n",
    "print(\"Decoded phoneme sequences:\", decoded_phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/440, Loss: 1.232065111398697\n",
      "Epoch 2/440, Loss: 1.2320519387722015\n",
      "Epoch 3/440, Loss: 1.2320634126663208\n",
      "Epoch 4/440, Loss: 1.2321169674396515\n",
      "Epoch 5/440, Loss: 1.2319994270801544\n",
      "Epoch 6/440, Loss: 1.2320965230464935\n",
      "Epoch 7/440, Loss: 1.2321026027202606\n",
      "Epoch 8/440, Loss: 1.23198801279068\n",
      "Epoch 9/440, Loss: 1.2320420444011688\n",
      "Epoch 10/440, Loss: 1.2320517599582672\n",
      "Epoch 11/440, Loss: 1.2320504784584045\n",
      "Epoch 12/440, Loss: 1.2319833040237427\n",
      "Epoch 13/440, Loss: 1.2319735288619995\n",
      "Epoch 14/440, Loss: 1.2320421040058136\n",
      "Epoch 15/440, Loss: 1.2319719791412354\n",
      "Epoch 16/440, Loss: 1.2320704758167267\n",
      "Epoch 17/440, Loss: 1.2320142090320587\n",
      "Epoch 18/440, Loss: 1.231958270072937\n",
      "Epoch 19/440, Loss: 1.232018381357193\n",
      "Epoch 20/440, Loss: 1.2319434583187103\n",
      "Epoch 21/440, Loss: 1.231940507888794\n",
      "Epoch 22/440, Loss: 1.231936663389206\n",
      "Epoch 23/440, Loss: 1.232029289007187\n",
      "Epoch 24/440, Loss: 1.2320004403591156\n",
      "Epoch 25/440, Loss: 1.2319075465202332\n",
      "Epoch 26/440, Loss: 1.231983870267868\n",
      "Epoch 27/440, Loss: 1.2319391369819641\n",
      "Epoch 28/440, Loss: 1.231981337070465\n",
      "Epoch 29/440, Loss: 1.2319700717926025\n",
      "Epoch 30/440, Loss: 1.2319495379924774\n",
      "Epoch 31/440, Loss: 1.2320405542850494\n",
      "Epoch 32/440, Loss: 1.2320345342159271\n",
      "Epoch 33/440, Loss: 1.231884926557541\n",
      "Epoch 34/440, Loss: 1.2319307923316956\n",
      "Epoch 35/440, Loss: 1.2320140302181244\n",
      "Epoch 36/440, Loss: 1.2319731414318085\n",
      "Epoch 37/440, Loss: 1.2319850623607635\n",
      "Epoch 38/440, Loss: 1.2320238053798676\n",
      "Epoch 39/440, Loss: 1.2320041358470917\n",
      "Epoch 40/440, Loss: 1.231885939836502\n",
      "Epoch 41/440, Loss: 1.231907069683075\n",
      "Epoch 42/440, Loss: 1.2318617403507233\n",
      "Epoch 43/440, Loss: 1.2319464981555939\n",
      "Epoch 44/440, Loss: 1.2319032549858093\n",
      "Epoch 45/440, Loss: 1.23193359375\n",
      "Epoch 46/440, Loss: 1.2318333983421326\n",
      "Epoch 47/440, Loss: 1.2319876551628113\n",
      "Epoch 48/440, Loss: 1.2319402396678925\n",
      "Epoch 49/440, Loss: 1.2319196462631226\n",
      "Epoch 50/440, Loss: 1.23190176486969\n",
      "Epoch 51/440, Loss: 1.2319056689739227\n",
      "Epoch 52/440, Loss: 1.2319608926773071\n",
      "Epoch 53/440, Loss: 1.2318676710128784\n",
      "Epoch 54/440, Loss: 1.2319120168685913\n",
      "Epoch 55/440, Loss: 1.2318135797977448\n",
      "Epoch 56/440, Loss: 1.2319509088993073\n",
      "Epoch 57/440, Loss: 1.2318857610225677\n",
      "Epoch 58/440, Loss: 1.2318560183048248\n",
      "Epoch 59/440, Loss: 1.2319380640983582\n",
      "Epoch 60/440, Loss: 1.231978178024292\n",
      "Epoch 61/440, Loss: 1.2318518459796906\n",
      "Epoch 62/440, Loss: 1.2317985892295837\n",
      "Epoch 63/440, Loss: 1.2317781448364258\n",
      "Epoch 64/440, Loss: 1.231940120458603\n",
      "Epoch 65/440, Loss: 1.231941044330597\n",
      "Epoch 66/440, Loss: 1.2319347262382507\n",
      "Epoch 67/440, Loss: 1.231861025094986\n",
      "Epoch 68/440, Loss: 1.231923669576645\n",
      "Epoch 69/440, Loss: 1.231893002986908\n",
      "Epoch 70/440, Loss: 1.2317746877670288\n",
      "Epoch 71/440, Loss: 1.2319510877132416\n",
      "Epoch 72/440, Loss: 1.2318778932094574\n",
      "Epoch 73/440, Loss: 1.2317728400230408\n",
      "Epoch 74/440, Loss: 1.2318713963031769\n",
      "Epoch 75/440, Loss: 1.23194020986557\n",
      "Epoch 76/440, Loss: 1.231836587190628\n",
      "Epoch 77/440, Loss: 1.2319263219833374\n",
      "Epoch 78/440, Loss: 1.2317522168159485\n",
      "Epoch 79/440, Loss: 1.2318367063999176\n",
      "Epoch 80/440, Loss: 1.2318329811096191\n",
      "Epoch 81/440, Loss: 1.231791913509369\n",
      "Epoch 82/440, Loss: 1.2318165302276611\n",
      "Epoch 83/440, Loss: 1.2318522334098816\n",
      "Epoch 84/440, Loss: 1.2318791151046753\n",
      "Epoch 85/440, Loss: 1.2318772971630096\n",
      "Epoch 86/440, Loss: 1.231721669435501\n",
      "Epoch 87/440, Loss: 1.2408416271209717\n",
      "Epoch 88/440, Loss: 1.2312159836292267\n",
      "Epoch 89/440, Loss: 1.2322956025600433\n",
      "Epoch 90/440, Loss: 1.2324731945991516\n",
      "Epoch 91/440, Loss: 1.2322429716587067\n",
      "Epoch 92/440, Loss: 1.2318950295448303\n",
      "Epoch 93/440, Loss: 1.2318113148212433\n",
      "Epoch 94/440, Loss: 1.2319195866584778\n",
      "Epoch 95/440, Loss: 1.2319466173648834\n",
      "Epoch 96/440, Loss: 1.2318586707115173\n",
      "Epoch 97/440, Loss: 1.2318685948848724\n",
      "Epoch 98/440, Loss: 1.2317782640457153\n",
      "Epoch 99/440, Loss: 1.2318695783615112\n",
      "Epoch 100/440, Loss: 1.2318638265132904\n",
      "Epoch 101/440, Loss: 1.2317811250686646\n",
      "Epoch 102/440, Loss: 1.231774091720581\n",
      "Epoch 103/440, Loss: 1.2318629026412964\n",
      "Epoch 104/440, Loss: 1.2319034337997437\n",
      "Epoch 105/440, Loss: 1.2318232655525208\n",
      "Epoch 106/440, Loss: 1.2318379878997803\n",
      "Epoch 107/440, Loss: 1.2317700684070587\n",
      "Epoch 108/440, Loss: 1.2318316996097565\n",
      "Epoch 109/440, Loss: 1.231825351715088\n",
      "Epoch 110/440, Loss: 1.2318196892738342\n",
      "Epoch 111/440, Loss: 1.2318852841854095\n",
      "Epoch 112/440, Loss: 1.231715977191925\n",
      "Epoch 113/440, Loss: 1.2317111790180206\n",
      "Epoch 114/440, Loss: 1.231874018907547\n",
      "Epoch 115/440, Loss: 1.231857419013977\n",
      "Epoch 116/440, Loss: 1.231691837310791\n",
      "Epoch 117/440, Loss: 1.2316996157169342\n",
      "Epoch 118/440, Loss: 1.2318137288093567\n",
      "Epoch 119/440, Loss: 1.2318085730075836\n",
      "Epoch 120/440, Loss: 1.2317965924739838\n",
      "Epoch 121/440, Loss: 1.2316739559173584\n",
      "Epoch 122/440, Loss: 1.2318171858787537\n",
      "Epoch 123/440, Loss: 1.231728434562683\n",
      "Epoch 124/440, Loss: 1.2317711412906647\n",
      "Epoch 125/440, Loss: 1.2318302989006042\n",
      "Epoch 126/440, Loss: 1.2318262457847595\n",
      "Epoch 127/440, Loss: 1.231818675994873\n",
      "Epoch 128/440, Loss: 1.2316611111164093\n",
      "Epoch 129/440, Loss: 1.2317331731319427\n",
      "Epoch 130/440, Loss: 1.2317196428775787\n",
      "Epoch 131/440, Loss: 1.2317326068878174\n",
      "Epoch 132/440, Loss: 1.231724351644516\n",
      "Epoch 133/440, Loss: 1.2317211627960205\n",
      "Epoch 134/440, Loss: 1.2316486537456512\n",
      "Epoch 135/440, Loss: 1.2317689955234528\n",
      "Epoch 136/440, Loss: 1.2317215204238892\n",
      "Epoch 137/440, Loss: 1.2317091226577759\n",
      "Epoch 138/440, Loss: 1.2317274808883667\n",
      "Epoch 139/440, Loss: 1.2317630350589752\n",
      "Epoch 140/440, Loss: 1.2317108809947968\n",
      "Epoch 141/440, Loss: 1.2316828668117523\n",
      "Epoch 142/440, Loss: 1.2317641079425812\n",
      "Epoch 143/440, Loss: 1.2316895723342896\n",
      "Epoch 144/440, Loss: 1.231721431016922\n",
      "Epoch 145/440, Loss: 1.2316290736198425\n",
      "Epoch 146/440, Loss: 1.2317423522472382\n",
      "Epoch 147/440, Loss: 1.231707215309143\n",
      "Epoch 148/440, Loss: 1.23162043094635\n",
      "Epoch 149/440, Loss: 1.231619507074356\n",
      "Epoch 150/440, Loss: 1.2316907942295074\n",
      "Epoch 151/440, Loss: 1.2316690683364868\n",
      "Epoch 152/440, Loss: 1.2316870093345642\n",
      "Epoch 153/440, Loss: 1.2316833138465881\n",
      "Epoch 154/440, Loss: 1.2316611409187317\n",
      "Epoch 155/440, Loss: 1.231685072183609\n",
      "Epoch 156/440, Loss: 1.2316094040870667\n",
      "Epoch 157/440, Loss: 1.2316687107086182\n",
      "Epoch 158/440, Loss: 1.2316677868366241\n",
      "Epoch 159/440, Loss: 1.2316702008247375\n",
      "Epoch 160/440, Loss: 1.2317053377628326\n",
      "Epoch 161/440, Loss: 1.2316659092903137\n",
      "Epoch 162/440, Loss: 1.2316415011882782\n",
      "Epoch 163/440, Loss: 1.2316594123840332\n",
      "Epoch 164/440, Loss: 1.2315874993801117\n",
      "Epoch 165/440, Loss: 1.2316399216651917\n",
      "Epoch 166/440, Loss: 1.2316283583641052\n",
      "Epoch 167/440, Loss: 1.2316257655620575\n",
      "Epoch 168/440, Loss: 1.2315631806850433\n",
      "Epoch 169/440, Loss: 1.2316170930862427\n",
      "Epoch 170/440, Loss: 1.231616050004959\n",
      "Epoch 171/440, Loss: 1.231553703546524\n",
      "Epoch 172/440, Loss: 1.2316741645336151\n",
      "Epoch 173/440, Loss: 1.2317350208759308\n",
      "Epoch 174/440, Loss: 1.231757640838623\n",
      "Epoch 175/440, Loss: 1.2317975759506226\n",
      "Epoch 176/440, Loss: 1.231708586215973\n",
      "Epoch 177/440, Loss: 1.231629729270935\n",
      "Epoch 178/440, Loss: 1.2316536903381348\n",
      "Epoch 179/440, Loss: 1.2316042184829712\n",
      "Epoch 180/440, Loss: 1.2316007614135742\n",
      "Epoch 181/440, Loss: 1.2315926253795624\n",
      "Epoch 182/440, Loss: 1.231604129076004\n",
      "Epoch 183/440, Loss: 1.2316836714744568\n",
      "Epoch 184/440, Loss: 1.2316170632839203\n",
      "Epoch 185/440, Loss: 1.2316964864730835\n",
      "Epoch 186/440, Loss: 1.2316220998764038\n",
      "Epoch 187/440, Loss: 1.2316475212574005\n",
      "Epoch 188/440, Loss: 1.2315786182880402\n",
      "Epoch 189/440, Loss: 1.2316551506519318\n",
      "Epoch 190/440, Loss: 1.2316084206104279\n",
      "Epoch 191/440, Loss: 1.2316293716430664\n",
      "Epoch 192/440, Loss: 1.2315471768379211\n",
      "Epoch 193/440, Loss: 1.2316842377185822\n",
      "Epoch 194/440, Loss: 1.2316015660762787\n",
      "Epoch 195/440, Loss: 1.2316056489944458\n",
      "Epoch 196/440, Loss: 1.231598138809204\n",
      "Epoch 197/440, Loss: 1.231655865907669\n",
      "Epoch 198/440, Loss: 1.2315932512283325\n",
      "Epoch 199/440, Loss: 1.2315893769264221\n",
      "Epoch 200/440, Loss: 1.2315847277641296\n",
      "Epoch 201/440, Loss: 1.231514811515808\n",
      "Epoch 202/440, Loss: 1.231637865304947\n",
      "Epoch 203/440, Loss: 1.2315135300159454\n",
      "Epoch 204/440, Loss: 1.2315025329589844\n",
      "Epoch 205/440, Loss: 1.2315858602523804\n",
      "Epoch 206/440, Loss: 1.2315659523010254\n",
      "Epoch 207/440, Loss: 1.231508582830429\n",
      "Epoch 208/440, Loss: 1.2316263318061829\n",
      "Epoch 209/440, Loss: 1.2314926385879517\n",
      "Epoch 210/440, Loss: 1.231548935174942\n",
      "Epoch 211/440, Loss: 1.231491357088089\n",
      "Epoch 212/440, Loss: 1.2314847111701965\n",
      "Epoch 213/440, Loss: 1.2315671145915985\n",
      "Epoch 214/440, Loss: 1.2316448986530304\n",
      "Epoch 215/440, Loss: 1.2315565943717957\n",
      "Epoch 216/440, Loss: 1.231617659330368\n",
      "Epoch 217/440, Loss: 1.231630951166153\n",
      "Epoch 218/440, Loss: 1.2314712703227997\n",
      "Epoch 219/440, Loss: 1.2315602898597717\n",
      "Epoch 220/440, Loss: 1.2316025793552399\n",
      "Epoch 221/440, Loss: 1.2315497994422913\n",
      "Epoch 222/440, Loss: 1.2314761579036713\n",
      "Epoch 223/440, Loss: 1.2314694225788116\n",
      "Epoch 224/440, Loss: 1.2315388321876526\n",
      "Epoch 225/440, Loss: 1.2315911948680878\n",
      "Epoch 226/440, Loss: 1.2314584255218506\n",
      "Epoch 227/440, Loss: 1.2315905690193176\n",
      "Epoch 228/440, Loss: 1.2314516305923462\n",
      "Epoch 229/440, Loss: 1.2316151857376099\n",
      "Epoch 230/440, Loss: 1.2315602004528046\n",
      "Epoch 231/440, Loss: 1.2315291464328766\n",
      "Epoch 232/440, Loss: 1.2315152287483215\n",
      "Epoch 233/440, Loss: 1.2315877974033356\n",
      "Epoch 234/440, Loss: 1.2315291166305542\n",
      "Epoch 235/440, Loss: 1.231571912765503\n",
      "Epoch 236/440, Loss: 1.2315671145915985\n",
      "Epoch 237/440, Loss: 1.2315817773342133\n",
      "Epoch 238/440, Loss: 1.2315073609352112\n",
      "Epoch 239/440, Loss: 1.231492817401886\n",
      "Epoch 240/440, Loss: 1.23145192861557\n",
      "Epoch 241/440, Loss: 1.2315410375595093\n",
      "Epoch 242/440, Loss: 1.2314403355121613\n",
      "Epoch 243/440, Loss: 1.2315101623535156\n",
      "Epoch 244/440, Loss: 1.2314462661743164\n",
      "Epoch 245/440, Loss: 1.2315450608730316\n",
      "Epoch 246/440, Loss: 1.2314711213111877\n",
      "Epoch 247/440, Loss: 1.2315088212490082\n",
      "Epoch 248/440, Loss: 1.2315730154514313\n",
      "Epoch 249/440, Loss: 1.2314433157444\n",
      "Epoch 250/440, Loss: 1.231427252292633\n",
      "Epoch 251/440, Loss: 1.2315475046634674\n",
      "Epoch 252/440, Loss: 1.231554001569748\n",
      "Epoch 253/440, Loss: 1.2314810752868652\n",
      "Epoch 254/440, Loss: 1.2314188778400421\n",
      "Epoch 255/440, Loss: 1.2314790487289429\n",
      "Epoch 256/440, Loss: 1.2314139604568481\n",
      "Epoch 257/440, Loss: 1.2315455377101898\n",
      "Epoch 258/440, Loss: 1.2314004004001617\n",
      "Epoch 259/440, Loss: 1.2314013838768005\n",
      "Epoch 260/440, Loss: 1.2315481007099152\n",
      "Epoch 261/440, Loss: 1.2314107716083527\n",
      "Epoch 262/440, Loss: 1.2315563261508942\n",
      "Epoch 263/440, Loss: 1.2315301299095154\n",
      "Epoch 264/440, Loss: 1.2315234243869781\n",
      "Epoch 265/440, Loss: 1.2313936054706573\n",
      "Epoch 266/440, Loss: 1.2315341532230377\n",
      "Epoch 267/440, Loss: 1.2315096855163574\n",
      "Epoch 268/440, Loss: 1.2314783036708832\n",
      "Epoch 269/440, Loss: 1.2314717173576355\n",
      "Epoch 270/440, Loss: 1.2314079701900482\n",
      "Epoch 271/440, Loss: 1.231517642736435\n",
      "Epoch 272/440, Loss: 1.2314401268959045\n",
      "Epoch 273/440, Loss: 1.2314897179603577\n",
      "Epoch 274/440, Loss: 1.231520265340805\n",
      "Epoch 275/440, Loss: 1.2314703166484833\n",
      "Epoch 276/440, Loss: 1.2313984632492065\n",
      "Epoch 277/440, Loss: 1.2314043641090393\n",
      "Epoch 278/440, Loss: 1.2313915491104126\n",
      "Epoch 279/440, Loss: 1.231505572795868\n",
      "Epoch 280/440, Loss: 1.2314477860927582\n",
      "Epoch 281/440, Loss: 1.2314655780792236\n",
      "Epoch 282/440, Loss: 1.23148775100708\n",
      "Epoch 283/440, Loss: 1.2314417958259583\n",
      "Epoch 284/440, Loss: 1.231479972600937\n",
      "Epoch 285/440, Loss: 1.2314315140247345\n",
      "Epoch 286/440, Loss: 1.2314721047878265\n",
      "Epoch 287/440, Loss: 1.2313784956932068\n",
      "Epoch 288/440, Loss: 1.2314293086528778\n",
      "Epoch 289/440, Loss: 1.231400340795517\n",
      "Epoch 290/440, Loss: 1.23136505484581\n",
      "Epoch 291/440, Loss: 1.231469750404358\n",
      "Epoch 292/440, Loss: 1.2314405143260956\n",
      "Epoch 293/440, Loss: 1.231416255235672\n",
      "Epoch 294/440, Loss: 1.2313847243785858\n",
      "Epoch 295/440, Loss: 1.2313815355300903\n",
      "Epoch 296/440, Loss: 1.2314361929893494\n",
      "Epoch 297/440, Loss: 1.2314108312129974\n",
      "Epoch 298/440, Loss: 1.2314535975456238\n",
      "Epoch 299/440, Loss: 1.2314230799674988\n",
      "Epoch 300/440, Loss: 1.2314860224723816\n",
      "Epoch 301/440, Loss: 1.2314521670341492\n",
      "Epoch 302/440, Loss: 1.2314411997795105\n",
      "Epoch 303/440, Loss: 1.2313457429409027\n",
      "Epoch 304/440, Loss: 1.2314185202121735\n",
      "Epoch 305/440, Loss: 1.2313260734081268\n",
      "Epoch 306/440, Loss: 1.2313948571681976\n",
      "Epoch 307/440, Loss: 1.231468915939331\n",
      "Epoch 308/440, Loss: 1.231397032737732\n",
      "Epoch 309/440, Loss: 1.2318775951862335\n",
      "Epoch 310/440, Loss: 1.2315822839736938\n",
      "Epoch 311/440, Loss: 1.2313468754291534\n",
      "Epoch 312/440, Loss: 1.231315940618515\n",
      "Epoch 313/440, Loss: 1.2313861846923828\n",
      "Epoch 314/440, Loss: 1.2313890755176544\n",
      "Epoch 315/440, Loss: 1.231389045715332\n",
      "Epoch 316/440, Loss: 1.2314415276050568\n",
      "Epoch 317/440, Loss: 1.2313056290149689\n",
      "Epoch 318/440, Loss: 1.231473594903946\n",
      "Epoch 319/440, Loss: 1.2313381135463715\n",
      "Epoch 320/440, Loss: 1.2312996685504913\n",
      "Epoch 321/440, Loss: 1.2313702404499054\n",
      "Epoch 322/440, Loss: 1.2313849031925201\n",
      "Epoch 323/440, Loss: 1.2312785387039185\n",
      "Epoch 324/440, Loss: 1.2313617169857025\n",
      "Epoch 325/440, Loss: 1.2314705848693848\n",
      "Epoch 326/440, Loss: 1.231454461812973\n",
      "Epoch 327/440, Loss: 1.2312979400157928\n",
      "Epoch 328/440, Loss: 1.2314095199108124\n",
      "Epoch 329/440, Loss: 1.231309413909912\n",
      "Epoch 330/440, Loss: 1.231307864189148\n",
      "Epoch 331/440, Loss: 1.2314363420009613\n",
      "Epoch 332/440, Loss: 1.2313766479492188\n",
      "Epoch 333/440, Loss: 1.2314293682575226\n",
      "Epoch 334/440, Loss: 1.2312843799591064\n",
      "Epoch 335/440, Loss: 1.2313125133514404\n",
      "Epoch 336/440, Loss: 1.2313593029975891\n",
      "Epoch 337/440, Loss: 1.2314105033874512\n",
      "Epoch 338/440, Loss: 1.2314273118972778\n",
      "Epoch 339/440, Loss: 1.2313378751277924\n",
      "Epoch 340/440, Loss: 1.2313637435436249\n",
      "Epoch 341/440, Loss: 1.231336772441864\n",
      "Epoch 342/440, Loss: 1.2313224375247955\n",
      "Epoch 343/440, Loss: 1.231287956237793\n",
      "Epoch 344/440, Loss: 1.2313310503959656\n",
      "Epoch 345/440, Loss: 1.231344223022461\n",
      "Epoch 346/440, Loss: 1.2312808334827423\n",
      "Epoch 347/440, Loss: 1.2313134968280792\n",
      "Epoch 348/440, Loss: 1.2314118146896362\n",
      "Epoch 349/440, Loss: 1.2312513589859009\n",
      "Epoch 350/440, Loss: 1.231318324804306\n",
      "Epoch 351/440, Loss: 1.2312319576740265\n",
      "Epoch 352/440, Loss: 1.231263905763626\n",
      "Epoch 353/440, Loss: 1.2314104437828064\n",
      "Epoch 354/440, Loss: 1.231296867132187\n",
      "Epoch 355/440, Loss: 1.2313939332962036\n",
      "Epoch 356/440, Loss: 1.2313012182712555\n",
      "Epoch 357/440, Loss: 1.2312884032726288\n",
      "Epoch 358/440, Loss: 1.2313920557498932\n",
      "Epoch 359/440, Loss: 1.2313219606876373\n",
      "Epoch 360/440, Loss: 1.2312384843826294\n",
      "Epoch 361/440, Loss: 1.2312604188919067\n",
      "Epoch 362/440, Loss: 1.2312500178813934\n",
      "Epoch 363/440, Loss: 1.2312509715557098\n",
      "Epoch 364/440, Loss: 1.2313773334026337\n",
      "Epoch 365/440, Loss: 1.2312356233596802\n",
      "Epoch 366/440, Loss: 1.2312939763069153\n",
      "Epoch 367/440, Loss: 1.2312211394309998\n",
      "Epoch 368/440, Loss: 1.2312133014202118\n",
      "Epoch 369/440, Loss: 1.2313104271888733\n",
      "Epoch 370/440, Loss: 1.231338381767273\n",
      "Epoch 371/440, Loss: 1.2313355505466461\n",
      "Epoch 372/440, Loss: 1.2313701808452606\n",
      "Epoch 373/440, Loss: 1.2313960492610931\n",
      "Epoch 374/440, Loss: 1.2313408851623535\n",
      "Epoch 375/440, Loss: 1.2313091456890106\n",
      "Epoch 376/440, Loss: 1.2313445806503296\n",
      "Epoch 377/440, Loss: 1.2313423454761505\n",
      "Epoch 378/440, Loss: 1.2313013076782227\n",
      "Epoch 379/440, Loss: 1.2312975823879242\n",
      "Epoch 380/440, Loss: 1.231229990720749\n",
      "Epoch 381/440, Loss: 1.2313242256641388\n",
      "Epoch 382/440, Loss: 1.23130664229393\n",
      "Epoch 383/440, Loss: 1.2312055230140686\n",
      "Epoch 384/440, Loss: 1.231227070093155\n",
      "Epoch 385/440, Loss: 1.231257677078247\n",
      "Epoch 386/440, Loss: 1.231215387582779\n",
      "Epoch 387/440, Loss: 1.2311980724334717\n",
      "Epoch 388/440, Loss: 1.2312773168087006\n",
      "Epoch 389/440, Loss: 1.2312156856060028\n",
      "Epoch 390/440, Loss: 1.2312673926353455\n",
      "Epoch 391/440, Loss: 1.231271207332611\n",
      "Epoch 392/440, Loss: 1.2313583195209503\n",
      "Epoch 393/440, Loss: 1.2314056158065796\n",
      "Epoch 394/440, Loss: 1.231368601322174\n",
      "Epoch 395/440, Loss: 1.2312961220741272\n",
      "Epoch 396/440, Loss: 1.2313847243785858\n",
      "Epoch 397/440, Loss: 1.2311798334121704\n",
      "Epoch 398/440, Loss: 1.2312088012695312\n",
      "Epoch 399/440, Loss: 1.2312989234924316\n",
      "Epoch 400/440, Loss: 1.2311546504497528\n",
      "Epoch 401/440, Loss: 1.2313256859779358\n",
      "Epoch 402/440, Loss: 1.2311756312847137\n",
      "Epoch 403/440, Loss: 1.2312881350517273\n",
      "Epoch 404/440, Loss: 1.231298714876175\n",
      "Epoch 405/440, Loss: 1.231188416481018\n",
      "Epoch 406/440, Loss: 1.2313507795333862\n",
      "Epoch 407/440, Loss: 1.2312922477722168\n",
      "Epoch 408/440, Loss: 1.2312888503074646\n",
      "Epoch 409/440, Loss: 1.2312589287757874\n",
      "Epoch 410/440, Loss: 1.2311971187591553\n",
      "Epoch 411/440, Loss: 1.231344759464264\n",
      "Epoch 412/440, Loss: 1.231309324502945\n",
      "Epoch 413/440, Loss: 1.2313174605369568\n",
      "Epoch 414/440, Loss: 1.231160819530487\n",
      "Epoch 415/440, Loss: 1.231300175189972\n",
      "Epoch 416/440, Loss: 1.231224685907364\n",
      "Epoch 417/440, Loss: 1.2311223447322845\n",
      "Epoch 418/440, Loss: 1.2312567234039307\n",
      "Epoch 419/440, Loss: 1.2312907576560974\n",
      "Epoch 420/440, Loss: 1.2312511205673218\n",
      "Epoch 421/440, Loss: 1.2313076555728912\n",
      "Epoch 422/440, Loss: 1.2311704754829407\n",
      "Epoch 423/440, Loss: 1.2311713695526123\n",
      "Epoch 424/440, Loss: 1.2312112152576447\n",
      "Epoch 425/440, Loss: 1.2312069535255432\n",
      "Epoch 426/440, Loss: 1.2311506569385529\n",
      "Epoch 427/440, Loss: 1.2312631011009216\n",
      "Epoch 428/440, Loss: 1.2311844229698181\n",
      "Epoch 429/440, Loss: 1.2311335802078247\n",
      "Epoch 430/440, Loss: 1.2311105132102966\n",
      "Epoch 431/440, Loss: 1.2311280071735382\n",
      "Epoch 432/440, Loss: 1.2311513721942902\n",
      "Epoch 433/440, Loss: 1.2311151921749115\n",
      "Epoch 434/440, Loss: 1.231202632188797\n",
      "Epoch 435/440, Loss: 1.2311499118804932\n",
      "Epoch 436/440, Loss: 1.231257826089859\n",
      "Epoch 437/440, Loss: 1.231216311454773\n",
      "Epoch 438/440, Loss: 1.231310248374939\n",
      "Epoch 439/440, Loss: 1.2311715483665466\n",
      "Epoch 440/440, Loss: 1.231141448020935\n"
     ]
    }
   ],
   "source": [
    "train_model(model2, train_loader, y_train_tensors, criterion, optimizer2, num_epochs=440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-MgaKDfGw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
