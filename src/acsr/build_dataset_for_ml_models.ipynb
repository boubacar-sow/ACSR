{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "annotated_frames_dir = \"/scratch2/bsow/Documents/ACSR/data/training_videos/annotated_frames\"\n",
    "extracted_features_dir = \"/scratch2/bsow/Documents/ACSR/output/extracted_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotated_frames(video_name):\n",
    "    \"\"\"Load annotated frames for a specific video.\"\"\"\n",
    "    annotated_path = os.path.join(annotated_frames_dir, f\"{video_name}.csv\")\n",
    "    annotated_frames = pd.read_csv(annotated_path)\n",
    "    \n",
    "    # Rename 'frame' column to 'frame_number' to match features\n",
    "    if 'frame' in annotated_frames.columns:\n",
    "        annotated_frames.rename(columns={'frame': 'frame_number'}, inplace=True)\n",
    "    \n",
    "    return annotated_frames\n",
    "\n",
    "def load_extracted_features(video_name):\n",
    "    \"\"\"Load extracted features for a specific video.\"\"\"\n",
    "    features_path = os.path.join(extracted_features_dir, f\"{video_name}_features.csv\")\n",
    "    return pd.read_csv(features_path)\n",
    "\n",
    "def filter_features_for_annotated_frames(annotated_frames, features):\n",
    "    \"\"\"\n",
    "    Merge annotated frames with extracted features based on frame_number.\n",
    "    \"\"\"\n",
    "    # Merge on the 'frame_number' column\n",
    "    merged_df = pd.merge(\n",
    "        features,  # Extracted features\n",
    "        annotated_frames[[\"frame_number\", \"shape\", \"position\"]],  # Annotations\n",
    "        on=\"frame_number\",  # Merge key\n",
    "        how=\"inner\"  # Keep only rows with matching frame_number\n",
    "    )\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of video names\n",
    "video_names = [f\"sent_{i:02d}\" for i in range(1, 21)]\n",
    "\n",
    "# Load and combine data for the first 14 videos (training/validation)\n",
    "train_val_data = []\n",
    "for video_name in video_names[:19]:\n",
    "    annotated_frames = load_annotated_frames(video_name)\n",
    "    features = load_extracted_features(video_name)\n",
    "    filtered_features = filter_features_for_annotated_frames(annotated_frames, features)\n",
    "    train_val_data.append(filtered_features)\n",
    "\n",
    "train_val_df = pd.concat(train_val_data, ignore_index=True)\n",
    "train_val_df.dropna(inplace=True)\n",
    "\n",
    "# Load data for the 2 last videos for testing\n",
    "test_data = []\n",
    "for video_name in video_names[19:]:\n",
    "    annotated_frames = load_annotated_frames(video_name)\n",
    "    features = load_extracted_features(video_name)\n",
    "    filtered_features = filter_features_for_annotated_frames(annotated_frames, features)\n",
    "    test_data.append(filtered_features)\n",
    "\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all features except the first two columns (fn_video and frame_number)\n",
    "feature_columns = train_val_df.columns[2:-2]  # Skip the first two columns\n",
    "\n",
    "# Split into features and labels\n",
    "X = train_val_df[feature_columns]  # All features\n",
    "y_shape = train_val_df[\"shape\"]  # Shape labels\n",
    "y_position = train_val_df[\"position\"]  # Position labels\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_shape_train, y_shape_val = train_test_split(X, y_shape, test_size=0.2, random_state=42)\n",
    "_, _, y_position_train, y_position_val = train_test_split(X, y_position, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train shape recognition model\n",
    "shape_model = RandomForestClassifier(random_state=42)\n",
    "shape_model.fit(X_train, y_shape_train)\n",
    "\n",
    "# Train position recognition model\n",
    "position_model = RandomForestClassifier(random_state=42)\n",
    "position_model.fit(X_train, y_position_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.86      0.86         7\n",
      "           2       0.67      1.00      0.80         6\n",
      "           3       0.87      0.91      0.89        22\n",
      "           4       0.86      0.55      0.67        11\n",
      "           5       0.79      1.00      0.88        11\n",
      "           6       0.75      0.60      0.67         5\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.80        69\n",
      "   macro avg       0.67      0.68      0.66        69\n",
      "weighted avg       0.79      0.80      0.78        69\n",
      "\n",
      "Accuracy: 0.80\n",
      "Position Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      1.00      0.92        30\n",
      "         2.0       1.00      1.00      1.00         5\n",
      "         3.0       0.94      0.80      0.86        20\n",
      "         4.0       1.00      0.80      0.89         5\n",
      "         5.0       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.91        69\n",
      "   macro avg       0.96      0.90      0.92        69\n",
      "weighted avg       0.92      0.91      0.91        69\n",
      "\n",
      "Accuracy: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate shape model\n",
    "y_shape_pred = shape_model.predict(X_val)\n",
    "print(\"Shape Model Evaluation:\")\n",
    "print(classification_report(y_shape_val, y_shape_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_shape_val, y_shape_pred):.2f}\")\n",
    "\n",
    "# Evaluate position model\n",
    "y_position_pred = position_model.predict(X_val)\n",
    "print(\"Position Model Evaluation:\")\n",
    "print(classification_report(y_position_val, y_position_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_position_val, y_position_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Model Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       0.83      1.00      0.91         5\n",
      "           4       0.33      0.33      0.33         3\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.52      0.55      0.51        15\n",
      "weighted avg       0.71      0.67      0.66        15\n",
      "\n",
      "Accuracy: 0.67\n",
      "Position Model Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.83      0.71         6\n",
      "           3       0.75      0.50      0.60         6\n",
      "           4       1.00      0.50      0.67         2\n",
      "           5       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.72      0.71      0.66        15\n",
      "weighted avg       0.72      0.67      0.66        15\n",
      "\n",
      "Accuracy: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bsow/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test = test_df[feature_columns]  # All features (excluding shape and position)\n",
    "y_shape_test = test_df[\"shape\"]  # Shape labels\n",
    "y_position_test = test_df[\"position\"]  # Position labels\n",
    "\n",
    "# Test shape model\n",
    "y_shape_test_pred = shape_model.predict(X_test)\n",
    "print(\"Shape Model Test Results:\")\n",
    "print(classification_report(y_shape_test, y_shape_test_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_shape_test, y_shape_test_pred):.2f}\")\n",
    "\n",
    "# Test position model\n",
    "y_position_test_pred = position_model.predict(X_test)\n",
    "print(\"Position Model Test Results:\")\n",
    "print(classification_report(y_position_test, y_position_test_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_position_test, y_position_test_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save shape model with feature names\n",
    "with open(r\"/scratch2/bsow/Documents/ACSR/output/saved_models/model_rf_shape.pkl\", \"wb\") as f:\n",
    "    pickle.dump((shape_model, feature_columns), f)\n",
    "\n",
    "# Save position model with feature names\n",
    "with open(r\"/scratch2/bsow/Documents/ACSR/output/saved_models/model_rf_position.pkl\", \"wb\") as f:\n",
    "    pickle.dump((position_model, feature_columns), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /scratch2/bsow/Documents/ACSR/output/saved_models/model_rf_position.pkl:\n",
      "RandomForestClassifier(random_state=42)\n",
      "Type of contents: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def inspect_model_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        contents = pickle.load(f)\n",
    "        print(f\"Contents of {filename}:\")\n",
    "        print(contents)\n",
    "        print(f\"Type of contents: {type(contents)}\")\n",
    "        if isinstance(contents, (list, tuple)):\n",
    "            print(f\"Number of items: {len(contents)}\")\n",
    "            for i, item in enumerate(contents):\n",
    "                print(f\"Item {i}: {item} (Type: {type(item)})\")\n",
    "\n",
    "# Example usage\n",
    "inspect_model_file(\"/scratch2/bsow/Documents/ACSR/output/saved_models/model_rf_position.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = r\"C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR\\output\\extracted_features_mp4\"\n",
    "annotations_dir = r\"C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR\\output\\annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn_video</th>\n",
       "      <th>frame_number</th>\n",
       "      <th>distance_face130_r_hand8</th>\n",
       "      <th>tan_angle_face130_r_hand8</th>\n",
       "      <th>distance_face152_r_hand8</th>\n",
       "      <th>tan_angle_face152_r_hand8</th>\n",
       "      <th>distance_face94_r_hand8</th>\n",
       "      <th>tan_angle_face94_r_hand8</th>\n",
       "      <th>distance_face130_r_hand9</th>\n",
       "      <th>tan_angle_face130_r_hand9</th>\n",
       "      <th>...</th>\n",
       "      <th>acceleration_x_r_hand8</th>\n",
       "      <th>acceleration_y_r_hand8</th>\n",
       "      <th>velocity_x_r_hand9</th>\n",
       "      <th>velocity_y_r_hand9</th>\n",
       "      <th>acceleration_x_r_hand9</th>\n",
       "      <th>acceleration_y_r_hand9</th>\n",
       "      <th>velocity_x_r_hand12</th>\n",
       "      <th>velocity_y_r_hand12</th>\n",
       "      <th>acceleration_x_r_hand12</th>\n",
       "      <th>acceleration_y_r_hand12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>csf001.mp4</td>\n",
       "      <td>22</td>\n",
       "      <td>4.544161</td>\n",
       "      <td>0.215778</td>\n",
       "      <td>2.993590</td>\n",
       "      <td>0.198311</td>\n",
       "      <td>3.952022</td>\n",
       "      <td>0.148803</td>\n",
       "      <td>5.012093</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>csf001.mp4</td>\n",
       "      <td>23</td>\n",
       "      <td>4.432777</td>\n",
       "      <td>0.226310</td>\n",
       "      <td>2.886811</td>\n",
       "      <td>0.213178</td>\n",
       "      <td>3.815771</td>\n",
       "      <td>0.157548</td>\n",
       "      <td>5.075631</td>\n",
       "      <td>0.046591</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016066</td>\n",
       "      <td>0.009251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>-0.019512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>csf001.mp4</td>\n",
       "      <td>24</td>\n",
       "      <td>4.395037</td>\n",
       "      <td>0.227604</td>\n",
       "      <td>2.859063</td>\n",
       "      <td>0.215205</td>\n",
       "      <td>3.769736</td>\n",
       "      <td>0.158046</td>\n",
       "      <td>5.071182</td>\n",
       "      <td>0.037993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003090</td>\n",
       "      <td>0.012441</td>\n",
       "      <td>-0.006354</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>-0.010821</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>0.019263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>csf001.mp4</td>\n",
       "      <td>25</td>\n",
       "      <td>2.349447</td>\n",
       "      <td>0.122057</td>\n",
       "      <td>0.855116</td>\n",
       "      <td>-0.115871</td>\n",
       "      <td>1.760024</td>\n",
       "      <td>-0.052596</td>\n",
       "      <td>3.514490</td>\n",
       "      <td>-0.052680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099307</td>\n",
       "      <td>-0.273851</td>\n",
       "      <td>-0.055388</td>\n",
       "      <td>-0.227089</td>\n",
       "      <td>-0.049035</td>\n",
       "      <td>-0.225518</td>\n",
       "      <td>-0.099749</td>\n",
       "      <td>-0.339577</td>\n",
       "      <td>-0.100445</td>\n",
       "      <td>-0.339328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>csf001.mp4</td>\n",
       "      <td>26</td>\n",
       "      <td>2.048286</td>\n",
       "      <td>0.077550</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>-0.449189</td>\n",
       "      <td>1.485955</td>\n",
       "      <td>-0.150858</td>\n",
       "      <td>3.263714</td>\n",
       "      <td>-0.084589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082108</td>\n",
       "      <td>0.241587</td>\n",
       "      <td>-0.013242</td>\n",
       "      <td>-0.036473</td>\n",
       "      <td>0.042146</td>\n",
       "      <td>0.190616</td>\n",
       "      <td>-0.021206</td>\n",
       "      <td>-0.033463</td>\n",
       "      <td>0.078543</td>\n",
       "      <td>0.306113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fn_video  frame_number  distance_face130_r_hand8  \\\n",
       "21  csf001.mp4            22                  4.544161   \n",
       "22  csf001.mp4            23                  4.432777   \n",
       "23  csf001.mp4            24                  4.395037   \n",
       "24  csf001.mp4            25                  2.349447   \n",
       "25  csf001.mp4            26                  2.048286   \n",
       "\n",
       "    tan_angle_face130_r_hand8  distance_face152_r_hand8  \\\n",
       "21                   0.215778                  2.993590   \n",
       "22                   0.226310                  2.886811   \n",
       "23                   0.227604                  2.859063   \n",
       "24                   0.122057                  0.855116   \n",
       "25                   0.077550                  0.628814   \n",
       "\n",
       "    tan_angle_face152_r_hand8  distance_face94_r_hand8  \\\n",
       "21                   0.198311                 3.952022   \n",
       "22                   0.213178                 3.815771   \n",
       "23                   0.215205                 3.769736   \n",
       "24                  -0.115871                 1.760024   \n",
       "25                  -0.449189                 1.485955   \n",
       "\n",
       "    tan_angle_face94_r_hand8  distance_face130_r_hand9  \\\n",
       "21                  0.148803                  5.012093   \n",
       "22                  0.157548                  5.075631   \n",
       "23                  0.158046                  5.071182   \n",
       "24                 -0.052596                  3.514490   \n",
       "25                 -0.150858                  3.263714   \n",
       "\n",
       "    tan_angle_face130_r_hand9  ...  acceleration_x_r_hand8  \\\n",
       "21                   0.068935  ...                     NaN   \n",
       "22                   0.046591  ...                     NaN   \n",
       "23                   0.037993  ...               -0.003090   \n",
       "24                  -0.052680  ...               -0.099307   \n",
       "25                  -0.084589  ...                0.082108   \n",
       "\n",
       "    acceleration_y_r_hand8  velocity_x_r_hand9  velocity_y_r_hand9  \\\n",
       "21                     NaN                 NaN                 NaN   \n",
       "22                     NaN           -0.016066            0.009251   \n",
       "23                0.012441           -0.006354           -0.001570   \n",
       "24               -0.273851           -0.055388           -0.227089   \n",
       "25                0.241587           -0.013242           -0.036473   \n",
       "\n",
       "    acceleration_x_r_hand9  acceleration_y_r_hand9  velocity_x_r_hand12  \\\n",
       "21                     NaN                     NaN                  NaN   \n",
       "22                     NaN                     NaN             0.009077   \n",
       "23                0.009712               -0.010821             0.000696   \n",
       "24               -0.049035               -0.225518            -0.099749   \n",
       "25                0.042146                0.190616            -0.021206   \n",
       "\n",
       "    velocity_y_r_hand12  acceleration_x_r_hand12  acceleration_y_r_hand12  \n",
       "21                  NaN                      NaN                      NaN  \n",
       "22            -0.019512                      NaN                      NaN  \n",
       "23            -0.000249                -0.008381                 0.019263  \n",
       "24            -0.339577                -0.100445                -0.339328  \n",
       "25            -0.033463                 0.078543                 0.306113  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\bouba\\OneDrive\\Documents\\ACSR\\ACSR\\output\\extracted_features_mp4\\csf001_features.csv\")\n",
    "df.dropna(how='all', inplace=True, subset=df.columns[-25:])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(features_path, annotations_path):\n",
    "    # Load the feature file\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    \n",
    "    # Load the annotation file\n",
    "    # Use header=0 to indicate that the first row is the header\n",
    "    annotations_df = pd.read_csv(annotations_path, header=0, names=['frame', 'shape', 'position'])\n",
    "    \n",
    "    # Convert the 'frame' column to integers\n",
    "    annotations_df['frame'] = pd.to_numeric(annotations_df['frame'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid frame numbers (e.g., NaN after conversion)\n",
    "    # annotations_df.dropna(subset=['frame'], inplace=True)\n",
    "    # annotations_df['frame'] = annotations_df['frame'].astype(int)\n",
    "    \n",
    "    # Strip whitespace from 'shape' and 'position' columns\n",
    "    annotations_df['shape'] = annotations_df['shape'].astype(str).str.strip()\n",
    "    annotations_df['position'] = annotations_df['position'].astype(str).str.strip()\n",
    "    \n",
    "    # Replace '_' with NaN in the 'shape' and 'position' columns\n",
    "    annotations_df['shape'] = annotations_df['shape'].replace('_', np.nan)\n",
    "    annotations_df['position'] = annotations_df['position'].replace('_', np.nan)\n",
    "    \n",
    "    # Find the first and last non-empty frame in the features DataFrame\n",
    "    # Drop rows where all feature columns are NaN\n",
    "    non_empty_features_df = features_df.dropna(how='all', subset=features_df.columns[2:])\n",
    "    first_non_empty_frame = non_empty_features_df['frame_number'].min()\n",
    "    last_non_empty_frame = non_empty_features_df['frame_number'].max()\n",
    "    \n",
    "    # Adjust the first and last frame in the annotations to match the first and last non-empty frame in the features\n",
    "    if annotations_df.iloc[0]['frame'] != first_non_empty_frame:\n",
    "        print(f\"Adjusting first annotation frame from {annotations_df.iloc[0]['frame']} to {first_non_empty_frame}\")\n",
    "        annotations_df.iloc[0, annotations_df.columns.get_loc('frame')] = first_non_empty_frame\n",
    "    if annotations_df.iloc[-1]['frame'] != last_non_empty_frame:\n",
    "        print(f\"Adjusting last annotation frame from {annotations_df.iloc[-1]['frame']} to {last_non_empty_frame}\")\n",
    "        annotations_df.iloc[-1, annotations_df.columns.get_loc('frame')] = last_non_empty_frame\n",
    "    \n",
    "    # Add shape and position columns to the features DataFrame\n",
    "    # Explicitly cast to object (string) type to avoid dtype warnings\n",
    "    features_df['shape'] = None\n",
    "    features_df['shape'] = features_df['shape'].astype(object)\n",
    "    features_df['position'] = None\n",
    "    features_df['position'] = features_df['position'].astype(object)\n",
    "    \n",
    "    # Set shape and position to NaN for frames before the first annotation frame\n",
    "    first_annotation_frame = annotations_df.iloc[0]['frame']\n",
    "    features_df.loc[features_df['frame_number'] < first_annotation_frame, 'shape'] = np.nan\n",
    "    features_df.loc[features_df['frame_number'] < first_annotation_frame, 'position'] = np.nan\n",
    "    \n",
    "    # Iterate through the annotation rows and fill in the shape and position\n",
    "    for i in range(len(annotations_df) - 1):\n",
    "        start_frame = annotations_df.iloc[i]['frame']\n",
    "        end_frame = annotations_df.iloc[i + 1]['frame']\n",
    "        shape = annotations_df.iloc[i]['shape']\n",
    "        position = annotations_df.iloc[i]['position']\n",
    "        \n",
    "        # Print the values being assigned for debugging\n",
    "        print(f\"Assigning shape={shape}, position={position} for frames {start_frame} to {end_frame - 1}\")\n",
    "        \n",
    "        # Fill in the shape and position for the range of frames\n",
    "        features_df.loc[(features_df['frame_number'] >= start_frame) & \n",
    "                        (features_df['frame_number'] < end_frame), 'shape'] = shape\n",
    "        features_df.loc[(features_df['frame_number'] >= start_frame) & \n",
    "                        (features_df['frame_number'] < end_frame), 'position'] = position\n",
    "    \n",
    "    # Handle the last row of annotations\n",
    "    last_annotation_frame = annotations_df.iloc[-1]['frame']\n",
    "    shape = annotations_df.iloc[-1]['shape']\n",
    "    position = annotations_df.iloc[-1]['position']\n",
    "    \n",
    "    # Print the values being assigned for debugging\n",
    "    print(f\"Assigning shape={shape}, position={position} for frames {last_annotation_frame} to end\")\n",
    "    \n",
    "    # Fill in the shape and position for the last range\n",
    "    features_df.loc[features_df['frame_number'] >= last_annotation_frame, 'shape'] = shape\n",
    "    features_df.loc[features_df['frame_number'] >= last_annotation_frame, 'position'] = position\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting first annotation frame from 25 to 22\n",
      "Adjusting last annotation frame from 133 to 131\n",
      "Assigning shape=5, position=1 for frames 22 to 33\n",
      "Assigning shape=6, position=1 for frames 34 to 40\n",
      "Assigning shape=6, position=2 for frames 41 to 46\n",
      "Assigning shape=5, position=3 for frames 47 to 54\n",
      "Assigning shape=0, position=0 for frames 55 to 56\n",
      "Assigning shape=2, position=1 for frames 57 to 65\n",
      "Assigning shape=2, position=4 for frames 66 to 74\n",
      "Assigning shape=3, position=4 for frames 75 to 85\n",
      "Assigning shape=3, position=3 for frames 86 to 123\n",
      "Assigning shape=3, position=0 for frames 124 to 126\n",
      "Assigning shape=0, position=0 for frames 127 to 130\n",
      "Assigning shape=nan, position=nan for frames 131 to end\n",
      "Adjusting first annotation frame from 29 to 27\n",
      "Adjusting last annotation frame from 373 to 372\n",
      "Assigning shape=5, position=0 for frames 27 to 35\n",
      "Assigning shape=5, position=3 for frames 36 to 45\n",
      "Assigning shape=6, position=1 for frames 46 to 53\n",
      "Assigning shape=0, position=0 for frames 54 to 61\n",
      "Assigning shape=3, position=2 for frames 62 to 75\n",
      "Assigning shape=0, position=0 for frames 76 to 76\n",
      "Assigning shape=7, position=1 for frames 77 to 86\n",
      "Assigning shape=0, position=0 for frames 87 to 91\n",
      "Assigning shape=3, position=0 for frames 92 to 93\n",
      "Assigning shape=3, position=2 for frames 94 to 95\n",
      "Assigning shape=3, position=3 for frames 96 to 104\n",
      "Assigning shape=5, position=3 for frames 105 to 119\n",
      "Assigning shape=3, position=3 for frames 120 to 130\n",
      "Assigning shape=3, position=2 for frames 131 to 141\n",
      "Assigning shape=0, position=0 for frames 142 to 142\n",
      "Assigning shape=0, position=0 for frames 143 to 143\n",
      "Assigning shape=1, position=2 for frames 144 to 147\n",
      "Assigning shape=1, position=5 for frames 148 to 162\n",
      "Assigning shape=0, position=5 for frames 163 to 168\n",
      "Assigning shape=3, position=4 for frames 169 to 170\n",
      "Assigning shape=3, position=0 for frames 171 to 173\n",
      "Assigning shape=3, position=3 for frames 174 to 178\n",
      "Assigning shape=1, position=1 for frames 179 to 184\n",
      "Assigning shape=6, position=1 for frames 185 to 202\n",
      "Assigning shape=5, position=1 for frames 203 to 210\n",
      "Assigning shape=0, position=0 for frames 211 to 214\n",
      "Assigning shape=2, position=0 for frames 215 to 217\n",
      "Assigning shape=2, position=4 for frames 218 to 228\n",
      "Assigning shape=2, position=0 for frames 229 to 234\n",
      "Assigning shape=3, position=0 for frames 235 to 236\n",
      "Assigning shape=3, position=2 for frames 237 to 257\n",
      "Assigning shape=4, position=2 for frames 258 to 263\n",
      "Assigning shape=4, position=3 for frames 264 to 275\n",
      "Assigning shape=0, position=0 for frames 276 to 279\n",
      "Assigning shape=2, position=1 for frames 280 to 291\n",
      "Assigning shape=1, position=1 for frames 292 to 293\n",
      "Assigning shape=1, position=5 for frames 294 to 306\n",
      "Assigning shape=6, position=0 for frames 307 to 312\n",
      "Assigning shape=6, position=3 for frames 313 to 360\n",
      "Assigning shape=6, position=0 for frames 361 to 366\n",
      "Assigning shape=0, position=0 for frames 367 to 371\n",
      "Assigning shape=nan, position=nan for frames 372 to end\n",
      "Adjusting first annotation frame from 36 to 34\n",
      "Adjusting last annotation frame from 179 to 176\n",
      "Assigning shape=2, position=0 for frames 34 to 41\n",
      "Assigning shape=2, position=1 for frames 42 to 51\n",
      "Assigning shape=6, position=1 for frames 52 to 72\n",
      "Assigning shape=6, position=0 for frames 73 to 74\n",
      "Assigning shape=6, position=5 for frames 75 to 90\n",
      "Assigning shape=4, position=4 for frames 91 to 107\n",
      "Assigning shape=0, position=4 for frames 108 to 113\n",
      "Assigning shape=1, position=0 for frames 114 to 115\n",
      "Assigning shape=1, position=3 for frames 116 to 167\n",
      "Assigning shape=1, position=0 for frames 168 to 172\n",
      "Assigning shape=1, position=0 for frames 173 to 175\n",
      "Assigning shape=0, position=0 for frames 176 to 175\n",
      "Assigning shape=nan, position=nan for frames 176 to end\n",
      "Adjusting first annotation frame from 36 to 39\n",
      "Adjusting last annotation frame from 179 to 335\n",
      "Assigning shape=2, position=0 for frames 39 to 41\n",
      "Assigning shape=2, position=1 for frames 42 to 51\n",
      "Assigning shape=6, position=1 for frames 52 to 72\n",
      "Assigning shape=6, position=0 for frames 73 to 74\n",
      "Assigning shape=6, position=5 for frames 75 to 90\n",
      "Assigning shape=4, position=4 for frames 91 to 107\n",
      "Assigning shape=0, position=4 for frames 108 to 113\n",
      "Assigning shape=1, position=0 for frames 114 to 115\n",
      "Assigning shape=1, position=3 for frames 116 to 167\n",
      "Assigning shape=1, position=0 for frames 168 to 172\n",
      "Assigning shape=1, position=0 for frames 173 to 175\n",
      "Assigning shape=0, position=0 for frames 176 to 334\n",
      "Assigning shape=nan, position=nan for frames 335 to end\n",
      "Adjusting first annotation frame from 44 to 46\n",
      "Adjusting last annotation frame from 180 to 177\n",
      "Assigning shape=0, position=0 for frames 46 to 46\n",
      "Assigning shape=1, position=0 for frames 47 to 56\n",
      "Assigning shape=1, position=4 for frames 57 to 67\n",
      "Assigning shape=4, position=5 for frames 68 to 81\n",
      "Assigning shape=1, position=0 for frames 82 to 85\n",
      "Assigning shape=1, position=2 for frames 86 to 101\n",
      "Assigning shape=5, position=3 for frames 102 to 116\n",
      "Assigning shape=2, position=0 for frames 117 to 123\n",
      "Assigning shape=2, position=4 for frames 124 to 167\n",
      "Assigning shape=2, position=5 for frames 168 to 170\n",
      "Assigning shape=0, position=0 for frames 171 to 176\n",
      "Assigning shape=nan, position=nan for frames 177 to end\n",
      "Adjusting first annotation frame from 41 to 40\n",
      "Adjusting last annotation frame from 352 to 349\n",
      "Assigning shape=0, position=0 for frames 40 to 42\n",
      "Assigning shape=6, position=0 for frames 43 to 47\n",
      "Assigning shape=6, position=5 for frames 48 to 61\n",
      "Assigning shape=6, position=0 for frames 62 to 66\n",
      "Assigning shape=1, position=0 for frames 67 to 70\n",
      "Assigning shape=1, position=2 for frames 71 to 85\n",
      "Assigning shape=2, position=0 for frames 86 to 86\n",
      "Assigning shape=2, position=1 for frames 87 to 96\n",
      "Assigning shape=4, position=1 for frames 97 to 100\n",
      "Assigning shape=5, position=1 for frames 101 to 104\n",
      "Assigning shape=8, position=1 for frames 105 to 111\n",
      "Assigning shape=8, position=3 for frames 112 to 126\n",
      "Assigning shape=0, position=0 for frames 127 to 131\n",
      "Assigning shape=3, position=2 for frames 132 to 149\n",
      "Assigning shape=3, position=3 for frames 150 to 160\n",
      "Assigning shape=3, position=0 for frames 161 to 163\n",
      "Assigning shape=5, position=1 for frames 164 to 175\n",
      "Assigning shape=0, position=1 for frames 176 to 177\n",
      "Assigning shape=3, position=1 for frames 178 to 187\n",
      "Assigning shape=5, position=1 for frames 188 to 192\n",
      "Assigning shape=5, position=5 for frames 193 to 209\n",
      "Assigning shape=5, position=0 for frames 210 to 212\n",
      "Assigning shape=1, position=0 for frames 213 to 217\n",
      "Assigning shape=1, position=2 for frames 218 to 236\n",
      "Assigning shape=0, position=0 for frames 237 to 237\n",
      "Assigning shape=5, position=1 for frames 238 to 255\n",
      "Assigning shape=3, position=1 for frames 256 to 341\n",
      "Assigning shape=0, position=0 for frames 342 to 348\n",
      "Assigning shape=nan, position=nan for frames 349 to end\n",
      "Adjusting first annotation frame from 46 to 43\n",
      "Adjusting last annotation frame from 214 to 213\n",
      "Assigning shape=5, position=0 for frames 43 to 54\n",
      "Assigning shape=5, position=4 for frames 55 to 59\n",
      "Assigning shape=0, position=0 for frames 60 to 64\n",
      "Assigning shape=8, position=0 for frames 65 to 71\n",
      "Assigning shape=8, position=2 for frames 72 to 85\n",
      "Assigning shape=0, position=0 for frames 86 to 88\n",
      "Assigning shape=5, position=1 for frames 89 to 99\n",
      "Assigning shape=6, position=1 for frames 100 to 119\n",
      "Assigning shape=5, position=1 for frames 120 to 129\n",
      "Assigning shape=3, position=1 for frames 130 to 133\n",
      "Assigning shape=3, position=0 for frames 134 to 137\n",
      "Assigning shape=3, position=3 for frames 138 to 196\n",
      "Assigning shape=3, position=4 for frames 197 to 200\n",
      "Assigning shape=3, position=5 for frames 201 to 204\n",
      "Assigning shape=0, position=0 for frames 205 to 212\n",
      "Assigning shape=nan, position=nan for frames 213 to end\n",
      "Adjusting first annotation frame from 38 to 36\n",
      "Adjusting last annotation frame from 323 to 325\n",
      "Assigning shape=5, position=0 for frames 36 to 49\n",
      "Assigning shape=5, position=1 for frames 50 to 63\n",
      "Assigning shape=4, position=1 for frames 64 to 64\n",
      "Assigning shape=4, position=0 for frames 65 to 71\n",
      "Assigning shape=3, position=nan for frames 72 to 83\n",
      "Assigning shape=0, position=3 for frames 84 to 84\n",
      "Assigning shape=3, position=3 for frames 85 to 104\n",
      "Assigning shape=4, position=0 for frames 105 to 121\n",
      "Assigning shape=4, position=3 for frames 122 to 144\n",
      "Assigning shape=6, position=1 for frames 145 to 157\n",
      "Assigning shape=6, position=0 for frames 158 to 161\n",
      "Assigning shape=6, position=2 for frames 162 to 174\n",
      "Assigning shape=1, position=1 for frames 175 to 176\n",
      "Assigning shape=1, position=0 for frames 177 to 187\n",
      "Assigning shape=1, position=2 for frames 188 to 197\n",
      "Assigning shape=0, position=0 for frames 198 to 203\n",
      "Assigning shape=5, position=5 for frames 204 to 220\n",
      "Assigning shape=0, position=0 for frames 221 to 224\n",
      "Assigning shape=1, position=1 for frames 225 to 233\n",
      "Assigning shape=0, position=0 for frames 234 to 237\n",
      "Assigning shape=3, position=3 for frames 238 to 308\n",
      "Assigning shape=3, position=0 for frames 309 to 311\n",
      "Assigning shape=0, position=0 for frames 312 to 324\n",
      "Assigning shape=nan, position=nan for frames 325 to end\n",
      "Adjusting first annotation frame from 40 to 38\n",
      "Adjusting last annotation frame from 160 to 163\n",
      "Assigning shape=5, position=0 for frames 38 to 47\n",
      "Assigning shape=5, position=3 for frames 48 to 65\n",
      "Assigning shape=0, position=0 for frames 66 to 67\n",
      "Assigning shape=6, position=1 for frames 68 to 77\n",
      "Assigning shape=1, position=1 for frames 78 to 80\n",
      "Assigning shape=1, position=0 for frames 81 to 83\n",
      "Assigning shape=1, position=5 for frames 84 to 94\n",
      "Assigning shape=0, position=0 for frames 95 to 98\n",
      "Assigning shape=7, position=0 for frames 99 to 101\n",
      "Assigning shape=7, position=3 for frames 102 to 148\n",
      "Assigning shape=7, position=0 for frames 149 to 152\n",
      "Assigning shape=0, position=0 for frames 153 to 162\n",
      "Assigning shape=nan, position=nan for frames 163 to end\n",
      "Adjusting first annotation frame from 40 to 37\n",
      "Adjusting last annotation frame from 343 to 344\n",
      "Assigning shape=0, position=0 for frames 37 to 40\n",
      "Assigning shape=6, position=0 for frames 41 to 45\n",
      "Assigning shape=6, position=1 for frames 46 to 53\n",
      "Assigning shape=0, position=1 for frames 54 to 55\n",
      "Assigning shape=2, position=1 for frames 56 to 59\n",
      "Assigning shape=2, position=0 for frames 60 to 63\n",
      "Assigning shape=2, position=4 for frames 64 to 73\n",
      "Assigning shape=0, position=4 for frames 74 to 78\n",
      "Assigning shape=3, position=4 for frames 79 to 90\n",
      "Assigning shape=0, position=0 for frames 91 to 92\n",
      "Assigning shape=6, position=1 for frames 93 to 104\n",
      "Assigning shape=1, position=1 for frames 105 to 115\n",
      "Assigning shape=0, position=1 for frames 116 to 119\n",
      "Assigning shape=0, position=0 for frames 120 to 121\n",
      "Assigning shape=3, position=0 for frames 122 to 131\n",
      "Assigning shape=3, position=4 for frames 132 to 135\n",
      "Assigning shape=0, position=4 for frames 136 to 140\n",
      "Assigning shape=0, position=0 for frames 141 to 147\n",
      "Assigning shape=3, position=1 for frames 148 to 176\n",
      "Assigning shape=5, position=4 for frames 177 to 194\n",
      "Assigning shape=5, position=0 for frames 195 to 198\n",
      "Assigning shape=5, position=3 for frames 199 to 209\n",
      "Assigning shape=0, position=0 for frames 210 to 214\n",
      "Assigning shape=2, position=0 for frames 215 to 217\n",
      "Assigning shape=2, position=1 for frames 218 to 225\n",
      "Assigning shape=0, position=0 for frames 226 to 228\n",
      "Assigning shape=3, position=0 for frames 229 to 231\n",
      "Assigning shape=3, position=5 for frames 232 to 253\n",
      "Assigning shape=0, position=0 for frames 254 to 255\n",
      "Assigning shape=6, position=0 for frames 256 to 258\n",
      "Assigning shape=6, position=5 for frames 259 to 270\n",
      "Assigning shape=2, position=5 for frames 271 to 275\n",
      "Assigning shape=2, position=0 for frames 276 to 278\n",
      "Assigning shape=8, position=0 for frames 279 to 285\n",
      "Assigning shape=8, position=5 for frames 286 to 339\n",
      "Assigning shape=2, position=0 for frames 340 to 341\n",
      "Assigning shape=0, position=0 for frames 342 to 343\n",
      "Assigning shape=nan, position=nan for frames 344 to end\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store processed DataFrames\n",
    "processed_dfs = []\n",
    "\n",
    "# Iterate through all feature files\n",
    "for feature_file in os.listdir(features_dir):\n",
    "    if feature_file.endswith('.csv'):\n",
    "        # Construct the full path to the feature file\n",
    "        features_path = os.path.join(features_dir, feature_file)\n",
    "        \n",
    "        # Construct the corresponding annotation file path\n",
    "        annotation_file = feature_file.replace('_features.csv', '_annotations.csv')\n",
    "        annotations_path = os.path.join(annotations_dir, annotation_file)\n",
    "        \n",
    "        # Check if the annotation file exists\n",
    "        if os.path.exists(annotations_path):\n",
    "            # Process the files\n",
    "            processed_df = process_files(features_path, annotations_path)\n",
    "            processed_dfs.append(processed_df)\n",
    "        else:\n",
    "            print(f\"Annotation file not found for {feature_file}\")\n",
    "\n",
    "# Concatenate all processed DataFrames\n",
    "training_df = pd.concat(processed_dfs[:-1], ignore_index=True)\n",
    "testing_df = processed_dfs[-1]\n",
    "\n",
    "#training_df = pd.concat(processed_dfs[:3] + processed_dfs[4:], ignore_index=True)\n",
    "#testing_df = processed_dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1432, 28)\n",
      "X_val shape: (359, 28)\n",
      "y_shape_train shape: (1432,)\n",
      "y_shape_val shape: (359,)\n",
      "y_position_train shape: (1432,)\n",
      "y_position_val shape: (359,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove rows with missing values or NaNs\n",
    "training_df.dropna(inplace=True)\n",
    "testing_df.dropna(inplace=True)\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = training_df.drop(columns=['fn_video', 'frame_number', 'shape', 'position'])\n",
    "y_shape = training_df['shape']\n",
    "y_position = training_df['position']\n",
    "\n",
    "X_test = testing_df.drop(columns=['fn_video', 'frame_number', 'shape', 'position'])\n",
    "y_shape_test = testing_df['shape']\n",
    "y_position_test = testing_df['position']\n",
    "\n",
    "# Perform train-validation split (only once)\n",
    "X_train, X_val, y_shape_train, y_shape_val, y_position_train, y_position_val = train_test_split(\n",
    "    X_train, y_shape, y_position, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_shape_train shape:\", y_shape_train.shape)\n",
    "print(\"y_shape_val shape:\", y_shape_val.shape)\n",
    "print(\"y_position_train shape:\", y_position_train.shape)\n",
    "print(\"y_position_val shape:\", y_position_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        43\n",
      "           1       0.98      1.00      0.99        46\n",
      "           2       0.96      0.96      0.96        27\n",
      "           3       0.98      0.95      0.96        85\n",
      "           4       0.96      0.96      0.96        26\n",
      "           5       0.98      1.00      0.99        46\n",
      "           6       0.98      0.93      0.96        59\n",
      "           7       0.94      1.00      0.97        15\n",
      "           8       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.96       359\n",
      "   macro avg       0.96      0.97      0.96       359\n",
      "weighted avg       0.96      0.96      0.96       359\n",
      "\n",
      "Position Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        81\n",
      "           1       0.91      0.92      0.91        73\n",
      "           2       0.89      1.00      0.94        25\n",
      "           3       0.98      0.99      0.99       111\n",
      "           4       0.97      0.90      0.93        31\n",
      "           5       1.00      0.92      0.96        38\n",
      "\n",
      "    accuracy                           0.94       359\n",
      "   macro avg       0.94      0.94      0.94       359\n",
      "weighted avg       0.95      0.94      0.94       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a model for shape\n",
    "shape_model = RandomForestClassifier(random_state=42)\n",
    "shape_model.fit(X_train, y_shape_train)\n",
    "\n",
    "# Evaluate the shape model\n",
    "y_shape_pred = shape_model.predict(X_val)\n",
    "print(\"Shape Model Classification Report:\")\n",
    "print(classification_report(y_shape_val, y_shape_pred))\n",
    "\n",
    "# Train a model for position\n",
    "position_model = RandomForestClassifier(random_state=42)\n",
    "position_model.fit(X_train, y_position_train)\n",
    "\n",
    "# Evaluate the position model\n",
    "y_position_pred = position_model.predict(X_val)\n",
    "print(\"Position Model Classification Report:\")\n",
    "print(classification_report(y_position_val, y_position_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Model Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.53      0.39        43\n",
      "           1       0.06      0.55      0.11        11\n",
      "           2       0.88      0.59      0.71        39\n",
      "           3       0.82      0.46      0.59        80\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.84      0.94      0.89        33\n",
      "           6       0.67      0.35      0.46        40\n",
      "           8       0.00      0.00      0.00        61\n",
      "\n",
      "    accuracy                           0.44       307\n",
      "   macro avg       0.45      0.43      0.39       307\n",
      "weighted avg       0.55      0.44      0.46       307\n",
      "\n",
      "Position Model Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.65      0.39        71\n",
      "           1       0.81      0.44      0.57        78\n",
      "           3       0.34      1.00      0.51        11\n",
      "           4       0.85      0.54      0.66        54\n",
      "           5       0.84      0.33      0.48        93\n",
      "\n",
      "    accuracy                           0.49       307\n",
      "   macro avg       0.63      0.59      0.52       307\n",
      "weighted avg       0.69      0.49      0.51       307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on the test set\n",
    "y_shape_pred = shape_model.predict(X_test)\n",
    "y_position_pred = position_model.predict(X_test)\n",
    "\n",
    "# Print classification reports with zero_division=0 (default behavior)\n",
    "print(\"Shape Model Test Classification Report:\")\n",
    "print(classification_report(y_shape_test, y_shape_pred, zero_division=0))\n",
    "\n",
    "print(\"Position Model Test Classification Report:\")\n",
    "print(classification_report(y_position_test, y_position_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun 24 11:37:05 2022\n",
    "\n",
    "\"\"\"\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import cv2  # Import opencv\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp  # Import mediapipe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textgrids\n",
    "from scipy.signal import argrelextrema, savgol_filter\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model, feature_names = pickle.load(f)\n",
    "    return model, feature_names\n",
    "\n",
    "\n",
    "def load_video(path2file):\n",
    "    cap = cv2.VideoCapture(path2file)\n",
    "    cap.set(3,640) # camera width\n",
    "    cap.set(4,480) # camera height\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extract_class_from_fn(fn):\n",
    "    '''\n",
    "    get class number from filename, e.g.,\n",
    "    '4' from 'position_04.mp4'\n",
    "    '''\n",
    "    if fn is not None:\n",
    "        st = fn.find('_') + 1\n",
    "        ed = fn.find('.')\n",
    "        c = fn[st:ed]#.split('_')[0]\n",
    "        return int(c)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_distance(df_name, landmark1, landmark2, norm_factor=None):\n",
    "    '''\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_name : TYPE\n",
    "        DESCRIPTION.\n",
    "    landmark1 : STR\n",
    "        name of first landmark (e.g., hand20)\n",
    "    landmark2 : STR\n",
    "        name of second landmark (e.g., face234)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    series for dataframe\n",
    "    The distance between landmark1 and landmark2\n",
    "\n",
    "    '''\n",
    "\n",
    "    x1 = df_name[f'x_{landmark1}']\n",
    "    x2 = df_name[f'x_{landmark2}']\n",
    "    y1 = df_name[f'y_{landmark1}']\n",
    "    y2 = df_name[f'y_{landmark2}']\n",
    "    z1 = df_name[f'z_{landmark1}']\n",
    "    z2 = df_name[f'z_{landmark2}']\n",
    "    d = np.sqrt((x1-x2)**2 + (y1-y2)**2 + (z1-z2)**2)\n",
    "\n",
    "    # NORMALIZE\n",
    "    if norm_factor is not None:\n",
    "        d /= norm_factor\n",
    "\n",
    "    return  d\n",
    "\n",
    "def get_delta_dim(df_name, landmark1, landmark2, dim, norm_factor=None):\n",
    "    delta = df_name[f'{dim}_{landmark1}'] - df_name[f'{dim}_{landmark2}']\n",
    "    # NORMALIZE\n",
    "    if norm_factor is not None:\n",
    "        delta /= norm_factor\n",
    "    return  delta\n",
    "\n",
    "\n",
    "def get_frames_around_event(fn_video, frame_number, n_neighbor_frames):\n",
    "    st = frame_number - n_neighbor_frames\n",
    "    ed = frame_number + n_neighbor_frames + 1\n",
    "    frame_numbers = range(st, ed)\n",
    "\n",
    "    extracted_frames = []\n",
    "    cap = cv2.VideoCapture(fn_video)\n",
    "\n",
    "    for frame_number in frame_numbers:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            extracted_frames.append(frame)\n",
    "    cap.release()\n",
    "        \n",
    "    return extracted_frames\n",
    "\n",
    "\n",
    "def create_video_from_frames(fn_video, extracted_frames):\n",
    "    out = None\n",
    "    if extracted_frames:\n",
    "        height, width, _ = extracted_frames[0].shape\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for MP4 video\n",
    "        out = cv2.VideoWriter(fn_video, fourcc, 30.0, (width, height))\n",
    "        for frame in extracted_frames:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_coordinates(cap, fn_video, show_video=False, verbose=True):\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Extracting coordinates for: {fn_video}\")\n",
    "    mp_drawing = mp.solutions.drawing_utils  # Drawing helpers\n",
    "    mp_holistic = mp.solutions.holistic  # Mediapipe Solutions\n",
    "\n",
    "    columns = [\"fn_video\", \"frame_number\"]\n",
    "    num_coords_face = 468\n",
    "    num_coords_hand = 21\n",
    "\n",
    "    # generate columns names\n",
    "    for val in range(0, num_coords_face):\n",
    "        columns += [\n",
    "            \"x_face{}\".format(val),\n",
    "            \"y_face{}\".format(val),\n",
    "            \"z_face{}\".format(val),\n",
    "            \"v_face{}\".format(val),\n",
    "        ]\n",
    "\n",
    "    for val in range(0, num_coords_hand):\n",
    "        columns += [\n",
    "            \"x_r_hand{}\".format(val),\n",
    "            \"y_r_hand{}\".format(val),\n",
    "            \"z_r_hand{}\".format(val),\n",
    "            \"v_r_hand{}\".format(val),\n",
    "        ]\n",
    "\n",
    "    df_coords = pd.DataFrame(columns=columns)\n",
    "\n",
    "    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if verbose:\n",
    "        print(f\"Number of frames in video: {n_frames}\")\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        print(f\"Frames per second: {fps}\")\n",
    "        video_length = n_frames / fps\n",
    "        print(f\"Video length: {video_length} seconds\")\n",
    "    pbar = tqdm(total=n_frames)\n",
    "\n",
    "    # Initiate holistic model\n",
    "    i_frame = 0\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as holistic:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            i_frame += 1\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "            # Recolor Feed\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(\n",
    "                image\n",
    "            )\n",
    "\n",
    "            # Recolor image back to BGR for rendering\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # 4. Pose Detections\n",
    "            if show_video:\n",
    "                # Draw face landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.face_landmarks,\n",
    "                    mp_holistic.FACEMESH_TESSELATION,\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(80, 110, 10), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(80, 256, 121), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                # Right hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.right_hand_landmarks,\n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(80, 22, 10), thickness=2, circle_radius=4\n",
    "                    ),\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(80, 44, 121), thickness=2, circle_radius=2\n",
    "                    ),\n",
    "                )\n",
    "                # Pose landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(245, 117, 66), thickness=2, circle_radius=4\n",
    "                    ),\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(245, 66, 230), thickness=2, circle_radius=2\n",
    "                    ),\n",
    "                )\n",
    "                cv2.imshow(\"cued_estimated\", image)\n",
    "\n",
    "            # Export coordinates\n",
    "            if results.face_landmarks is not None:\n",
    "                face = results.face_landmarks.landmark\n",
    "                face_row = list(\n",
    "                    np.array(\n",
    "                        [\n",
    "                            [\n",
    "                                landmark.x, landmark.y, landmark.z,\n",
    "                                landmark.visibility\n",
    "                            ]\n",
    "                            for landmark in face\n",
    "                        ]\n",
    "                    ).flatten()\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                face_row = [None] * 4\n",
    "            # Extract right hand landmarks\n",
    "            if results.right_hand_landmarks is not None:\n",
    "                r_hand = results.right_hand_landmarks.landmark\n",
    "                r_hand_row = list(\n",
    "                    np.array(\n",
    "                        [\n",
    "                            [\n",
    "                                landmark.x, landmark.y, landmark.z,\n",
    "                                landmark.visibility\n",
    "                            ]\n",
    "                            for landmark in r_hand\n",
    "                        ]\n",
    "                    ).flatten()\n",
    "                )\n",
    "            else:\n",
    "                r_hand_row = [None] * 4\n",
    "\n",
    "            # Create the row that will be written in the file\n",
    "            row = [fn_video, i_frame] + face_row + r_hand_row\n",
    "            curr_df = pd.DataFrame(dict(zip(columns, row)), index=[0])\n",
    "            # print(i_frame, curr_df)\n",
    "            df_coords = pd.concat([df_coords, curr_df], ignore_index=True)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "                print(\"WARNING!\" * 5)\n",
    "                print('break due to cv2.waitKey(10) & 0xFF == ord(\"q\"')\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # print(len(df_coords), n_frames)\n",
    "    assert n_frames - df_coords.shape[0] <= 1\n",
    "\n",
    "    return df_coords\n",
    "\n",
    "\n",
    "def get_index_pairs(property_type):\n",
    "    index_pairs = []\n",
    "    if property_type == 'shape':\n",
    "        index_pairs.extend([\n",
    "            (2, 4), (5, 8), (9, 12), (13, 16), (17, 20),\n",
    "            (4, 5), (4, 8), (8, 12), (7, 11), (6, 10), \n",
    "            (4, 12), (4, 16), (4, 20),  # Thumb to other fingertips\n",
    "            (5, 9), (9, 13)   # Finger bases\n",
    "        ])\n",
    "    elif property_type == 'position':\n",
    "        hand_indices = [8, 9, 12]  # index and middle fingers\n",
    "        face_indices = [130, 152, 94]  # right eye, chin, nose\n",
    "        for hand_index in hand_indices:\n",
    "            for face_index in face_indices:\n",
    "                index_pairs.append((hand_index, face_index))\n",
    "    return index_pairs\n",
    "\n",
    "def get_angle(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    Compute the angle between three points p1, p2, and p3.\n",
    "    \"\"\"\n",
    "    v1 = p1 - p2\n",
    "    v2 = p3 - p2\n",
    "    cosine_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    return np.degrees(angle)\n",
    "\n",
    "\n",
    "def extract_features(df_coords):\n",
    "    # Create the df of relevant features\n",
    "    df_features = pd.DataFrame()\n",
    "    df_features[\"fn_video\"] = df_coords[\"fn_video\"].copy()\n",
    "    df_features[\"frame_number\"] = df_coords[\"frame_number\"]\n",
    "\n",
    "    # Face width to normalize the distance\n",
    "    face_width = get_distance(df_coords, \"face234\", \"face454\").mean()\n",
    "    norm_factor = face_width\n",
    "    print(f\"Face width computed for normalization: {face_width}\")\n",
    "\n",
    "    # HAND-FACE DISTANCES AS FEATURES FOR POSITION DECODING\n",
    "    position_index_pairs = get_index_pairs(\"position\")\n",
    "    for hand_index, face_index in position_index_pairs:\n",
    "        dx = get_delta_dim(\n",
    "            df_coords,\n",
    "            f\"face{face_index}\",\n",
    "            f\"r_hand{hand_index}\",\n",
    "            \"x\",\n",
    "            norm_factor=norm_factor,\n",
    "        )\n",
    "\n",
    "        dy = get_delta_dim(\n",
    "            df_coords,\n",
    "            f\"face{face_index}\",\n",
    "            f\"r_hand{hand_index}\",\n",
    "            \"y\",\n",
    "            norm_factor=norm_factor,\n",
    "        )\n",
    "\n",
    "        # Handle division by zero or NaN\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            tan_angle = np.divide(dx, dy)\n",
    "            tan_angle = np.nan_to_num(tan_angle, nan=0.0, posinf=0.0, neginf=0.0)  # Replace NaN and infinities with 0\n",
    "        feature_name = f\"tan_angle_face{face_index}_r_hand{hand_index}\"\n",
    "        df_features[feature_name] = tan_angle\n",
    "\n",
    "    for hand_index, face_index in position_index_pairs:\n",
    "        feature_name = f\"distance_face{face_index}_r_hand{hand_index}\"\n",
    "        df_features[feature_name] = get_distance(\n",
    "            df_coords,\n",
    "            f\"face{face_index}\",\n",
    "            f\"r_hand{hand_index}\",\n",
    "            norm_factor=norm_factor,\n",
    "        )\n",
    "\n",
    "    # HAND-HAND DISTANCES AS FEATURES FOR SHAPE DECODING\n",
    "    shape_index_pairs = get_index_pairs(\"shape\")\n",
    "    for hand_index1, hand_index2 in shape_index_pairs:\n",
    "        feature_name = f\"distance_r_hand{hand_index1}_r_hand{hand_index2}\"\n",
    "        df_features[feature_name] = get_distance(\n",
    "            df_coords,\n",
    "            f\"r_hand{hand_index1}\",\n",
    "            f\"r_hand{hand_index2}\",\n",
    "            norm_factor=norm_factor,\n",
    "        )\n",
    "\n",
    "    # HAND-HAND ANGLES AS FEATURES FOR SHAPE DECODING\n",
    "    angle_pairs = [\n",
    "        #(4, 8, 12),  # Thumb, index, middle\n",
    "        (8, 12, 16)  # Index, middle, ring\n",
    "    ]\n",
    "    for p1, p2, p3 in angle_pairs:\n",
    "        feature_name = f\"angle_r_hand{p1}_r_hand{p2}_r_hand{p3}\"\n",
    "        df_features[feature_name] = df_coords.apply(\n",
    "            lambda row: get_angle(\n",
    "                row[[f\"x_r_hand{p1}\", f\"y_r_hand{p1}\"]],\n",
    "                row[[f\"x_r_hand{p2}\", f\"y_r_hand{p2}\"]],\n",
    "                row[[f\"x_r_hand{p3}\", f\"y_r_hand{p3}\"]]\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # HAND-FACE ORIENTATION FEATURES FOR POSITION DECODING\n",
    "    for hand_index in [8, 9, 12]:  # Index and middle fingers\n",
    "        for face_index in [130, 152, 94]:  # Right eye, chin, nose\n",
    "            # Horizontal offset\n",
    "            feature_name = f\"offset_x_face{face_index}_r_hand{hand_index}\"\n",
    "            df_features[feature_name] = get_delta_dim(\n",
    "                df_coords,\n",
    "                f\"face{face_index}\",\n",
    "                f\"r_hand{hand_index}\",\n",
    "                \"x\",\n",
    "                norm_factor=norm_factor,\n",
    "            )\n",
    "\n",
    "            # Vertical offset\n",
    "            feature_name = f\"offset_y_face{face_index}_r_hand{hand_index}\"\n",
    "            df_features[feature_name] = get_delta_dim(\n",
    "                df_coords,\n",
    "                f\"face{face_index}\",\n",
    "                f\"r_hand{hand_index}\",\n",
    "                \"y\",\n",
    "                norm_factor=norm_factor,\n",
    "            )\n",
    "\n",
    "    # TEMPORAL FEATURES\n",
    "    for hand_index in [8, 9, 12]:  # Index and middle fingers\n",
    "        # Velocity (change in position)\n",
    "        feature_name = f\"velocity_x_r_hand{hand_index}\"\n",
    "        df_features[feature_name] = df_coords[f\"x_r_hand{hand_index}\"].diff()\n",
    "        feature_name = f\"velocity_y_r_hand{hand_index}\"\n",
    "        df_features[feature_name] = df_coords[f\"y_r_hand{hand_index}\"].diff()\n",
    "\n",
    "        # Acceleration (change in velocity)\n",
    "        feature_name = f\"acceleration_x_r_hand{hand_index}\"\n",
    "        df_features[feature_name] = df_features[f\"velocity_x_r_hand{hand_index}\"].diff()\n",
    "        feature_name = f\"acceleration_y_r_hand{hand_index}\"\n",
    "        df_features[feature_name] = df_features[f\"velocity_y_r_hand{hand_index}\"].diff()\n",
    "\n",
    "    ## Replace NaN and infinite values in the entire DataFrame\n",
    "    #df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    #df_features.fillna(0, inplace=True)\n",
    "#\n",
    "    ## Normalize features\n",
    "    #scaler = StandardScaler()\n",
    "    #df_features_normalized = scaler.fit_transform(df_features.drop(columns=[\"fn_video\", \"frame_number\"]))\n",
    "    #df_features_normalized = pd.DataFrame(df_features_normalized, columns=df_features.columns[2:])\n",
    "    #df_features_normalized[\"fn_video\"] = df_features[\"fn_video\"]\n",
    "    #df_features_normalized[\"frame_number\"] = df_features[\"frame_number\"]\n",
    "\n",
    "    return df_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_logging(loglevel):\n",
    "    \"\"\"Setup basic logging\n",
    "\n",
    "    Args:\n",
    "        loglevel (int): Minimum loglevel for emitting messages\n",
    "    \"\"\"\n",
    "    logformat = \"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\"\n",
    "    logging.basicConfig(\n",
    "        level=loglevel, stream=sys.stdout, format=logformat,\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_predictions(model, df_features):\n",
    "    '''\n",
    "    model - sklean model\n",
    "    df_features - dataframe with n_samples X n_features\n",
    "    '''\n",
    "    X = df_features.to_numpy()\n",
    "\n",
    "    predicted_class, predicted_probs = [], []\n",
    "    for X_i in X:\n",
    "        if (None in X_i) or (np.nan in X_i) or any([xi!=xi for xi in X_i]):\n",
    "            predicted_c = None\n",
    "            predicted_p = None\n",
    "        else:\n",
    "            predicted_c = model.predict([X_i])[0]\n",
    "            predicted_p = model.predict_proba([X_i])[0]\n",
    "        predicted_class.append(predicted_c)\n",
    "        predicted_probs.append(predicted_p)\n",
    "\n",
    "    return np.asarray(predicted_probs, dtype=object), \\\n",
    "        np.asarray(predicted_class)\n",
    "\n",
    "\n",
    "def compute_velocity(df, landmark, fn=None):\n",
    "    frame_number = df['frame_number']\n",
    "    x = df['x_' + landmark].values\n",
    "    y = df['y_' + landmark].values\n",
    "    z = df['z_' + landmark].values\n",
    "\n",
    "    dx = np.gradient(x, frame_number)\n",
    "    dy = np.gradient(y, frame_number)\n",
    "    dz = np.gradient(z, frame_number)\n",
    "\n",
    "    dx2 = np.gradient(dx, frame_number)\n",
    "    dy2 = np.gradient(dy, frame_number)\n",
    "    dz2 = np.gradient(dz, frame_number)\n",
    "\n",
    "    v = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    a = np.sqrt(dx2**2 + dy2**2 + dz2**2)\n",
    "\n",
    "    v_smoothed = savgol_filter(v, 9, 3) # window\n",
    "    a_smoothed = savgol_filter(a, 9, 3) # window\n",
    "\n",
    "    if fn is not None:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(v_smoothed, lw=3, color='k')\n",
    "        ax.plot(a_smoothed, lw=3, color='b')\n",
    "        ax.set_xlabel('Frame', fontsize=16)\n",
    "        ax.set_ylabel('Velocity', fontsize=16)\n",
    "        ax.set_ylim([-0.01, 0.01])\n",
    "        fig.savefig(fn + '.png')\n",
    "    return  v_smoothed, a_smoothed\n",
    "\n",
    "\n",
    "def get_phone_onsets(fn_textgrid):\n",
    "    times, labels = [], []\n",
    "\n",
    "    grid = textgrids.TextGrid(fn_textgrid)\n",
    "    phones = grid['phones']\n",
    "    for phone in phones:\n",
    "        if phone.text.transcode() != '':\n",
    "            times.append(phone.xmin)\n",
    "            labels.append(phone.text.transcode())\n",
    "\n",
    "    return times, labels\n",
    "\n",
    "\n",
    "def get_stimulus_string(fn_video):\n",
    "    fn_base = os.path.basename(fn_video)[:-4]\n",
    "    fn_stimulus = fn_base + '.txt'\n",
    "    fn_stimulus = os.path.join('ACSR/stimuli/words/mfa_in', fn_stimulus)\n",
    "    s = open(fn_stimulus, 'r').readlines()\n",
    "    return s[0].strip('\\n')\n",
    "\n",
    "\n",
    "def dict_phone_transcription():\n",
    "    # Megalex (key) to MFA (value) phone labels\n",
    "    d = {}\n",
    "    d['R'] = 'ʁ'\n",
    "    d['N'] = 'ɲ'\n",
    "    d['§'] = 'ɔ̃'\n",
    "    d['Z'] = 'ʒ'\n",
    "    d['5'] = 'ɛ̃'\n",
    "    d['E'] = 'ɛ'\n",
    "    d['9'] = 'œ'\n",
    "    d['8'] = 'ɥ'\n",
    "    d['S'] = 'ʃ'\n",
    "    d['O'] = 'ɔ'\n",
    "    d['2'] = 'ø'\n",
    "    d['g'] = 'ɟ'\n",
    "    d['g'] = 'ɡ'\n",
    "    d['@'] = 'ɑ̃'\n",
    "    d['8'] = 'ɥ'\n",
    "    return d\n",
    "\n",
    "def find_syllable_onsets(lpc_syllables, times_phones, labels_phones):\n",
    "    phones = labels_phones.copy()\n",
    "    d_phone_transcription = dict_phone_transcription()\n",
    "    #print(lpc_syllables)\n",
    "    #[print(p, t) for p, t in zip(phones, times_phones)]\n",
    "    #print('-'*100)\n",
    "    times = []\n",
    "    for syllable in lpc_syllables:\n",
    "        first_phone = syllable[0]\n",
    "        if first_phone in d_phone_transcription.keys():\n",
    "            first_phone = d_phone_transcription[first_phone]\n",
    "        for i, phone in enumerate(phones):\n",
    "            if first_phone == phone:\n",
    "                times.append(times_phones[i])\n",
    "                del phones[i]\n",
    "                del times_phones[i]\n",
    "                break\n",
    "    return times\n",
    "\n",
    "\n",
    "def get_syllable_onset_frames_from_lpc_file(fn_video):\n",
    "    fn_base = os.path.basename(fn_video)[:-4]\n",
    "\n",
    "    # Get LPC parsing of stimulus, into separate SYLLABLES\n",
    "    # (MFA is for ALL phones and we need to know which phones are at the beginning of each syllable)\n",
    "    fn_lpc_parsing = fn_base + '.lpc'\n",
    "    fn_lpc_parsing = os.path.join('ACSR/stimuli/words/txt', fn_lpc_parsing)\n",
    "    lpc_syllables = open(fn_lpc_parsing, 'r').readlines()[0].strip('\\n').split()\n",
    "\n",
    "    return lpc_syllables\n",
    "\n",
    "    return \n",
    "def get_syllable_onset_frames_from_mfa(fn_video, lpc_syllables):\n",
    "\n",
    "    # Load video and get number of frames per second (fps)\n",
    "    cap = load_video(fn_video)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) # frames per second\n",
    "    assert fps > 0; 'Frames per seconds is not a positive number'\n",
    "\n",
    "    # Load corresponing TextGrid file\n",
    "    fn_base = os.path.basename(fn_video)[:-4]\n",
    "    fn_textgrid = fn_base + '.TextGrid'\n",
    "    fn_textgrid = os.path.join('../stimuli/words/mfa_out', fn_textgrid)\n",
    "\n",
    "    # Get LPC parsing of stimulus, into separate SYLLABLES\n",
    "    # (MFA is for ALL phones and we need to know which phones are at the beginning of each syllable)\n",
    "    #fn_lpc_parsing = fn_base + '.lpc'\n",
    "    #fn_lpc_parsing = os.path.join('../stimuli/words/txt', fn_lpc_parsing)\n",
    "    #lpc_syllables = open(fn_lpc_parsing, 'r').readlines()[0].strip('\\n').split()\n",
    "\n",
    "    # PHONE onests in seconds from MFA\n",
    "    onset_secs_phones_mfa, labels_phones_textgrid = get_phone_onsets(fn_textgrid)\n",
    "    print(onset_secs_phones_mfa, labels_phones_textgrid)\n",
    "    # SYLLABLE ONSET from MFA based on the onset of their FIRST PHONE\n",
    "    onset_secs_syllables_mfa = find_syllable_onsets(lpc_syllables, # in seconds\n",
    "                                                    onset_secs_phones_mfa,\n",
    "                                                    labels_phones_textgrid)\n",
    "    onset_frames_syllables_mfa = [int(t*fps) for t in onset_secs_syllables_mfa] # in frames\n",
    "\n",
    "    return onset_frames_syllables_mfa\n",
    "\n",
    "\n",
    "\n",
    "def find_onsets_based_on_extrema(time_series,\n",
    "                                 n_syllables=None,\n",
    "                                 onset_frames_syllables_mfa=None,\n",
    "                                 thresh=None): # condition: time_series > thresh\n",
    "\n",
    "    if onset_frames_syllables_mfa is not None: \n",
    "        onset_frames_syllables_mfa = np.asarray(onset_frames_syllables_mfa)\n",
    "\n",
    "    # find extrema\n",
    "    onset_frames_extrema = argrelextrema(time_series, np.greater)[0]\n",
    "    # Threshold\n",
    "    if thresh is not None:\n",
    "        onset_frames_extrema = np.asarray([onset_frame for onset_frame in onset_frames_extrema if time_series[onset_frame]>thresh])\n",
    "\n",
    "    onset_frames_extrema_temp = onset_frames_extrema.copy()\n",
    "    onset_frames_picked = []\n",
    "    if onset_frames_syllables_mfa is not None: # use MFA onsets to constrain the solution\n",
    "        if len(onset_frames_syllables_mfa) == len(onset_frames_extrema_temp):\n",
    "            onset_frames_picked = onset_frames_extrema_temp\n",
    "        else:\n",
    "            for i_frame, onset_frame_syl_mfa in enumerate(onset_frames_syllables_mfa):\n",
    "                # Find extremum that is nearest to current MFA onset\n",
    "                delta = np.abs(onset_frames_extrema_temp - onset_frame_syl_mfa)\n",
    "                IX_onset_frame_extremum_nearest_mfa = np.argmin(delta)\n",
    "                onset_frame_extremum_nearest_mfa = onset_frames_extrema_temp[IX_onset_frame_extremum_nearest_mfa]\n",
    "                onset_frames_picked.append(onset_frame_extremum_nearest_mfa)\n",
    "                # Remove past indexes, in order to make sure the next onset frame is in the future\n",
    "                onset_frames_extrema_temp = onset_frames_extrema_temp[onset_frames_extrema_temp > onset_frame_extremum_nearest_mfa]\n",
    "                if len(onset_frames_extrema_temp)==0:\n",
    "                    while len(onset_frames_picked) < len(onset_frames_syllables_mfa): # Fill None values if not enough identified extrema\n",
    "                        onset_frames_picked.append(None)\n",
    "                    break\n",
    "    else:\n",
    "        IXs = np.argpartition(onset_frames_extrema, -n_syllables)[-n_syllables:]\n",
    "        onset_frames_picked = list(onset_frames_extrema[IXs])\n",
    "\n",
    "    return onset_frames_picked, onset_frames_extrema\n",
    "\n",
    "def scale_velocity(velocity):\n",
    "    q25, q75 = np.percentile(velocity, 25), np.percentile(velocity, 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    velocity = np.clip(velocity, lower, upper)\n",
    "    velocity_scaled = minmax_scale(velocity)\n",
    "    return velocity_scaled\n",
    "\n",
    "\n",
    "def get_joint_measure(df_predictions_pos,\n",
    "                      df_predictions_shape,\n",
    "                      velocity_scaled,\n",
    "                      weight_velocity=1):\n",
    "\n",
    "    # MAX PROBABILITIES (POSITION AND SHAPE)\n",
    "    max_probs_pos = df_predictions_pos.copy().filter(regex=(\"p_class*\")).to_numpy().max(axis=1)\n",
    "    max_probs_shape = df_predictions_shape.copy().filter(regex=(\"p_class*\")).to_numpy().max(axis=1)\n",
    "    probs_product = max_probs_pos * max_probs_shape\n",
    "    # JOINT\n",
    "    joint_measure = (weight_velocity * (1-velocity_scaled) + probs_product)/(1+weight_velocity)\n",
    "    joint_measure_smoothed = savgol_filter(joint_measure, 15, 3) # window, smooth\n",
    "    # replace nans caused by smoothing with original values\n",
    "    is_nan_smoothed = np.isnan(joint_measure_smoothed)\n",
    "    joint_measure_smoothed[is_nan_smoothed] = joint_measure[is_nan_smoothed]\n",
    "\n",
    "    return joint_measure_smoothed\n",
    "\n",
    "\n",
    "def write_onsets_to_file(str_stimulus, lpc_syllables, onset_frames_picked, fn_txt):\n",
    "    \n",
    "    # HACK TO EQUALIZE THE NUMBER OF EXPECTED ONSETS (NUM SYLLABLES) AND THE ONE FOUND\n",
    "    if len(lpc_syllables) < len(onset_frames_picked): # REMOVE EXTRA ONSETS\n",
    "        onset_frames_picked = onset_frames_picked[:3]\n",
    "    for i_sy in range(len(lpc_syllables)-len(onset_frames_picked)): # ADD DUMMY ONSETS\n",
    "        onset_frames_picked = list(onset_frames_picked)\n",
    "        last_onset = onset_frames_picked[-1]\n",
    "        onset_frames_picked.append(last_onset + i_sy + 1)\n",
    "\n",
    "    assert len(lpc_syllables) == len(onset_frames_picked)\n",
    "\n",
    "    with open(fn_txt, 'w') as f:\n",
    "        f.write(f'{str_stimulus}\\n')\n",
    "        f.write('event,stimulus,frame_number\\n')\n",
    "        for (syllable, onset) in zip(lpc_syllables, onset_frames_picked):\n",
    "            f.write(f'SYLLABLE ONSET, {syllable}, {onset}\\n')\n",
    "    return None\n",
    "\n",
    "# FROM HAGAR\n",
    "\n",
    "def get_LPC_p(word):\n",
    "    lex = pd.read_csv(\"/home/yair/projects/ACSR/data/hagar/Lexique380.utf8.csv\")\n",
    "    lex = lex[(lex.ortho.str.contains('-| ') == False) & (lex.phon.str.contains('°') == False)]  # suppress schwa\n",
    "    lex = lex.drop_duplicates(subset='ortho', keep=\"first\")\n",
    "    lex = lex[['ortho','phon', 'p_cvcv','nbhomogr','cv-cv','syll']]\n",
    "    dic = lex.set_index('ortho').to_dict()\n",
    "\n",
    "    cv_dic = dic['cv-cv']\n",
    "    p_cv_dic = dic['syll']\n",
    "    phon_dic = dic['phon']    \n",
    "\n",
    "    dev_syl = pd.read_csv(\"/home/yair/projects/ACSR/data/hagar/lpc_syl_configurations.csv\")\n",
    "    dev_syl['lpc_n'] = dev_syl['LPC_config'].apply(lambda x: x.split('-'))\n",
    "    dev_syl['lpc_n'] = dev_syl['lpc_n'].apply(lambda x: len(x))\n",
    "    dic2 = dev_syl.set_index('spoken_config').to_dict()\n",
    "    \n",
    "    g_cv_dic = dic2['LPC_config']\n",
    "    \n",
    "    lpc_cv = get_LPC_cv(word, cv_dic, g_cv_dic)\n",
    "    \n",
    "    new_word = ''\n",
    "    phon = phon_dic[word]\n",
    "    if lpc_cv == cv_dic[word]:\n",
    "        return p_cv_dic[word]\n",
    "    else:\n",
    "        l_lpc = lpc_cv.split('-')\n",
    "        for syl in l_lpc:\n",
    "            new_word += phon[:len(syl)]+'-'\n",
    "            phon = phon[len(syl):]\n",
    "        return new_word[:-1]\n",
    "\n",
    "\n",
    "def get_LPC_cv(word, cv_dic, g_cv_dic):\n",
    "    \n",
    "\n",
    "    LPC_cv = ''\n",
    "    if word in cv_dic:\n",
    "        cv_lst = cv_dic[word].split('-')\n",
    "        for syl in cv_lst:\n",
    "            LPC_cv = LPC_cv + g_cv_dic[syl] + '-'\n",
    "        return LPC_cv[:-1]\n",
    "\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def get_word_code(syll):\n",
    "    position = {'a': '0', 'o': '0', '9': '0', '5': '1', '2': '1', 'i': '2', '§': '2', '@': '2', 'E': '3', 'u': '3', 'O': '3', '1': '4', 'y': '4', 'e': '4'}\n",
    "    configuration = {'p': '0', 'd': '0', 'Z': '0', 'k': '1', 'v': '1', 'z': '1', 's': '2', 'R': '2', 'b': '3', 'n': '3', '8': '3', 't': '4', 'm': '4', 'f': '4', 'l': '5', 'S': '5', 'N': '5', 'w': '5', 'g': '6', 'j': '7', 'G': '7'}\n",
    "    try:\n",
    "        code_word = ''\n",
    "        if len(syll) == 1:\n",
    "            if syll in configuration:\n",
    "                code_word += configuration[syll]\n",
    "                code_word += '0'\n",
    "            else:\n",
    "                code_word += '4'\n",
    "                code_word += position[syll]\n",
    "        else:\n",
    "            for i in range (0,len(syll)):\n",
    "                if syll[i] in configuration:\n",
    "                    code_word += configuration[syll[i]]\n",
    "                else:\n",
    "                    code_word += position[syll[i]]\n",
    "        return code_word\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def shape_position_code(word):\n",
    "    code_word = \"\"\n",
    "    syll_lst = get_LPC_p(word).split(\"-\")\n",
    "    for syll in syll_lst:\n",
    "        code_word += get_word_code(syll) + '-'  \n",
    "    return code_word[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-jnt0UJEK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
