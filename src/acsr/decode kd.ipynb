{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 36 unique phonemes to /scratch2/bsow/Documents/ACSR/data/training_videos/phoneme_dictionary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_phonemes_from_lpc(directory, output_file):\n",
    "    \"\"\"\n",
    "    Extract all unique phonemes from .lpc files and save them to a file.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .lpc files.\n",
    "        output_file (str): Path to the output file (CSV or text) where phonemes will be saved.\n",
    "    \"\"\"\n",
    "    # Initialize a set to store unique phonemes\n",
    "    unique_phonemes = set()\n",
    "\n",
    "    # Iterate through all .lpc files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".lpc\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    # Extract the phoneme-syllable pair (first column)\n",
    "                    phoneme_syllable = line.strip().split()[0]\n",
    "                    # Split by underscore and extract the phoneme\n",
    "                    syllabe = list(phoneme_syllable.split('_')[0])\n",
    "                    if len(syllabe) == 3:\n",
    "                        unique_phonemes.add(syllabe[0])\n",
    "                        unique_phonemes.add(\"\".join(syllabe[1:]))\n",
    "                    else:\n",
    "                        for phoneme in syllabe:\n",
    "                            unique_phonemes.add(phoneme)\n",
    "\n",
    "    # Save the unique phonemes to the output file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for phoneme in sorted(unique_phonemes):\n",
    "            f.write(f\"{phoneme}\\n\")\n",
    "\n",
    "    print(f\"Saved {len(unique_phonemes)} unique phonemes to {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "lpc_directory = \"/scratch2/bsow/Documents/ACSR/data/training_videos/lpc\"\n",
    "output_file = \"/scratch2/bsow/Documents/ACSR/data/training_videos/phoneme_dictionary.txt\"\n",
    "extract_phonemes_from_lpc(lpc_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load CSV files from a directory based on a filename pattern\n",
    "def load_csv_files(directory, filename_pattern, type=\"position\"):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            df.dropna(inplace=True)\n",
    "            base_name = filename.split(f'_{type}_')[1].split('.csv')[0]\n",
    "            files_data[base_name] = df\n",
    "    return files_data\n",
    "\n",
    "# Load features from .npy files based on a filename pattern\n",
    "def load_features(directory, filename_pattern):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            features = pd.read_csv(os.path.join(directory, filename))\n",
    "            features.dropna(inplace=True)\n",
    "            base_name = filename.split('_features')[0]\n",
    "            files_data[base_name] = features\n",
    "    return files_data\n",
    "\n",
    "# Find corresponding phoneme files based on the base names of position filenames\n",
    "def find_phoneme_files(directory, base_names):\n",
    "    phoneme_files = {}\n",
    "    for base_name in base_names:\n",
    "        phoneme_file = os.path.join(directory, f'{base_name}.lpc')\n",
    "        if os.path.exists(phoneme_file):\n",
    "            phoneme_files[base_name] = phoneme_file\n",
    "    return phoneme_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from praatio import textgrid as tgio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================================\n",
    "# Helper Functions\n",
    "# ==========================================================\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of sequences to pad.\n",
    "        max_length (int): Maximum length to pad to.\n",
    "        pad_value (int): Value to use for padding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded sequences.\n",
    "    \"\"\"\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.full((max_length - len(seq), seq.shape[1]), pad_value)\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "def combine_sequences_with_padding(video_data):\n",
    "    \"\"\"\n",
    "    Combine sequences with padding to ensure uniform length.\n",
    "\n",
    "    Args:\n",
    "        video_data (dict): Dictionary containing video data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded input sequences (X_student_hand_shape, X_student_hand_pos, X_student_lips, X_teacher) and padded labels (y).\n",
    "    \"\"\"\n",
    "    max_length = max(len(video_data[video][\"X_student_hand_shape\"]) for video in video_data)\n",
    "    \n",
    "    # Pad hand shape features\n",
    "    X_student_hand_shape_padded = [\n",
    "        pad_sequences([video_data[video][\"X_student_hand_shape\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad hand position features\n",
    "    X_student_hand_pos_padded = [\n",
    "        pad_sequences([video_data[video][\"X_student_hand_pos\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad lip features\n",
    "    X_student_lips_padded = [\n",
    "        pad_sequences([video_data[video][\"X_student_lips\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad teacher features\n",
    "    X_teacher_padded = [\n",
    "        pad_sequences([video_data[video][\"X_teacher\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad labels\n",
    "    y_padded = [\n",
    "        video_data[video][\"y\"]\n",
    "        + [phoneme_to_index[\" \"]] * (max_length - len(video_data[video][\"y\"]))\n",
    "        for video in video_data\n",
    "    ]\n",
    "    \n",
    "    return X_student_hand_shape_padded, X_student_hand_pos_padded, X_student_lips_padded, X_teacher_padded, y_padded\n",
    "\n",
    "def compute_log_mel_spectrogram(audio_path, sr=16000, n_fft=400, hop_length=160, n_mels=161):\n",
    "    \"\"\"\n",
    "    Compute the log-mel spectrogram for an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        sr (int): Sample rate.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.\n",
    "        n_mels (int): Number of mel bands.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Log-mel spectrogram of shape (num_frames, n_mels).\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "    # Compute mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
    "    )\n",
    "\n",
    "    # Convert to log scale\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Transpose to (num_frames, n_mels)\n",
    "    log_mel_spectrogram = log_mel_spectrogram.T\n",
    "\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "def parse_textgrid(textgrid_path):\n",
    "    \"\"\"\n",
    "    Parse a TextGrid file to extract phoneme-level intervals.\n",
    "\n",
    "    Args:\n",
    "        textgrid_path (str): Path to the TextGrid file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (start_time, end_time, phoneme) tuples.\n",
    "    \"\"\"\n",
    "    tg = tgio.openTextgrid(textgrid_path, includeEmptyIntervals=False)\n",
    "    phone_tier = tg.getTier(\"phones\")\n",
    "    return [(start, end, label) for start, end, label in phone_tier.entries]\n",
    "\n",
    "def get_phoneme_labels_for_frames(phoneme_intervals, num_frames, fps):\n",
    "    \"\"\"\n",
    "    Map phoneme intervals to video frames.\n",
    "\n",
    "    Args:\n",
    "        phoneme_intervals (list): List of (start_time, end_time, phoneme) tuples.\n",
    "        num_frames (int): Total number of video frames.\n",
    "        fps (int): Frame rate of the video.\n",
    "\n",
    "    Returns:\n",
    "        list: Phoneme labels for each frame.\n",
    "    \"\"\"\n",
    "    phoneme_labels = []\n",
    "    for frame_idx in range(num_frames):\n",
    "        frame_time = frame_idx / fps\n",
    "        phoneme = \" \"  # Default to silence/space\n",
    "        for start, end, label in phoneme_intervals:\n",
    "            if start <= frame_time < end:\n",
    "                phoneme = label\n",
    "                break\n",
    "        phoneme_labels.append(phoneme)\n",
    "    return phoneme_labels\n",
    "\n",
    "# Load phoneme-to-index mapping\n",
    "with open(\n",
    "    r\"/scratch2/bsow/Documents/ACSR/data/training_videos/phoneme_dictionary.txt\", \"r\"\n",
    ") as file:\n",
    "    reader = csv.reader(file)\n",
    "    vocabulary_list = [row[0] for row in reader]\n",
    "\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(vocabulary_list)}\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "phoneme_to_index[\" \"] = len(phoneme_to_index)\n",
    "index_to_phoneme[len(index_to_phoneme)] = \" \"\n",
    "\n",
    "def load_coordinates(directory, base_name):\n",
    "    \"\"\"\n",
    "    Load pre-extracted coordinates from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory containing the coordinate files.\n",
    "        base_name (str): Base name of the video (e.g., 'sent_01').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the coordinates.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(directory, f\"{base_name}_coordinates.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)  # Drop rows with NaN values\n",
    "    return df\n",
    "\n",
    "def prepare_data_for_videos_no_sliding_windows(\n",
    "    hand_position_data, phoneme_files, audio_dir, textgrid_dir, video_dir, coordinates_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data for all videos without sliding windows.\n",
    "\n",
    "    Args:\n",
    "        hand_position_data (dict): Dictionary of hand position data.\n",
    "        phoneme_files (dict): Dictionary of phoneme file paths.\n",
    "        audio_dir (str): Directory containing audio files.\n",
    "        textgrid_dir (str): Directory containing TextGrid files.\n",
    "        video_dir (str): Directory containing video files.\n",
    "        coordinates_dir (str): Directory containing pre-extracted coordinate files.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing combined features, spectrograms, and phoneme indices.\n",
    "    \"\"\"\n",
    "    all_videos_data = {}\n",
    "    for base_name in hand_position_data:\n",
    "        if base_name in phoneme_files:\n",
    "            # Load pre-extracted coordinates\n",
    "            coordinates_df = load_coordinates(coordinates_dir, base_name)\n",
    "            if 'frame_number' not in coordinates_df.columns:\n",
    "                raise ValueError(f\"Coordinate file for {base_name} does not contain 'frame_number' column.\")\n",
    "            frame_numbers = coordinates_df['frame_number'].values\n",
    "\n",
    "            # Separate coordinates into hand shape, hand position, and lip landmarks\n",
    "            hand_shape_columns = [f\"hand_x{i}\" for i in range(21)] + [f\"hand_y{i}\" for i in range(21)] + [f\"hand_z{i}\" for i in range(21)]\n",
    "            hand_pos_columns = [\"hand_pos_x\", \"hand_pos_y\", \"hand_pos_z\"]\n",
    "            lip_columns = [f\"lip_x{i}\" for i in range(40)] + [f\"lip_y{i}\" for i in range(40)] + [f\"lip_z{i}\" for i in range(40)]\n",
    "\n",
    "            X_student_hand_shape = coordinates_df[hand_shape_columns].to_numpy()\n",
    "            X_student_hand_pos = coordinates_df[hand_pos_columns].to_numpy()\n",
    "            X_student_lips = coordinates_df[lip_columns].to_numpy()\n",
    "\n",
    "            # Load audio and compute spectrogram\n",
    "            audio_path = os.path.join(audio_dir, f\"{base_name}.wav\")\n",
    "            log_mel_spectrogram = compute_log_mel_spectrogram(audio_path)\n",
    "\n",
    "            # Load TextGrid and get phoneme labels for each frame\n",
    "            textgrid_path = os.path.join(textgrid_dir, f\"{base_name}.TextGrid\")\n",
    "            phoneme_intervals = parse_textgrid(textgrid_path)\n",
    "\n",
    "            # Get video FPS\n",
    "            video_path = os.path.join(video_dir, f\"{base_name}.mp4\")\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            cap.release()\n",
    "\n",
    "            # Map phoneme labels to frames\n",
    "            phoneme_labels = get_phoneme_labels_for_frames(phoneme_intervals, len(frame_numbers), fps)\n",
    "\n",
    "            # Convert phoneme labels to indices\n",
    "            phoneme_indices = [phoneme_to_index.get(phoneme, -1) for phoneme in phoneme_labels]\n",
    "\n",
    "            # Combine features, spectrogram, and phoneme indices\n",
    "            all_videos_data[base_name] = {\n",
    "                \"X_student_hand_shape\": X_student_hand_shape,  # Hand shape coordinates\n",
    "                \"X_student_hand_pos\": X_student_hand_pos,      # Hand position coordinates\n",
    "                \"X_student_lips\": X_student_lips,              # Lip landmarks\n",
    "                \"X_teacher\": log_mel_spectrogram,              # Audio features (log-mel spectrogram)\n",
    "                \"y\": phoneme_indices,                          # Phoneme labels (shared)\n",
    "            }\n",
    "    return all_videos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of position files: 95\n",
      "Number of shape files: 95\n",
      "Number of phoneme files: 95\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "data_dir = r'/scratch2/bsow/Documents/ACSR/output/predictions'\n",
    "phoneme_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/lpc'\n",
    "audio_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/audio'\n",
    "textgrid_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/textgrids'\n",
    "video_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/videos'\n",
    "coordinates_dir = r'/scratch2/bsow/Documents/ACSR/output/extracted_coordinates_old'\n",
    "\n",
    "# Load position and shape data\n",
    "hand_position_data = load_csv_files(data_dir, 'predictions_rf_position', type='position')\n",
    "\n",
    "# Find phoneme files\n",
    "base_names = hand_position_data.keys()\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n",
    "\n",
    "# Prepare data\n",
    "all_videos_data = prepare_data_for_videos_no_sliding_windows(\n",
    "    hand_position_data, phoneme_files, audio_dir, textgrid_dir, video_dir, coordinates_dir\n",
    ")\n",
    "\n",
    "# Combine sequences with padding\n",
    "X_student_hand_shape_padded, X_student_hand_pos_padded, X_student_lips_padded, X_teacher_padded, y_padded = combine_sequences_with_padding(all_videos_data)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_student_hand_shape_tensor = torch.tensor(X_student_hand_shape_padded, dtype=torch.float32)\n",
    "X_student_hand_pos_tensor = torch.tensor(X_student_hand_pos_padded, dtype=torch.float32)\n",
    "X_student_lips_tensor = torch.tensor(X_student_lips_padded, dtype=torch.float32)\n",
    "X_teacher_tensor = torch.tensor(X_teacher_padded, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_padded, dtype=torch.long)\n",
    "\n",
    "# Final organized data\n",
    "all_videos_data = {\n",
    "    \"X_student_hand_shape\": X_student_hand_shape_tensor,  # Hand shape coordinates\n",
    "    \"X_student_hand_pos\": X_student_hand_pos_tensor,      # Hand position coordinates\n",
    "    \"X_student_lips\": X_student_lips_tensor,              # Lip landmarks\n",
    "    \"X_teacher\": X_teacher_tensor,                        # Audio features (log-mel spectrogram)\n",
    "    \"y\": y_tensor,                                        # Phoneme labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to split data into training and validation sets\n",
    "def train_val_split(data, train_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing the dataset.\n",
    "        train_ratio (float): Proportion of data to use for training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two dictionaries for training and validation data.\n",
    "    \"\"\"\n",
    "    num_samples = len(data['X_student_hand_shape'])\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    \n",
    "    # Randomize the data\n",
    "    indices = torch.randperm(num_samples)\n",
    "    \n",
    "    # Split hand shape features\n",
    "    X_student_hand_shape = data['X_student_hand_shape'][indices]\n",
    "    X_student_hand_shape_train = X_student_hand_shape[:split_idx]\n",
    "    X_student_hand_shape_val = X_student_hand_shape[split_idx:]\n",
    "    \n",
    "    # Split hand position features\n",
    "    X_student_hand_pos = data['X_student_hand_pos'][indices]\n",
    "    X_student_hand_pos_train = X_student_hand_pos[:split_idx]\n",
    "    X_student_hand_pos_val = X_student_hand_pos[split_idx:]\n",
    "    \n",
    "    # Split lip features\n",
    "    X_student_lips = data['X_student_lips'][indices]\n",
    "    X_student_lips_train = X_student_lips[:split_idx]\n",
    "    X_student_lips_val = X_student_lips[split_idx:]\n",
    "    \n",
    "    # Split teacher features\n",
    "    X_teacher = data['X_teacher'][indices]\n",
    "    X_teacher_train = X_teacher[:split_idx]\n",
    "    X_teacher_val = X_teacher[split_idx:]\n",
    "    \n",
    "    # Split labels\n",
    "    y = data['y'][indices]\n",
    "    y_train = y[:split_idx]\n",
    "    y_val = y[split_idx:]\n",
    "    \n",
    "    # Create train and validation data dictionaries\n",
    "    train_data = {\n",
    "        'X_student_hand_shape': X_student_hand_shape_train,\n",
    "        'X_student_hand_pos': X_student_hand_pos_train,\n",
    "        'X_student_lips': X_student_lips_train,\n",
    "        'X_teacher': X_teacher_train,\n",
    "        'y': y_train\n",
    "    }\n",
    "    val_data = {\n",
    "        'X_student_hand_shape': X_student_hand_shape_val,\n",
    "        'X_student_hand_pos': X_student_hand_pos_val,\n",
    "        'X_student_lips': X_student_lips_val,\n",
    "        'X_teacher': X_teacher_val,\n",
    "        'y': y_val\n",
    "    }\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "# Convert data to DataLoader format\n",
    "def data_to_dataloader(data, batch_size=4, shuffle=True):\n",
    "    \"\"\"\n",
    "    Convert data into PyTorch DataLoader format.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing the dataset.\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader object.\n",
    "    \"\"\"\n",
    "    X_student_hand_shape_tensors = data['X_student_hand_shape']\n",
    "    X_student_hand_pos_tensors = data['X_student_hand_pos']\n",
    "    X_student_lips_tensors = data['X_student_lips']\n",
    "    X_teacher_tensors = data['X_teacher']\n",
    "    y_tensors = data['y']\n",
    "    \n",
    "    # Create a TensorDataset with inputs and labels\n",
    "    dataset = TensorDataset(\n",
    "        X_student_hand_shape_tensors,\n",
    "        X_student_hand_pos_tensors,\n",
    "        X_student_lips_tensors,\n",
    "        X_teacher_tensors,\n",
    "        y_tensors\n",
    "    )\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_val_split(all_videos_data)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = data_to_dataloader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = data_to_dataloader(val_data, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Len of train dataset\", len(train_data['X_student_hand_shape']))\n",
    "print(\"Len of val dataset\", len(val_data['X_student_hand_shape']))\n",
    "\n",
    "# Check the DataLoader output\n",
    "for batch_X_student_hand_shape, batch_X_student_hand_pos, batch_X_student_lips, batch_X_teacher, batch_y in train_loader:\n",
    "    print(\"Batch X_student_hand_shape shape:\", batch_X_student_hand_shape.shape)\n",
    "    print(\"Batch X_student_hand_pos shape:\", batch_X_student_hand_pos.shape)\n",
    "    print(\"Batch X_student_lips shape:\", batch_X_student_lips.shape)\n",
    "    print(\"Batch X_teacher shape:\", batch_X_teacher.shape)\n",
    "    print(\"Batch y shape:\", batch_y.shape)\n",
    "    print(batch_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining of a deep speech model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "class IPADataset(Dataset):\n",
    "    def __init__(self, manifest_file, alphabet_file, sample_rate=16000, n_mels=80):\n",
    "        self.manifest = pd.read_csv(manifest_file, header=None)\n",
    "        with open(alphabet_file, \"r\") as f:\n",
    "            self.alphabet = f.read().splitlines()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, ipa_path = self.manifest.iloc[idx]\n",
    "        \n",
    "        # Load audio and compute mel spectrogram\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
    "        )\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Load IPA transcription\n",
    "        with open(ipa_path, \"r\") as f:\n",
    "            ipa = f.read().strip()\n",
    "        \n",
    "        # Convert IPA to indices\n",
    "        ipa_indices = [self.alphabet.index(c) for c in ipa.split()]\n",
    "        \n",
    "        return torch.tensor(mel_spec, dtype=torch.float32), torch.tensor(ipa_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepSpeech2(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=1024, num_layers=5):\n",
    "        super(DeepSpeech2, self).__init__()\n",
    "        \n",
    "        # 2D Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        # Bidirectional LSTM Layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32,  # Output size of the last convolutional layer\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional LSTM\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, num_mel_bins, time_steps)\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Reshape for LSTM: (batch_size, time_steps, num_features)\n",
    "        x = x.permute(0, 3, 1, 2)  # Move time_steps to the second dimension\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten the last two dimensions\n",
    "        \n",
    "        # LSTM Layers\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Output shape: (batch_size, time_steps, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to pad mel spectrograms and IPA indices to the same length.\n",
    "    Args:\n",
    "        batch: List of tuples (mel_spec, ipa_indices).\n",
    "    Returns:\n",
    "        Padded mel spectrograms and IPA indices.\n",
    "    \"\"\"\n",
    "    # Separate mel spectrograms and IPA indices\n",
    "    mel_specs, ipa_indices = zip(*batch)\n",
    "    \n",
    "    # Pad mel spectrograms to the same length\n",
    "    mel_specs_padded = pad_sequence(mel_specs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad IPA indices to the same length\n",
    "    ipa_indices_padded = pad_sequence(ipa_indices, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return mel_specs_padded, ipa_indices_padded\n",
    "\n",
    "dataset = IPADataset(manifest_file=\"/scratch2/bsow/Documents/ACSR/data/train.csv\", alphabet_file=\"/scratch2/bsow/Documents/ACSR/data/training_videos/phoneme_dictionary.txt\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [80, 124] at entry 0 and [80, 210] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipa_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add channel dimension\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# CTC expects (time, batch, num_classes)\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [80, 124] at entry 0 and [80, 210] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = DeepSpeech2(num_classes=len(dataset.alphabet))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CTCLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for batch_idx, (mel_spec, ipa_indices) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(mel_spec.unsqueeze(1))  # Add channel dimension\n",
    "        outputs = outputs.permute(1, 0, 2)  # CTC expects (time, batch, num_classes)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        input_lengths = torch.full((mel_spec.size(0),), outputs.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.tensor([len(ipa) for ipa in ipa_indices], dtype=torch.long)\n",
    "        loss = criterion(outputs, ipa_indices, input_lengths, target_lengths)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/10], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch 1/200, Loss: 67.96739664944735\n",
      "Epoch 2/200, Loss: 8.29189978946339\n",
      "Epoch 3/200, Loss: 5.486008535731923\n",
      "Epoch 4/200, Loss: 4.9963651136918505\n",
      "Epoch 5/200, Loss: 4.879518053748391\n",
      "Epoch 6/200, Loss: 4.8767499273473565\n",
      "Epoch 7/200, Loss: 4.869186011227694\n",
      "Epoch 8/200, Loss: 4.850904573093761\n",
      "Epoch 9/200, Loss: 4.860606930472634\n",
      "Epoch 10/200, Loss: 4.8490895357998935\n",
      "Epoch 11/200, Loss: 4.858245957981456\n",
      "Epoch 12/200, Loss: 4.851324341513894\n",
      "Epoch 13/200, Loss: 4.8643506223505195\n",
      "Epoch 14/200, Loss: 4.844618840651079\n",
      "Epoch 15/200, Loss: 4.849085742777044\n",
      "Epoch 16/200, Loss: 4.834958033128218\n",
      "Epoch 17/200, Loss: 4.854825366627086\n",
      "Epoch 18/200, Loss: 4.831333550539884\n",
      "Epoch 19/200, Loss: 4.827303019436923\n",
      "Epoch 20/200, Loss: 4.841424270109697\n",
      "Epoch 21/200, Loss: 4.84000284021551\n",
      "Epoch 22/200, Loss: 4.82599156553095\n",
      "Epoch 23/200, Loss: 4.825075734745372\n",
      "Epoch 24/200, Loss: 4.829496361992576\n",
      "Epoch 25/200, Loss: 4.845784143968062\n",
      "Epoch 26/200, Loss: 4.8297295570373535\n",
      "Epoch 27/200, Loss: 4.820190971547907\n",
      "Epoch 28/200, Loss: 4.814160412008112\n",
      "Epoch 29/200, Loss: 4.806631153280085\n",
      "Epoch 30/200, Loss: 4.798413926904852\n",
      "Epoch 31/200, Loss: 4.795331066304987\n",
      "Epoch 32/200, Loss: 4.781768321990967\n",
      "Epoch 33/200, Loss: 4.768434091047808\n",
      "Epoch 34/200, Loss: 4.755092230710116\n",
      "Epoch 35/200, Loss: 4.755203702233055\n",
      "Epoch 36/200, Loss: 4.742233471436934\n",
      "Epoch 37/200, Loss: 4.724861816926436\n",
      "Epoch 38/200, Loss: 4.713771213184703\n",
      "Epoch 39/200, Loss: 4.7152886824174365\n",
      "Epoch 40/200, Loss: 4.7150374109094795\n",
      "Epoch 41/200, Loss: 4.680255044590343\n",
      "Epoch 42/200, Loss: 4.6836818781766025\n",
      "Epoch 43/200, Loss: 4.704619559374723\n",
      "Epoch 44/200, Loss: 4.679389628497037\n",
      "Epoch 45/200, Loss: 4.66664643721147\n",
      "Epoch 46/200, Loss: 4.664801966060292\n",
      "Epoch 47/200, Loss: 4.67019154808738\n",
      "Epoch 48/200, Loss: 4.663147406144575\n",
      "Epoch 49/200, Loss: 4.655133334073153\n",
      "Epoch 50/200, Loss: 4.660174868323586\n",
      "Epoch 51/200, Loss: 4.641472708095204\n",
      "Epoch 52/200, Loss: 4.6172003962776875\n",
      "Epoch 53/200, Loss: 4.619521552866155\n",
      "Epoch 54/200, Loss: 4.62633204460144\n",
      "Epoch 55/200, Loss: 4.612913153388283\n",
      "Epoch 56/200, Loss: 4.605836694890803\n",
      "Epoch 57/200, Loss: 4.6002845547416\n",
      "Epoch 58/200, Loss: 4.603538729927757\n",
      "Epoch 59/200, Loss: 4.583636088804766\n",
      "Epoch 60/200, Loss: 4.576207832856611\n",
      "Epoch 61/200, Loss: 4.59679388999939\n",
      "Epoch 62/200, Loss: 4.573238026012074\n",
      "Epoch 63/200, Loss: 4.573550961234353\n",
      "Epoch 64/200, Loss: 4.558105165308172\n",
      "Epoch 65/200, Loss: 4.5639801458878955\n",
      "Epoch 66/200, Loss: 4.560050574215976\n",
      "Epoch 67/200, Loss: 4.554034666581587\n",
      "Epoch 68/200, Loss: 4.549527038227428\n",
      "Epoch 69/200, Loss: 4.550414627248591\n",
      "Epoch 70/200, Loss: 4.529422976753929\n",
      "Epoch 71/200, Loss: 4.536462090232155\n",
      "Epoch 72/200, Loss: 4.524856350638649\n",
      "Epoch 73/200, Loss: 4.516071883114901\n",
      "Epoch 74/200, Loss: 4.516095204786821\n",
      "Epoch 75/200, Loss: 4.507447697899559\n",
      "Epoch 76/200, Loss: 4.511954155835238\n",
      "Epoch 77/200, Loss: 4.504125118255615\n",
      "Epoch 78/200, Loss: 4.508198391307484\n",
      "Epoch 79/200, Loss: 4.4976115876978096\n",
      "Epoch 80/200, Loss: 4.486072973771528\n",
      "Epoch 81/200, Loss: 4.4825545224276455\n",
      "Epoch 82/200, Loss: 4.479160222140226\n",
      "Epoch 83/200, Loss: 4.482218525626442\n",
      "Epoch 84/200, Loss: 4.468472567471591\n",
      "Epoch 85/200, Loss: 4.480705694718794\n",
      "Epoch 86/200, Loss: 4.485688339580189\n",
      "Epoch 87/200, Loss: 4.4704366814006455\n",
      "Epoch 88/200, Loss: 4.467468456788496\n",
      "Epoch 89/200, Loss: 4.47233061356978\n",
      "Epoch 90/200, Loss: 4.4426633661443535\n",
      "Epoch 91/200, Loss: 4.44267615404996\n",
      "Epoch 92/200, Loss: 4.43174223466353\n",
      "Epoch 93/200, Loss: 4.422001730312001\n",
      "Epoch 94/200, Loss: 4.423173492605036\n",
      "Epoch 95/200, Loss: 4.396844387054443\n",
      "Epoch 96/200, Loss: 4.40452690558\n",
      "Epoch 97/200, Loss: 4.395287600430575\n",
      "Epoch 98/200, Loss: 4.381356130946767\n",
      "Epoch 99/200, Loss: 4.39293757351962\n",
      "Epoch 100/200, Loss: 4.37973503632979\n",
      "Epoch 101/200, Loss: 4.372470075433904\n",
      "Epoch 102/200, Loss: 4.401558702642268\n",
      "Epoch 103/200, Loss: 4.379604469646107\n",
      "Epoch 104/200, Loss: 4.3458970026536425\n",
      "Epoch 105/200, Loss: 4.341785279187289\n",
      "Epoch 106/200, Loss: 4.349325440146706\n",
      "Epoch 107/200, Loss: 4.347172086889094\n",
      "Epoch 108/200, Loss: 4.3088600202040235\n",
      "Epoch 109/200, Loss: 4.306805220517245\n",
      "Epoch 110/200, Loss: 4.318650072271174\n",
      "Epoch 111/200, Loss: 4.296926455064253\n",
      "Epoch 112/200, Loss: 4.281072074716741\n",
      "Epoch 113/200, Loss: 4.2588183012875644\n",
      "Epoch 114/200, Loss: 4.27577146616849\n",
      "Epoch 115/200, Loss: 4.244479645382274\n",
      "Epoch 116/200, Loss: 4.237928087061102\n",
      "Epoch 117/200, Loss: 4.263084368272261\n",
      "Epoch 118/200, Loss: 4.260226759043607\n",
      "Epoch 119/200, Loss: 4.240436315536499\n",
      "Epoch 120/200, Loss: 4.219279776919972\n",
      "Epoch 121/200, Loss: 4.23144396868619\n",
      "Epoch 122/200, Loss: 4.199494307691401\n",
      "Epoch 123/200, Loss: 4.1800373792648315\n",
      "Epoch 124/200, Loss: 4.177763982252642\n",
      "Epoch 125/200, Loss: 4.151996580037204\n",
      "Epoch 126/200, Loss: 4.154509696093473\n",
      "Epoch 127/200, Loss: 4.143247669393366\n",
      "Epoch 128/200, Loss: 4.1270588636398315\n",
      "Epoch 129/200, Loss: 4.119410048831593\n",
      "Epoch 130/200, Loss: 4.110988584431735\n",
      "Epoch 131/200, Loss: 4.114930098707026\n",
      "Epoch 132/200, Loss: 4.113432981751182\n",
      "Epoch 133/200, Loss: 4.084666620601308\n",
      "Epoch 134/200, Loss: 4.0755507404154\n",
      "Epoch 135/200, Loss: 4.066768082705411\n",
      "Epoch 136/200, Loss: 4.061143550005826\n",
      "Epoch 137/200, Loss: 4.040102698586204\n",
      "Epoch 138/200, Loss: 4.04283449866555\n",
      "Epoch 139/200, Loss: 3.976084665818648\n",
      "Epoch 140/200, Loss: 4.000958150083369\n",
      "Epoch 141/200, Loss: 3.9907608248970727\n",
      "Epoch 142/200, Loss: 3.9780977639285\n",
      "Epoch 143/200, Loss: 3.960652221332897\n",
      "Epoch 144/200, Loss: 3.9661205898631704\n",
      "Epoch 145/200, Loss: 3.931189168583263\n",
      "Epoch 146/200, Loss: 3.9071895317597822\n",
      "Epoch 147/200, Loss: 3.8774552670392124\n",
      "Epoch 148/200, Loss: 3.870549299500205\n",
      "Epoch 149/200, Loss: 3.8547061681747437\n",
      "Epoch 150/200, Loss: 3.8565453507683496\n",
      "Epoch 151/200, Loss: 3.8090969865972344\n",
      "Epoch 152/200, Loss: 3.8087332248687744\n",
      "Epoch 153/200, Loss: 3.851357178254561\n",
      "Epoch 154/200, Loss: 3.8289567015387793\n",
      "Epoch 155/200, Loss: 3.802581050179221\n",
      "Epoch 156/200, Loss: 3.768495798110962\n",
      "Epoch 157/200, Loss: 3.7422237504612315\n",
      "Epoch 158/200, Loss: 3.7096302184191616\n",
      "Epoch 159/200, Loss: 3.7325621518221768\n",
      "Epoch 160/200, Loss: 3.7050400538878008\n",
      "Epoch 161/200, Loss: 3.672305302186446\n",
      "Epoch 162/200, Loss: 3.678368178280917\n",
      "Epoch 163/200, Loss: 3.6581456552852285\n",
      "Epoch 164/200, Loss: 3.6752706441012295\n",
      "Epoch 165/200, Loss: 3.6285946694287388\n",
      "Epoch 166/200, Loss: 3.61486833745783\n",
      "Epoch 167/200, Loss: 3.6024224649776113\n",
      "Epoch 168/200, Loss: 3.5653666908090766\n",
      "Epoch 169/200, Loss: 3.590918107466264\n",
      "Epoch 170/200, Loss: 3.589418053627014\n",
      "Epoch 171/200, Loss: 3.583926970308477\n",
      "Epoch 172/200, Loss: 3.5264733704653652\n",
      "Epoch 173/200, Loss: 3.496986237439242\n",
      "Epoch 174/200, Loss: 3.498703187162226\n",
      "Epoch 175/200, Loss: 3.4691785790703515\n",
      "Epoch 176/200, Loss: 3.521102049133994\n",
      "Epoch 177/200, Loss: 3.4755740599198774\n",
      "Epoch 178/200, Loss: 3.472778710451993\n",
      "Epoch 179/200, Loss: 3.4411401748657227\n",
      "Epoch 180/200, Loss: 3.414426424286582\n",
      "Epoch 181/200, Loss: 3.4005618420514194\n",
      "Epoch 182/200, Loss: 3.38085156137293\n",
      "Epoch 183/200, Loss: 3.3515680703249844\n",
      "Epoch 184/200, Loss: 3.370722077109597\n",
      "Epoch 185/200, Loss: 3.332674730907787\n",
      "Epoch 186/200, Loss: 3.2870789766311646\n",
      "Epoch 187/200, Loss: 3.2852852561257104\n",
      "Epoch 188/200, Loss: 3.2148490385575728\n",
      "Epoch 189/200, Loss: 3.2181343165310947\n",
      "Epoch 190/200, Loss: 3.1650846979834815\n",
      "Epoch 191/200, Loss: 3.192149964245883\n",
      "Epoch 192/200, Loss: 3.1906063123182817\n",
      "Epoch 193/200, Loss: 3.10752948847684\n",
      "Epoch 194/200, Loss: 3.1036283753134986\n",
      "Epoch 195/200, Loss: 3.121229486031966\n",
      "Epoch 196/200, Loss: 3.09802904996005\n",
      "Epoch 197/200, Loss: 3.0844344334168867\n",
      "Epoch 198/200, Loss: 3.0981352112509986\n",
      "Epoch 199/200, Loss: 3.1060820709575307\n",
      "Epoch 200/200, Loss: 3.0768184661865234\n",
      "Validation Loss: 5.295373439788818\n"
     ]
    }
   ],
   "source": [
    "from coqui_stt import Model\n",
    "\n",
    "# Load the teacher model\n",
    "teacher_model_path = \"/scratch2/bsow/Documents/ACSR/data/models/kenlm.scorer\"\n",
    "alphabet_path = \"/scratch2/bsow/Documents/ACSR/data/models/alphabet.txt\"\n",
    "\n",
    "# Initialize the teacher model\n",
    "teacher_model = Model(teacher_model_path)\n",
    "teacher_model.enableExternalScorer(teacher_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.295373439788818\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded training phoneme sequences: [['la', 'p', 'i', '', 'w', '', 't'], ['pu', 'v', 'z', '', '', 'p', 'sa', 't'], ['m', 'p', '', '', 't'], ['a', 't'], ['b', 't', '', 'p', '', 't'], ['d', 't'], ['le', 'la', 'ka', 'do', 'pu', 'a', 't'], ['t'], ['s', 'f', 'l', 'l', 't'], ['la', 'p', 'l', 't', 'd', 't'], ['i', 'p', 't'], ['pu', 'o', 'n', 't', 't'], ['t'], ['i', 't'], ['pu', 'a', '', 'za', 'm', '', 'ma', 'n', 't'], ['i', 't'], ['', 'a', 't'], ['t'], ['le', 'p', 't', 'k', 'ji', '', 'd', 't'], ['a', 't'], [''], ['le', 'n', 'z', 's', 'l', 'ze', 't'], ['d', 't'], ['i', 't', '', 'ze', 'na', 't', 'e'], ['la', 's', 's', 's', 't', '', 't'], ['la', 'ya', 's', 'a', 't'], ['pu', 't'], ['le', ''], ['l', 't'], ['i', 'la', 'f', 'd', 's', '', 'd', 't'], ['pu', 'z', 'l', 't'], ['la', 'd', 's', 'a', 'ti', 'a', 't'], ['la', 'd', 's', 't'], ['t'], ['a', 'p', 'd', 't'], ['i', 'a', 'i', 'di', 'fi', 't'], ['le', 'k', 't'], ['b', 'k', 'ji', 't'], ['la', 'p', 't'], ['le', 't', 't'], ['a', 't'], ['la', '', 't'], ['d', 't'], ['la', 't', 'd', '', 't'], ['d', 't'], ['le', '', 'p', 'd', 'l', 't'], ['o', 'p', 'l', 'a', 'd', 't'], ['la', 'p', 't'], ['pu', 'd', '', 't'], ['l', '', 'b', 'd', 'l', 'd', 't'], ['pu', 'b', 'd', 'd', 't'], ['a', 'n', 'fu', 't'], ['pu', 't', 'm', 'a', 'd', 't'], ['pu', 'se', 't'], ['pu', 't', 'd', 'vo', 'pa', 't'], ['le', 't'], ['l', 'n', 'fu', 't'], ['i', 'd', 't', 't'], ['a', ''], ['a', 't'], ['l', '', 't'], ['i', 't'], ['le', 't'], ['e', '', 't'], ['', 'd', 'ye', 'n', 'sa', 't'], ['la', 'd', 'l', 't'], ['d', 't'], ['i', 'a', 't'], ['pu', '', 's', 'l', 't'], ['le', 'm', 'ji', 'l', 't'], ['a', 't'], ['la', 'p', 'la', 't'], ['la', 'd', 't'], ['a', 'm', 'e', 't'], ['le', 'o', '', 'ce', 't', 'o', 'ze', 't', 'a', 't'], ['i', 't'], ['i', 'z', 'k', 's', 't'], ['j', 'n', 'tu', 'e', 't'], ['pu', 'n', 'pa', 'v', 'wa', '', 'f', 'i', 'lo', 't'], ['d', ''], ['o', 'd', '', 'zi', 'ke', 't'], ['pu', 'k', 'e', 'ci', 'd', 't'], ['le', 't'], ['na', 't', 'se', 'e', 't'], ['d', 't']]\n",
      "True training phoneme sequences: [['d', 'p', 'i', 't', '', 'w', '', '', 'l', 't', 'a', 'va', 'ja', 'j', 'd', 'la', '', 'm'], ['k', 'm', 'se', 'v', 'wa', 'z', '', 'la', '', 'l', 'm', 'm', '', 'p', 'sa', 'v', 'wa', 't', 'y', ''], ['m', 'p', '', 'f', 's', '', 'ma', 'di', 'ci', 'la', 'v', 'tu', 'u', '', 'y', 'p', '', 'd', 'la', 'po', 'v', '', 'te'], ['di', 'na', 'a', 'e', 'te', 's', 'y', '', 'p', 'i', 'z', 'da', 'p', '', 'd', '', 'k', 's', 'm', 'j', 'a', 'mi', 'e', 't', 'm', ''], ['sa', 'nu', 'v', 'l', '', 'b', 'a', '', 'm', 'y', '', 't', 'j', '', 'm', 'p', '', 'u', ''], ['d', 's', 'a', 'i', 've', 'd', 'l', '', '', 'ma', 'a', 'z', '', 'l', 's', 'a', '', 'te', '', 'pa', 'a', 'p', 'l', 'i'], ['la', 'fi', 'j', 'ta', '', 's', 'y', 's', 'ka', 'do', 'pu', '', 's', 'a', 'i', 'v', '', 's', ''], ['', '', 't', '', 'le', 'a', 't', 'u', 'va', 's', 'y', '', 'la', 'ta', 'b', 'l', 's', 'v', 'j', 'ku', 'to'], ['', 'f', 'l', 'a', 'v', 'k', 'l', '', 'y', 'de', 'p', 'm'], ['a', 'p', '', 'a', 'v', 'wa', 'e', 'te', 'la', 've', 'l', 'k', 'l', 'ne', 't', 'p', 'l', 'y', 'ta', 'e', 'd', 'b', 'wa'], ['i', 'la', 'p', 'e', 'si', 'la', 'i', '', 's', 'ka', 'i', 'l', '', '', 'di', 'd', 'la', 'po', 'v', '', 'te'], ['y', 'o', 'vi', 'v', 'd', 'y', 'n', 'ka', 'ba', 'n', 'ci', 'la', 'v', 'l', 'i', 'm', 'm', 'k', 's', 't', '', 'i', 't'], ['', 't', '', '', 'n', '', 'j', 'sa', 'p', 'l', '', 'jo'], ['i', 'l', '', 't', 'e', 'd', 's', 't', '', 'b', 'e', '', 'm', 't', 'n', 'ma', 'la', 'd'], ['pu', '', 'e', 'y', 'si', 'a', 'l', '', 'za', 'm', '', 'ma', 'n', 'm', 'c', 'pa', 'd', 'f', '', 's'], ['d', 's', 't', 'b', 'wa', 't', 's', 'ka', '', '', 'jo'], ['', 's', 'i', 'a', 'le', 'a', '', 'te', '', 'a'], ['a', 'v', 'k', 'l', 'p', 'wa', 's', 'la', 'so', 's', 'ne', 't', 'pa', 't', '', 'f', '', 't'], ['i', 'za', 'b', 'le', 't', 's', '', 'ti', 'k', 'ji', '', 'bu', 'c', 'd', 'm', 't'], ['le', 'd', 'v', 'wa', 'z', '', 'di', 's', 'k', 'y', 'te', 'e', '', 'k', 'p', 'i', 'ci', 'le', 't', 'a', 'mu', ''], ['ma', 'n', 'a', 'e', 'te', 't', 'i', 's', 't', 'd', 'de', 'ku', 'v', 'i', '', 'k', 's', 'p', 'wa', 's', 'u', 'e', 't', 'm', ''], ['le', 'n', '', 'm', 'va', 'i', 'z', 's', 'b', 'l', 'p', 'ze', 'y', 'n', 't', 'n'], ['a', 'p', '', 'd', 's', 'm', 'ni', 'i', 'a', 'v', 'p', 'l', 'd', 'v', 't', 'm', 'sa', 'la', 'la', 've'], ['i', 'l', 'vu', 'l', 't', 'a', 'va', 'je', 'pu', '', 'y', 'n', 't', '', 'p', 'i', 'ze', 'na', 'pa', 'e', 'zi', 'te', 'a', 'si', 'e', 'l', 'k', 't', 'a'], ['la', 'n', 's', 'd', 's', 'l', 'd', 't', '', '', 'p', '', 't', 'ta', 'a', 'ti', 'e', 'y', 'ni', 'm', 's', 'fu', 'l'], ['la', 'ku', 'l', '', 'la', 'p', 'l', 'ya', 's', 's', 'je', 'a', 'la', 'k', 'l', '', 'l', 'u', ''], ['pu', 'a', 'i', 've', 'a', 't', 'i', 'lo', '', 'd', 'y', 'ma', '', 'e', 'p', 'l', 'y', 'vi', 't'], ['pu', '', 'n', 'e', 'la', 'm', '', 'd', 's', 'k', 'l', '', 'ci', 'ma', 'a', 'si', 's', 'te', 'a', 's', '', 't', '', 'm'], ['l', '', 'y', 'd', '', 't', '', 'a', 'si', 'd'], ['p', 'd', 'la', 'e', 'y', '', 'le', '', 'f', 'd', 'sa', 's', '', 'de', 'si', 'de', 'd', 'f', '', 'p'], ['pu', '', '', 't', 'e', 'd', 'la', 'm', 'z', 'l', 's', 'p', 'k', 't', 'a', 'd', 'y', 'm', 't', 'e', 's', 'ba'], ['', 'la', 's', 'y', '', 'e', 'e', 'a', 's', 'p', 'ti', 'a', 'mi', 'ba', '', 'b', 'y', 'd', 's', 'a', 'ze'], ['i', 'la', 'di', 'a', 's', 'f', '', '', 'ci', 'l', 'd', 'v', 'v', '', 'm', 's', 'a', 'ze'], ['s', 'si', 'd'], ['a', 'p', '', 's', 't', '', 'o', 's', 'be', 'ti', 'zi', 'l', 'u', 'i', 'd', '', 't'], ['m', 'f', '', '', 'p', 'se', 'd', 'de', 'a', 'i', 'mo', 'di', 'fi', 'si', 'la', 'nu', 'i', ''], ['i', 'la', 'v', 'p', '', 'de', 'mi', 'k', '', 'be', 'k', '', '', 'd', 't', 'be', 'ma', 'la', 'd'], ['b', '', 'na', '', 'la', 'v', 'a', 'k', 'ji', 'a', 'v', 'k', 'te', 'a', 'la', 'm', 't'], ['', 'sa', 'p', '', '', 'd', 'y', 'n', '', 'y', '', 'la', 'e', 'te', 'pi', 'ce', 'pa', '', 'y', 'na', 'b', 'j'], ['le', 'p', 'i', 's', 'je', 'na', 'v', 'pa', 'd', 'y', 'tu', 'k', 'p', 'i', 'c', 'l', 'se', 't', '', 'k', 't', 'e'], ['', 'l', 'y', 'k', 's', 'ku', 'pe', 'l', 'd', 'wa', 'a', 'v', 'k', 'ku', 'to'], ['la', 'm', '', 'c', 'ta', 'v', 'p', 'l', 'y', 'z', 'j', '', 'bu', 'a', 'nu', 'i', ''], ['d', 'l', '', '', 'tu', 'a', 'la', 'm', 'z', 'va', 'l', 't', 'l', 'i', 'a', 'd', 'ne', 'y', 'n', 's', '', 'v', 'j', 't'], ['la', 'p', 'ti', 't', 'fi', 'je', 't', 'k', 't', 't', 'd', 'p', '', 'd', '', 'sa', 'p', '', '', '', 'd'], ['le', 'ma', '', '', 'd', 'le', '', 'y', 'm', 'n', 'pa', 'la', 'bi', 't', 'y', 'd', 'd', 'f', '', 'la', '', ''], ['le', 'ci', 'p', 's', '', 't', '', 'ne', 'd', 't', '', 'l', '', 's', 'm', 'n', 's', 'y', '', 's', 'ba', 'to'], ['o', 'p', '', 't', 'd', 'l', 'a', '', 'd', 's', 'p', '', 't', 'b', 'la', 'p', 'lu', 'z'], ['la', 'p', '', 'mi', 'di', 'i', 'l', 's', 'y', 'pu', '', 'k', 'wa', 'i', 'la', 'v', '', 't'], ['pu', 'a', 'me', '', 'e', 's', 't', '', 's', 'ti', 'l', 'fo', 'd', '', 'a', 'u', 'te', '', 'p', 'd', 's', 'y', 'k', ''], ['d', 'l', '', '', 'b', '', 'le', '', 'f', '', 't', 'd', 'b', 'j', 'l', 'b', '', 'i', 'd', 'la', 'f', 't'], ['', '', 'a', '', 'd', 's', 'i', 'v', 'i', 'l', 'd', 'vi', 'ne', 'l', 's', 'p', 's', 'd', 'la', 'b', 'j'], ['la', 'v', 'j', 'j', 'fa', 'm', 'm', 'd', 's', 'sa', 'k', 'p', 'ti', 'b', 'i', 'c', 'e', 'y', 'n', 'fu', '', '', 't'], ['pu', 'u', 'v', 'i', '', 'la', 'p', '', 't', 't', 'm', 'a', 'ti', 'e', 'd', 'tu', 't', 'se', 'f', '', 's'], ['d', 'sa', 'ma', '', 'i', 'te', 'i', 'l', 'se', 't', '', 'a', 'e', 'd', 'la', '', 'me'], ['', 'y', 'a', 'e', 't', 'd', 'nu', 'vo', 'pa', '', 'ti', '', 'va', 'k', 'sa', 'v', 'k', 's', 'a', 'mi'], ['', 'l', 'n', '', 'k', '', 'a', 'm', 'p', 'se', 'd', 'v', 'la', 'bu', '', 's'], ['l', 'p', 'ti', 'a', '', 's', 'm', '', 's', 's', 't', 'ka', 'v', 'k', 'ku', 'to', 'e', 'y', 'n', 'fu', '', '', 't'], ['e', 'mi', 'i', 'la', 'v', 'de', 't', 's', 'te', 'd', 'l', 'm', 'm', 'u', '', 'l', 'se', 't', '', 'k', 't', 'e'], ['sa', 'a', 'di', 'mi', 'n', 'e', 'e', '', 'l', 'd', 'wa', 'a', '', 'te', 'y', 'n', 'nu', 'v', 'l', 'p', '', 'd', 'l', 'y', 'n', 't'], ['s', 'p', 'a', 'v', 'd', 'l', 't', 'y', 'n', 'k', '', 'wa', 'ja', 'b', 'l', 'k', 'l', 'k', 's', 'j', 'd', 'f', 't', 's'], ['l', 'p', '', '', 'm', 'pa', 'se', 'la', 'u', '', 'ne', '', 'm', '', 's', 'y', '', 's', 'ba', 'to'], ['a', 'p', '', 'y', 'n', 'ba', 'la', 'd', 'd', 'le', '', 'me', 'b', 'te', 't', 'ku', 'v', '', 't', 'd', 'bu'], ['le', 't', 's', 'j', '', 't', '', 'le', 'p', 'i', 'pu', '', 'a', 'bu', 'ti', 'a', 'la', '', ''], ['e', '', 't', 'd', 'y', 'v', 'j', 'le', '', 'a'], ['a', 's', 'pa', 'e', 't', 'p', '', 'd', 'ye', 'n', 'sa', 'v', 'pa', 'k', 'wa', 'f', ''], ['', 'la', 'a', '', 'te', '', 'po', 'd', 'f', 'l', '', 'pu', '', 's', 'a', 'i', 'v', '', 's', ''], ['le', 'fo', '', 'y', 'mo', 'e', 't', 'o', 'si', 'di', 'fe', '', 'k', 'l', 'u', 'e', 'la', 'n', 'i'], ['a', 'p', '', 'sa', 'pa', 'n', 'pa', 's', 'la', '', 'de', 'a', 'pu', 'se', 'sa', 'v', 'wa', 't', 'y', ''], ['pu', 'a', 'v', 'wa', '', 'pa', '', 'ti', 'si', 'pe', 'a', 'la', 'ku', '', 's', 'l', 'f', 'a', '', 's', 'y', 'ba'], ['le', '', 'm', 'fa', 'ji', 't', 'be', 'd', 'l', 't', 'u', 'ci', 'la', 'v', 'k', '', 'ze'], ['i', 'l', 'va', 'a', '', 'te', 'de', 'a', 'k', 's', 'j', 'k', 'te', '', 'bu', '', 's'], ['s', 'p', 'la', '', 'bo', 'ku', 't', 'o', 'a', 'si', 'd'], ['la', 'bu', 'ti', 'k', 'da', 'k', 's', 's', 'wa', 'e', 't', 'd', '', 'ka', '', 't', 'je', 'e', 'l', 'wa', 'n', 'je', 'd', 'la', 'pu', 'l'], ['s', 'a', 'mi', 'tu', 'u', 'a', 'd', 'e', 'm', 'e', 'de', 'p', 'm'], ['le', 'o', 'e', 't', '', 'ce', 'd', 'v', 's', 't', 'o', 'ze', 't', '', 'a', 'f', ''], ['i', 'l', 'l', 'i', 'a', 'mi', '', 'ku', 'to', 'su', 'la', '', '', ''], ['ka', 'mi', 'je', 't', 't', '', 's', 'y', '', 'p', 'i', 'z', 'k', 's', 'ma', 'i', 'n', 'p', 'a', 'e'], ['f', 'ta', 't', 's', 'j', 'a', 'n', 'pa', 'tu', 'e', 'la', 'p', 'wa', 'l', 'k', 't', 'l', 'o', 'd'], ['pu', '', 'n', 'pa', 'a', 'v', 'wa', '', 'f', 'i', 'lo', '', 'd', 'y', 'm', 'e', 'p', 'l', 'y', 'vi', 't'], ['', 'la', '', 'p', '', 'te', 's', 'nu', 'v', 'la', 'pa', '', 'j', 'pu', '', 'p', '', 'd', '', 'd', 'b', 'l', 'f', 't', 's'], ['o', 'k', 's', '', 'd', '', 'k', 'la', 'm', 'y', 'zi', 'ke', 't', 't', 'o', 'f', '', 't'], ['ce', 'vi', 'ne', 't', 'de', 's', 'y', 'k', 's', 'e', 'ci', 'p', 'd', 'ba', 's', 'c', 't', 'n', 'pa', 'a', 'e'], ['a', 'p', '', 'c', 'l', 'k', 'm', 'wa', 'i', 'l', '', 'a', 'e', '', '', 'm', 'pu', '', 'tu', 'la', 've'], ['a', 'p', '', 's', 't', '', 'la', 've', 'le', 'm', 'na', 't', 'le', 'se', 'e', 'a', 'v', 'k', 'y', 'n', 's', '', 'vo'], ['d', 'la', 'p', '', '', '', 'u', 't', 'd', 'p', 'l', 'i', '', 'la', 'u', 'v', '', 's', 'pa', 'a', 'p', 'li']]\n",
      "Decoded validation phoneme sequences: [['pu', 't'], ['la', 'd', 't'], ['le', 't'], ['pu', 'd', 'd', 't'], ['d', 't'], ['la', 't', 't', 'a', 'k', 'd', 'l', 'k', 'n', 'ji', 't'], ['a', ''], ['wa', 'j', 'n', 'e', 't', 'd', 'ti', 't'], ['la', 't', 'pa', 'n', 't'], ['i', 't']]\n",
      "True validation phoneme sequences: [['pu', '', 'f', 'te', 's', 'de', 'pa', '', 'se', 'a', 'mi', 'l', 'i', '', '', '', 'a', 'i', 'ze', 'y', 'n', 'f', 't'], ['l', 'po', 'v', '', 'm', 's', 'ce', 't', 'tu', 'u', '', 'd', 'p', '', 'd', '', 'y', 'n', 'nu', 'v', 'l', 'd'], ['', 'l', 'm', 'bo', 'ku', 'la', 'i', 'b', '', 'te', 'k', 'l', '', 'f', '', 'la', 'n', 'i'], ['pu', '', '', 'd', '', 's', 'ka', 'fe', 'm', 'w', 'a', 'm', '', 'l', 'mi', '', 'm', '', 'so', 'd', 's', 'y', 'k', ''], ['le', 'd', 'e', 't', 'y', 'd', 'j', 's', 's', '', 'k', 't', 'e', 'e', 's', 'a', 'pi', 'd', 'm', 't', 'be', 'a', 'mu', ''], ['p', 'd', 'la', '', 'd', 'ne', 'le', 'p', 'je', 'd', 'i', 'la', 'l', 'i', 'f', 'z', 'p', 'l', 'y', 'ma', 'la', 'a', 'k', 'p'], ['i', 'l', 'm', 'l', 'k', 'm', '', 'se', 'a', 'de', 'si', 'de', 'du', 'v', 'i', '', 'ma', 'a', 'z', 'd', 'l', 'y', 'n', 't'], ['k', 'i', 'di', 'k', 's', '', 'j', 'a', 'v', 'tu', 'u', 'e', 'te', 's', 'm', 'j', 'a', 'mi'], ['a', 't', 'd', 'nu', 'a', 's', 'k', 's', 't', 'm', 'z', 'n', 's', 'wa', 'pa', 'v', '', 'm', 'o', 'd'], ['', 'l', 'tu', 'u', '', 'y', 'ma', 'l']]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(output, blank):\n",
    "    \"\"\"\n",
    "    Decode model outputs using a greedy decoder.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model outputs of shape (batch_size, sequence_length, num_classes).\n",
    "        blank (int): Index of the blank token.\n",
    "\n",
    "    Returns:\n",
    "        list: List of decoded sequences.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)  # Get the most likely class for each time step\n",
    "    decodes = []\n",
    "    for args in arg_maxes:\n",
    "        decode = []\n",
    "        previous_idx = None\n",
    "        for index in args:\n",
    "            if index != blank and (previous_idx is None or index != previous_idx):\n",
    "                decode.append(index.item())  # Append non-blank and non-repeated tokens\n",
    "            previous_idx = index\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "\n",
    "def decode_loader(model, loader, blank, index_to_phoneme):\n",
    "    \"\"\"\n",
    "    Decode outputs for all batches in a DataLoader and return both decoded and true sequences.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        loader (torch.utils.data.DataLoader): DataLoader containing input data and labels.\n",
    "        blank (int): Index of the blank token.\n",
    "        index_to_phoneme (dict): Mapping from indices to phonemes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (decoded_sequences, true_sequences), where:\n",
    "            - decoded_sequences: List of decoded phoneme sequences.\n",
    "            - true_sequences: List of true phoneme sequences.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_decoded_sequences = []\n",
    "    all_true_sequences = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, y_batch in loader:  # Iterate over batches (X_batch: inputs, y_batch: labels)\n",
    "            X_batch = X_batch.to(device)  # Move inputs to device\n",
    "            y_batch = y_batch.to(device)  # Move labels to device\n",
    "            outputs = model(X_batch)  # Get model predictions\n",
    "            decoded_phoneme_sequences = greedy_decoder(outputs, blank=blank)  # Decode outputs\n",
    "            decoded_phonemes = [[index_to_phoneme[idx] for idx in sequence] for sequence in decoded_phoneme_sequences]  # Convert indices to phonemes\n",
    "            all_decoded_sequences.extend(decoded_phonemes)  # Add to the list of decoded sequences\n",
    "\n",
    "            # Convert true labels to phoneme sequences\n",
    "            true_phoneme_sequences = [[index_to_phoneme[idx.item()] for idx in sequence if idx != blank and \n",
    "                                       index_to_phoneme[idx.item()] != \" \"] for sequence in y_batch]\n",
    "            all_true_sequences.extend(true_phoneme_sequences)  # Add to the list of true sequences\n",
    "\n",
    "    return all_decoded_sequences, all_true_sequences\n",
    "\n",
    "\n",
    "# Example usage\n",
    "blank_token = len(phoneme_to_index)  # Index of the blank token\n",
    "decoded_train_sequences, true_train_sequences = decode_loader(model, train_loader, blank_token, index_to_phoneme)\n",
    "decoded_val_sequences, true_val_sequences = decode_loader(model, val_loader, blank_token, index_to_phoneme)\n",
    "\n",
    "# Print results\n",
    "print(\"Decoded training phoneme sequences:\", decoded_train_sequences)\n",
    "print(\"True training phoneme sequences:\", true_train_sequences)\n",
    "print(\"Decoded validation phoneme sequences:\", decoded_val_sequences)\n",
    "print(\"True validation phoneme sequences:\", true_val_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PER (jiwer): 0.84232868405094 1 - PER:  0.15767131594906003\n",
      "Validation PER (jiwer): 0.9512195121951219 1 - PER:  0.04878048780487809\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_per_with_jiwer(decoded_sequences, true_sequences):\n",
    "    \"\"\"\n",
    "    Calculate the Phoneme Error Rate (PER) using jiwer.\n",
    "\n",
    "    Args:\n",
    "        decoded_sequences (list): List of decoded phoneme sequences.\n",
    "        true_sequences (list): List of true phoneme sequences.\n",
    "\n",
    "    Returns:\n",
    "        float: Phoneme Error Rate (PER).\n",
    "    \"\"\"\n",
    "    # Convert phoneme sequences to space-separated strings\n",
    "    decoded_str = [\" \".join(seq) for seq in decoded_sequences]\n",
    "    true_str = [\" \".join(seq) for seq in true_sequences]\n",
    "\n",
    "    # Calculate PER using jiwer\n",
    "    per = jiwer.wer(true_str, decoded_str)\n",
    "    return per\n",
    "\n",
    "# Example usage\n",
    "train_per = calculate_per_with_jiwer(decoded_train_sequences, true_train_sequences)\n",
    "val_per = calculate_per_with_jiwer(decoded_val_sequences, true_val_sequences)\n",
    "\n",
    "print(\"Training PER (jiwer):\", train_per, \"1 - PER: \", 1 - train_per)\n",
    "print(\"Validation PER (jiwer):\", val_per, \"1 - PER: \", 1 - val_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-jnt0UJEK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
