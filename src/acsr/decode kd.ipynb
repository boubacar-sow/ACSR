{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load CSV files from a directory based on a filename pattern\n",
    "def load_csv_files(directory, filename_pattern):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            df.dropna(inplace=True)\n",
    "            base_name = filename.split(filename_pattern)[0]\n",
    "            files_data[base_name] = df\n",
    "    return files_data\n",
    "\n",
    "# Load features from .npy files based on a filename pattern\n",
    "def load_features(directory, filename_pattern):\n",
    "    files_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename_pattern in filename:\n",
    "            features = pd.read_csv(os.path.join(directory, filename))\n",
    "            features.dropna(inplace=True)\n",
    "            base_name = filename.split('_features')[0]\n",
    "            files_data[base_name] = features\n",
    "    return files_data\n",
    "\n",
    "# Find corresponding phoneme files based on the base names of position filenames\n",
    "def find_phoneme_files(directory, base_names):\n",
    "    phoneme_files = {}\n",
    "    for base_name in base_names:\n",
    "        phoneme_file = os.path.join(directory, f'{base_name}.lpc')\n",
    "        if os.path.exists(phoneme_file):\n",
    "            phoneme_files[base_name] = phoneme_file\n",
    "    return phoneme_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from praatio import textgrid as tgio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================================\n",
    "# Helper Functions\n",
    "# ==========================================================\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of sequences to pad.\n",
    "        max_length (int): Maximum length to pad to.\n",
    "        pad_value (int): Value to use for padding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded sequences.\n",
    "    \"\"\"\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            padding = np.full((max_length - len(seq), seq.shape[1]), pad_value)\n",
    "            padded_seq = np.vstack((seq, padding))\n",
    "        else:\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "def combine_sequences_with_padding(video_data):\n",
    "    \"\"\"\n",
    "    Combine sequences with padding to ensure uniform length.\n",
    "\n",
    "    Args:\n",
    "        video_data (dict): Dictionary containing video data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded input sequences (X_student_hand_shape, X_student_hand_pos, X_student_lips, X_teacher) and padded labels (y).\n",
    "    \"\"\"\n",
    "    max_length = max(len(video_data[video][\"X_student_hand_shape\"]) for video in video_data)\n",
    "    \n",
    "    # Pad hand shape features\n",
    "    X_student_hand_shape_padded = [\n",
    "        pad_sequences([video_data[video][\"X_student_hand_shape\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad hand position features\n",
    "    X_student_hand_pos_padded = [\n",
    "        pad_sequences([video_data[video][\"X_student_hand_pos\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad lip features\n",
    "    X_student_lips_padded = [\n",
    "        pad_sequences([video_data[video][\"X_student_lips\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad teacher features\n",
    "    X_teacher_padded = [\n",
    "        pad_sequences([video_data[video][\"X_teacher\"]], max_length)[0] for video in video_data\n",
    "    ]\n",
    "    \n",
    "    # Pad labels\n",
    "    y_padded = [\n",
    "        video_data[video][\"y\"]\n",
    "        + [phoneme_to_index[\" \"]] * (max_length - len(video_data[video][\"y\"]))\n",
    "        for video in video_data\n",
    "    ]\n",
    "    \n",
    "    return X_student_hand_shape_padded, X_student_hand_pos_padded, X_student_lips_padded, X_teacher_padded, y_padded\n",
    "\n",
    "def compute_log_mel_spectrogram(audio_path, sr=16000, n_fft=400, hop_length=160, n_mels=80):\n",
    "    \"\"\"\n",
    "    Compute the log-mel spectrogram for an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        sr (int): Sample rate.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.\n",
    "        n_mels (int): Number of mel bands.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Log-mel spectrogram of shape (num_frames, n_mels).\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "    # Compute mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
    "    )\n",
    "\n",
    "    # Convert to log scale\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Transpose to (num_frames, n_mels)\n",
    "    log_mel_spectrogram = log_mel_spectrogram.T\n",
    "\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "def parse_textgrid(textgrid_path):\n",
    "    \"\"\"\n",
    "    Parse a TextGrid file to extract phoneme-level intervals.\n",
    "\n",
    "    Args:\n",
    "        textgrid_path (str): Path to the TextGrid file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (start_time, end_time, phoneme) tuples.\n",
    "    \"\"\"\n",
    "    tg = tgio.openTextgrid(textgrid_path, includeEmptyIntervals=False)\n",
    "    phone_tier = tg.getTier(\"phones\")\n",
    "    return [(start, end, label) for start, end, label in phone_tier.entries]\n",
    "\n",
    "# Load phoneme-to-index mapping\n",
    "with open(\n",
    "    r\"/scratch2/bsow/Documents/ACSR/data/training_videos/phoneme_dictionary.txt\", \"r\"\n",
    ") as file:\n",
    "    reader = csv.reader(file)\n",
    "    vocabulary_list = [row[0] for row in reader]\n",
    "\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(vocabulary_list)}\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "phoneme_to_index[\" \"] = len(phoneme_to_index)\n",
    "index_to_phoneme[len(index_to_phoneme)] = \" \"\n",
    "\n",
    "def load_coordinates(directory, base_name):\n",
    "    \"\"\"\n",
    "    Load pre-extracted coordinates from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory containing the coordinate files.\n",
    "        base_name (str): Base name of the video (e.g., 'sent_01').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the coordinates.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(directory, f\"{base_name}_coordinates.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)  # Drop rows with NaN values\n",
    "    return df\n",
    "\n",
    "def prepare_data_for_videos_no_sliding_windows(\n",
    "    base_names, phoneme_files, audio_dir, textgrid_dir, video_dir, coordinates_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data for all videos without sliding windows.\n",
    "\n",
    "    Args:\n",
    "        base_names (list): List of base names for videos.\n",
    "        phoneme_files (dict): Dictionary of phoneme file paths.\n",
    "        audio_dir (str): Directory containing audio files.\n",
    "        textgrid_dir (str): Directory containing TextGrid files.\n",
    "        video_dir (str): Directory containing video files.\n",
    "        coordinates_dir (str): Directory containing pre-extracted coordinate files.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing combined features, spectrograms, and phoneme sequences.\n",
    "    \"\"\"\n",
    "    all_videos_data = {}\n",
    "    for base_name in base_names:\n",
    "        if base_name in phoneme_files:\n",
    "            # Load pre-extracted coordinates\n",
    "            coordinates_df = load_coordinates(coordinates_dir, base_name)\n",
    "            if 'frame_number' not in coordinates_df.columns:\n",
    "                raise ValueError(f\"Coordinate file for {base_name} does not contain 'frame_number' column.\")\n",
    "            frame_numbers = coordinates_df['frame_number'].values\n",
    "\n",
    "            # Separate coordinates into hand shape, hand position, and lip landmarks\n",
    "            hand_shape_columns = [f\"hand_x{i}\" for i in range(21)] + [f\"hand_y{i}\" for i in range(21)] + [f\"hand_z{i}\" for i in range(21)]\n",
    "            hand_pos_columns = [\"hand_pos_x\", \"hand_pos_y\", \"hand_pos_z\"]\n",
    "            lip_columns = [f\"lip_x{i}\" for i in range(40)] + [f\"lip_y{i}\" for i in range(40)] + [f\"lip_z{i}\" for i in range(40)]\n",
    "\n",
    "            X_student_hand_shape = coordinates_df[hand_shape_columns].to_numpy()\n",
    "            X_student_hand_pos = coordinates_df[hand_pos_columns].to_numpy()\n",
    "            X_student_lips = coordinates_df[lip_columns].to_numpy()\n",
    "\n",
    "            # Load audio and compute spectrogram\n",
    "            audio_path = os.path.join(audio_dir, f\"{base_name}.wav\")\n",
    "            log_mel_spectrogram = compute_log_mel_spectrogram(audio_path)\n",
    "\n",
    "            # Load TextGrid and get phoneme labels for the entire sequence\n",
    "            textgrid_path = os.path.join(textgrid_dir, f\"{base_name}.TextGrid\")\n",
    "            phoneme_intervals = parse_textgrid(textgrid_path)\n",
    "\n",
    "            # Extract phoneme labels as a sequence (without frame-level alignment)\n",
    "            phoneme_labels = [interval[2] for interval in phoneme_intervals]  # Extract phoneme labels\n",
    "\n",
    "            # Convert phoneme labels to indices\n",
    "            phoneme_indices = [phoneme_to_index.get(phoneme, -1) for phoneme in phoneme_labels]\n",
    "\n",
    "            # Combine features, spectrogram, and phoneme indices\n",
    "            all_videos_data[base_name] = {\n",
    "                \"X_student_hand_shape\": X_student_hand_shape,  # Hand shape coordinates\n",
    "                \"X_student_hand_pos\": X_student_hand_pos,      # Hand position coordinates\n",
    "                \"X_student_lips\": X_student_lips,              # Lip landmarks\n",
    "                \"X_teacher\": log_mel_spectrogram,              # Audio features (log-mel spectrogram)\n",
    "                \"y\": phoneme_indices,                          # Phoneme labels (sequence)\n",
    "            }\n",
    "    return all_videos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178894/2338251819.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  X_student_hand_shape_tensor = torch.tensor(X_student_hand_shape_padded, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "data_dir = r'/scratch2/bsow/Documents/ACSR/output/predictions'\n",
    "phoneme_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/lpc'\n",
    "audio_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/audio'\n",
    "textgrid_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/textgrids'\n",
    "video_dir = r'/scratch2/bsow/Documents/ACSR/data/training_videos/videos'\n",
    "coordinates_dir = r'/scratch2/bsow/Documents/ACSR/output/extracted_coordinates'\n",
    "\n",
    "coordinates_data = load_csv_files(coordinates_dir, \"_coordinates\")\n",
    "# Find phoneme files\n",
    "base_names = coordinates_data.keys()\n",
    "phoneme_files = find_phoneme_files(phoneme_dir, base_names)\n",
    "\n",
    "# Prepare data\n",
    "all_videos_data = prepare_data_for_videos_no_sliding_windows(\n",
    "    base_names, phoneme_files, audio_dir, textgrid_dir, video_dir, coordinates_dir\n",
    ")\n",
    "\n",
    "# Combine sequences with padding\n",
    "X_student_hand_shape_padded, X_student_hand_pos_padded, X_student_lips_padded, X_teacher_padded, y_padded = combine_sequences_with_padding(all_videos_data)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_student_hand_shape_tensor = torch.tensor(X_student_hand_shape_padded, dtype=torch.float32)\n",
    "X_student_hand_pos_tensor = torch.tensor(X_student_hand_pos_padded, dtype=torch.float32)\n",
    "X_student_lips_tensor = torch.tensor(X_student_lips_padded, dtype=torch.float32)\n",
    "X_teacher_tensor = torch.tensor(X_teacher_padded, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_padded, dtype=torch.long)\n",
    "\n",
    "# Final organized data\n",
    "all_videos_data = {\n",
    "    \"X_student_hand_shape\": X_student_hand_shape_tensor,  # Hand shape coordinates\n",
    "    \"X_student_hand_pos\": X_student_hand_pos_tensor,      # Hand position coordinates\n",
    "    \"X_student_lips\": X_student_lips_tensor,              # Lip landmarks\n",
    "    \"X_teacher\": X_teacher_tensor,                        # Audio features (log-mel spectrogram)\n",
    "    \"y\": y_tensor,                                        # Phoneme labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train dataset 85\n",
      "Len of val dataset 10\n",
      "Batch X_student_hand_shape shape: torch.Size([4, 327, 63])\n",
      "Batch X_student_hand_pos shape: torch.Size([4, 327, 3])\n",
      "Batch X_student_lips shape: torch.Size([4, 327, 120])\n",
      "Batch X_teacher shape: torch.Size([4, 327, 80])\n",
      "Batch y shape: torch.Size([4, 327])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to split data into training and validation sets\n",
    "def train_val_split(data, train_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing the dataset.\n",
    "        train_ratio (float): Proportion of data to use for training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two dictionaries for training and validation data.\n",
    "    \"\"\"\n",
    "    num_samples = len(data['X_student_hand_shape'])\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    \n",
    "    # Randomize the data\n",
    "    indices = torch.randperm(num_samples)\n",
    "    \n",
    "    # Split hand shape features\n",
    "    X_student_hand_shape = data['X_student_hand_shape'][indices]\n",
    "    X_student_hand_shape_train = X_student_hand_shape[:split_idx]\n",
    "    X_student_hand_shape_val = X_student_hand_shape[split_idx:]\n",
    "    \n",
    "    # Split hand position features\n",
    "    X_student_hand_pos = data['X_student_hand_pos'][indices]\n",
    "    X_student_hand_pos_train = X_student_hand_pos[:split_idx]\n",
    "    X_student_hand_pos_val = X_student_hand_pos[split_idx:]\n",
    "    \n",
    "    # Split lip features\n",
    "    X_student_lips = data['X_student_lips'][indices]\n",
    "    X_student_lips_train = X_student_lips[:split_idx]\n",
    "    X_student_lips_val = X_student_lips[split_idx:]\n",
    "    \n",
    "    # Split teacher features\n",
    "    X_teacher = data['X_teacher'][indices]\n",
    "    X_teacher_train = X_teacher[:split_idx]\n",
    "    X_teacher_val = X_teacher[split_idx:]\n",
    "    \n",
    "    # Split labels\n",
    "    y = data['y'][indices]\n",
    "    y_train = y[:split_idx]\n",
    "    y_val = y[split_idx:]\n",
    "    \n",
    "    # Create train and validation data dictionaries\n",
    "    train_data = {\n",
    "        'X_student_hand_shape': X_student_hand_shape_train,\n",
    "        'X_student_hand_pos': X_student_hand_pos_train,\n",
    "        'X_student_lips': X_student_lips_train,\n",
    "        'X_teacher': X_teacher_train,\n",
    "        'y': y_train\n",
    "    }\n",
    "    val_data = {\n",
    "        'X_student_hand_shape': X_student_hand_shape_val,\n",
    "        'X_student_hand_pos': X_student_hand_pos_val,\n",
    "        'X_student_lips': X_student_lips_val,\n",
    "        'X_teacher': X_teacher_val,\n",
    "        'y': y_val\n",
    "    }\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "# Convert data to DataLoader format\n",
    "def data_to_dataloader(data, batch_size=4, shuffle=True):\n",
    "    \"\"\"\n",
    "    Convert data into PyTorch DataLoader format.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing the dataset.\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader object.\n",
    "    \"\"\"\n",
    "    X_student_hand_shape_tensors = data['X_student_hand_shape']\n",
    "    X_student_hand_pos_tensors = data['X_student_hand_pos']\n",
    "    X_student_lips_tensors = data['X_student_lips']\n",
    "    X_teacher_tensors = data['X_teacher']\n",
    "    y_tensors = data['y']\n",
    "    \n",
    "    # Create a TensorDataset with inputs and labels\n",
    "    dataset = TensorDataset(\n",
    "        X_student_hand_shape_tensors,\n",
    "        X_student_hand_pos_tensors,\n",
    "        X_student_lips_tensors,\n",
    "        X_teacher_tensors,\n",
    "        y_tensors\n",
    "    )\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_val_split(all_videos_data)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = data_to_dataloader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = data_to_dataloader(val_data, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Len of train dataset\", len(train_data['X_student_hand_shape']))\n",
    "print(\"Len of val dataset\", len(val_data['X_student_hand_shape']))\n",
    "\n",
    "# Check the DataLoader output\n",
    "for batch_X_student_hand_shape, batch_X_student_hand_pos, batch_X_student_lips, batch_X_teacher, batch_y in train_loader:\n",
    "    print(\"Batch X_student_hand_shape shape:\", batch_X_student_hand_shape.shape)\n",
    "    print(\"Batch X_student_hand_pos shape:\", batch_X_student_hand_pos.shape)\n",
    "    print(\"Batch X_student_lips shape:\", batch_X_student_lips.shape)\n",
    "    print(\"Batch X_teacher shape:\", batch_X_teacher.shape)\n",
    "    print(\"Batch y shape:\", batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining of a deep speech model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "class IPADataset(Dataset):\n",
    "    def __init__(self, manifest_file, alphabet_file, sample_rate=16000, n_mels=80):\n",
    "        self.manifest = pd.read_csv(manifest_file, header=None)\n",
    "        with open(alphabet_file, \"r\") as f:\n",
    "            self.alphabet = list(set(f.read().splitlines()))\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, ipa_path = self.manifest.iloc[idx]\n",
    "        \n",
    "        # Load audio and compute mel spectrogram\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
    "        )\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Load IPA transcription\n",
    "        with open(ipa_path, \"r\") as f:\n",
    "            ipa = f.read().strip()\n",
    "        \n",
    "        # Convert IPA to indices\n",
    "        ipa_indices = [self.alphabet.index(c) for c in ipa.split()]\n",
    "        \n",
    "        return torch.tensor(mel_spec, dtype=torch.float32), torch.tensor(ipa_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepSpeech2(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=1024, num_layers=5, n_mels=80):\n",
    "        super(DeepSpeech2, self).__init__()\n",
    "        \n",
    "        # 2D Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        # Batch Normalization for Convolutional Layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Bidirectional LSTM Layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32 * n_mels,  # Output size of the last convolutional layer\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # Batch Normalization for LSTM Output\n",
    "        self.bn_lstm = nn.BatchNorm1d(hidden_size * 2)  # *2 for bidirectional LSTM\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional LSTM\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional Layers with BatchNorm and ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        batch_size, channels, n_mels, time_steps = x.size()\n",
    "        x = x.permute(0, 3, 1, 2)  # Move time_steps to the second dimension\n",
    "        x = x.reshape(batch_size, time_steps, -1)  # Flatten the last two dimensions\n",
    "        \n",
    "        # LSTM Layers\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply BatchNorm to LSTM output\n",
    "        x = x.permute(0, 2, 1)  # Swap dimensions for BatchNorm\n",
    "        x = self.bn_lstm(x)\n",
    "        x = x.permute(0, 2, 1)  # Swap back to original dimensions\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "class IPADataset(Dataset):\n",
    "    def __init__(self, manifest_file, alphabet_file, sample_rate=16000, n_mels=80):\n",
    "        self.manifest = pd.read_csv(manifest_file, header=None)\n",
    "        with open(alphabet_file, \"r\") as f:\n",
    "            self.alphabet = f.read().splitlines()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(list(set(self.alphabet)))}\n",
    "        self.index_to_phoneme = {idx: phoneme for idx, phoneme in phoneme_to_index.items()}\n",
    "        self.index_to_phoneme[len(self.index_to_phoneme)] = \" \"\n",
    "        self.phoneme_to_index[\" \"] = len(self.phoneme_to_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, ipa_path = self.manifest.iloc[idx]\n",
    "        \n",
    "        # Load audio and compute mel spectrogram\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
    "        )\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Load IPA transcription\n",
    "        with open(ipa_path, \"r\") as f:\n",
    "            ipa = f.read().strip().split()\n",
    "        \n",
    "        # Convert IPA to indices\n",
    "        ipa_indices = [self.phoneme_to_index[phoneme] for phoneme in ipa]\n",
    "        return torch.tensor(mel_spec, dtype=torch.float32), torch.tensor(ipa_indices, dtype=torch.long), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to pad mel spectrograms and IPA indices to the same length.\n",
    "    Args:\n",
    "        batch: List of tuples (mel_spec, ipa_indices).\n",
    "    Returns:\n",
    "        Padded mel spectrograms and IPA indices.\n",
    "    \"\"\"\n",
    "    # Separate mel spectrograms and IPA indices\n",
    "    mel_specs, ipa_indices = zip(*batch)\n",
    "    \n",
    "    # Transpose mel spectrograms to have time dimension first\n",
    "    mel_specs = [mel_spec.T for mel_spec in mel_specs]  # Transpose to [time, n_mels]\n",
    "    \n",
    "    # Pad mel spectrograms to the same length\n",
    "    mel_specs_padded = pad_sequence(mel_specs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad IPA indices to the same length\n",
    "    ipa_indices_padded = pad_sequence(ipa_indices, batch_first=True, padding_value=47)\n",
    "    \n",
    "    return mel_specs_padded, ipa_indices_padded\n",
    "\n",
    "dataset = IPADataset(manifest_file=\"/scratch2/bsow/Documents/ACSR/data/train.csv\", alphabet_file=\"/scratch2/bsow/Documents/ACSR/data/training_videos/phoneme_dictionary.txt\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ø', 'œ', 'ɑ', 'ɑ̃', 'ɔ', 'ɔ̃', 'ə', 'ɛ', 'ɛ̃', 'ɡ', 'ɥ', 'ʁ', 'ʃ', 'ʒ', '̃', 'ɲ', 'ʎ', 'dʒ', 'mʲ', 'spn', 'g', 'r', 'ɪ', 'ɒ', 'ɪ', 'r', 'ʊ', 'ɑ', 'h', 'ɒ', 'h'] 47\n",
      "{'ɛ̃': 0, 'd': 1, 's': 2, 'ʁ': 3, 'l': 4, 'p': 5, 't': 6, 'h': 7, 'spn': 8, 'z': 9, 'm': 10, 'y': 11, 'ø': 12, 'n': 13, 'ɡ': 14, 'ʊ': 15, 'e': 16, 'c': 17, 'ɲ': 18, 'ʎ': 19, 'ɥ': 20, 'i': 21, 'ɑ': 22, 'ʒ': 23, 'v': 24, 'ʃ': 25, '̃': 26, 'k': 27, 'ɒ': 28, 'b': 29, 'ɛ': 30, 'u': 31, 'w': 32, 'dʒ': 33, 'mʲ': 34, 'a': 35, 'j': 36, 'r': 37, 'o': 38, 'œ': 39, 'ɪ': 40, 'ɑ̃': 41, 'ɔ': 42, 'ə': 43, 'g': 44, 'ɔ̃': 45, 'f': 46, ' ': 47} 48\n"
     ]
    }
   ],
   "source": [
    "print(dataset.alphabet, len(set(dataset.alphabet)))\n",
    "print(dataset.phoneme_to_index, len(dataset.phoneme_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape input:  torch.Size([32, 80, 285])\n",
      "shape output:  torch.Size([285, 32, 48])\n",
      "target lengths:  tensor([29, 30, 55, 63, 53, 53, 20, 38, 41, 35, 47, 55, 43, 53, 40, 39, 25, 56,\n",
      "        45, 48, 47, 30, 57, 55, 19, 53, 50, 26, 32, 59, 29, 53])\n",
      "padding:  47\n",
      "Epoch [1/10], Batch [1/16], Loss: 24.043476104736328\n",
      "shape input:  torch.Size([32, 80, 310])\n",
      "shape output:  torch.Size([310, 32, 48])\n",
      "target lengths:  tensor([38, 36, 20, 44, 43, 18, 40, 62, 35, 43, 38, 46, 50, 38, 61, 43, 33, 29,\n",
      "        35, 40, 34, 59, 48, 28, 69, 21, 45, 22, 43, 47, 52,  8])\n",
      "padding:  47\n",
      "Epoch [1/10], Batch [2/16], Loss: 12.449857711791992\n",
      "shape input:  torch.Size([32, 80, 275])\n",
      "shape output:  torch.Size([275, 32, 48])\n",
      "target lengths:  tensor([46, 54, 34, 33, 57, 41, 33, 25, 47, 25, 29, 45, 36, 46, 57, 52, 38, 22,\n",
      "        33, 45, 19, 22, 27, 37, 13, 54, 49, 36, 49, 21, 24, 38])\n",
      "padding:  47\n",
      "Epoch [1/10], Batch [3/16], Loss: 11.36514663696289\n",
      "shape input:  torch.Size([32, 80, 299])\n",
      "shape output:  torch.Size([299, 32, 48])\n",
      "target lengths:  tensor([48, 19, 30, 38, 35, 14, 53, 37, 68, 43, 63, 59, 61, 33, 64, 53, 39, 26,\n",
      "        45, 18, 15, 28, 37, 40, 37, 43, 65, 76, 41, 58, 38, 20])\n",
      "padding:  47\n",
      "Epoch [1/10], Batch [4/16], Loss: 7.338805675506592\n",
      "shape input:  torch.Size([32, 80, 292])\n",
      "shape output:  torch.Size([292, 32, 48])\n",
      "target lengths:  tensor([42, 15, 27, 73, 29, 36, 53, 50, 34, 53, 52, 35, 28, 29, 56, 27, 73, 41,\n",
      "        45, 27, 49, 45, 52, 42, 63, 21, 39, 56, 34, 60, 40, 25])\n",
      "padding:  47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/10], Batch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = DeepSpeech2(num_classes=len(phoneme_to_index))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CTCLoss()\n",
    "\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    for batch_idx, (mel_spec, ipa_indices) in enumerate(dataloader):\n",
    "        # Transpose mel spectrograms back to [batch, n_mels, time]\n",
    "        mel_spec = mel_spec.permute(0, 2, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(mel_spec.unsqueeze(1))  # Add channel dimension\n",
    "        outputs = outputs.permute(1, 0, 2)  # CTC expects (time, batch, num_classes)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        input_lengths = torch.full((mel_spec.size(0),), outputs.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.tensor([len(ipa[ipa != 47]) for ipa in ipa_indices], dtype=torch.long)\n",
    "        loss = criterion(F.log_softmax(outputs, dim=-1), ipa_indices, input_lengths, target_lengths)\n",
    "        \n",
    "        print(\"shape input: \", mel_spec.shape)\n",
    "        print(\"shape output: \", outputs.shape)\n",
    "        print(\"target lengths: \", target_lengths)\n",
    "        print(\"padding: \", phoneme_to_index[\" \"])\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/10], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pretrained model\n",
    "torch.save(model.state_dict(), \"/scratch2/bsow/Documents/ACSR/output/saved_models/deepspeech2_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, hand_shape_dim, hand_pos_dim, lips_dim, hidden_dim, output_dim):\n",
    "        super(StudentModel, self).__init__()\n",
    "        \n",
    "        # Feature extractors\n",
    "        self.hand_shape_extractor = nn.Sequential(\n",
    "            nn.Linear(hand_shape_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        self.hand_pos_extractor = nn.Sequential(\n",
    "            nn.Linear(hand_pos_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        self.lips_extractor = nn.Sequential(\n",
    "            nn.Linear(lips_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=64 + 32 + 128,  # Combined feature size\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Output phoneme predictions\n",
    "\n",
    "    def forward(self, hand_shape, hand_pos, lips):\n",
    "        # Extract features\n",
    "        hand_shape_features = self.hand_shape_extractor(hand_shape)\n",
    "        hand_pos_features = self.hand_pos_extractor(hand_pos)\n",
    "        lips_features = self.lips_extractor(lips)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([hand_shape_features, hand_pos_features, lips_features], dim=-1)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_out, _ = self.bilstm(combined_features)\n",
    "        \n",
    "        # Final predictions\n",
    "        output = self.fc(lstm_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_level_distillation_loss(student_logits, teacher_logits, batch_y, input_lengths, label_lengths):\n",
    "    \"\"\"\n",
    "    Compute the sequence-level distillation loss.\n",
    "\n",
    "    Args:\n",
    "        student_logits (torch.Tensor): Logits from the student model.\n",
    "        teacher_logits (torch.Tensor): Logits from the teacher model.\n",
    "        batch_y (torch.Tensor): Padded ground-truth labels.\n",
    "        input_lengths (torch.Tensor): Lengths of input sequences.\n",
    "        label_lengths (torch.Tensor): Lengths of label sequences (excluding padding).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Combined loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cosine similarity loss\n",
    "    cosine_loss = 1 - F.cosine_similarity(student_logits, teacher_logits, dim=-1).mean()\n",
    "    \n",
    "    # CTC loss\n",
    "    log_probs = F.log_softmax(student_logits, dim=-1)  # Log-softmax of student_logits\n",
    "    log_probs = log_probs.permute(1, 0, 2)  # Reshape to [sequence_length, batch_size, num_classes]\n",
    "    \n",
    "    ctc_loss = F.ctc_loss(\n",
    "        log_probs,\n",
    "        batch_y,  # Padded labels\n",
    "        input_lengths,\n",
    "        label_lengths,  # Lengths of label sequences (excluding padding)\n",
    "        blank=phoneme_to_index[\" \"],  # Blank token index\n",
    "        reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = cosine_loss + ctc_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 37.37840158289129\n",
      "Epoch 2/10, Loss: 37.11240161548961\n",
      "Epoch 3/10, Loss: 37.10461547157981\n",
      "Epoch 4/10, Loss: 37.41879671270197\n",
      "Epoch 5/10, Loss: 36.94501521370628\n",
      "Epoch 6/10, Loss: 37.14860708063299\n",
      "Epoch 7/10, Loss: 37.82866304570978\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize the student model\n",
    "student_model = StudentModel(\n",
    "    hand_shape_dim=63,  # 21 keypoints × 3 coordinates\n",
    "    hand_pos_dim=3,     # 3 coordinates (x, y, z)\n",
    "    lips_dim=120,       # 40 keypoints × 3 coordinates\n",
    "    hidden_dim=256,\n",
    "    output_dim=len(phoneme_to_index)  # Number of phonemes\n",
    ")\n",
    "\n",
    "# load the pretrained teacher model\n",
    "teacher_model = DeepSpeech2(num_classes=len(phoneme_to_index))\n",
    "teacher_model.load_state_dict(torch.load(\"/scratch2/bsow/Documents/ACSR/output/saved_models/deepspeech2_pretrained.pth\"))\n",
    "\n",
    "# Set teacher model to evaluation mode\n",
    "teacher_model.eval()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_student_model(student_model, teacher_model, train_loader, val_loader, num_epochs=10):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X_student_hand_shape, batch_X_student_hand_pos, batch_X_student_lips, batch_X_teacher, batch_y in train_loader:\n",
    "            # Reshape batch_X_teacher for the teacher model\n",
    "            batch_X_teacher = batch_X_teacher.unsqueeze(1)  # Add channel dimension\n",
    "            batch_X_teacher = batch_X_teacher.permute(0, 1, 3, 2)  # Transpose to (batch_size, 1, num_mel_bins, time_steps)\n",
    "            \n",
    "            # Forward pass through the student model\n",
    "            student_logits = student_model(batch_X_student_hand_shape, batch_X_student_hand_pos, batch_X_student_lips)\n",
    "            \n",
    "            # Forward pass through the teacher model\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(batch_X_teacher)  # Ensure teacher model outputs logits\n",
    "            \n",
    "            # Compute input_lengths\n",
    "            input_lengths = torch.full(\n",
    "                (batch_X_student_hand_shape.size(0),),  # Batch size\n",
    "                student_logits.size(1),  # Sequence length (time steps) from student_logits\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Compute label_lengths (excluding padding)\n",
    "            label_lengths = torch.tensor(\n",
    "                [len(seq[seq != 42]) for seq in batch_y],  # Length of each label sequence (excluding padding)\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = sequence_level_distillation_loss(student_logits, teacher_logits, batch_y, input_lengths, label_lengths)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Start training\n",
    "train_student_model(student_model, teacher_model, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X_teacher shape: torch.Size([4, 327, 161])\n"
     ]
    }
   ],
   "source": [
    "print(f\"batch_X_teacher shape: {batch_X_teacher.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded training phoneme sequences: [['la', 'p', 'ɥi', 'ʁ', 'wɑ', 'ɑ̃', 't'], ['pu', 'v', 'zɛ̃', 'ɛ', 'ʁə', 'pɛ̃', 'sa', 't'], ['mɔ̃', 'p', 'ʁɔ', 'ʁ', 't'], ['a', 't'], ['bɛ', 't', 'ʁ', 'pɛ̃', 'ɑ̃', 't'], ['dɛ', 't'], ['le', 'la', 'ka', 'do', 'pu', 'a', 't'], ['t'], ['sə', 'fɛ', 'lø', 'lø', 't'], ['la', 'p', 'lø', 'tɛ', 'də', 't'], ['i', 'p', 't'], ['pu', 'ɡo', 'n', 't', 't'], ['t'], ['i', 't'], ['pu', 'ʁa', 'ɡ', 'za', 'mɛ̃', 'ɛ', 'ma', 'nə', 't'], ['i', 't'], ['ʒə', 'a', 't'], ['t'], ['le', 'p', 'tɛ', 'kœ', 'ji', 'ʁɛ̃', 'də', 't'], ['a', 't'], ['ʁ'], ['le', 'nɔ', 'z', 'sɑ̃', 'lɛ', 'ze', 't'], ['dɛ', 't'], ['i', 't', 'ʁə', 'ze', 'na', 'tɛ', 'e'], ['la', 's', 'sɔ', 's', 't', 'ʁ', 't'], ['la', 'ya', 'sɔ', 'a', 't'], ['pu', 't'], ['le', 'ʁ'], ['lø', 't'], ['i', 'la', 'fɑ̃', 'də', 'sœ', 'ʁɔ̃', 'də', 't'], ['pu', 'zɔ̃', 'lɛ̃', 't'], ['la', 'də', 'sɔ̃', 'a', 'ti', 'a', 't'], ['la', 'də', 'sə', 't'], ['t'], ['a', 'p', 'də', 't'], ['i', 'a', 'i', 'di', 'fi', 't'], ['le', 'k', 't'], ['bɛ', 'kœ', 'ji', 't'], ['la', 'p', 't'], ['le', 'tɛ', 't'], ['a', 't'], ['la', 'ʁ', 't'], ['dɛ', 't'], ['la', 't', 'də', 'ʁ', 't'], ['dɛ', 't'], ['le', 'ɑ̃', 'p', 'də', 'lɔ̃', 't'], ['o', 'p', 'lø', 'ʒa', 'dɛ̃', 't'], ['la', 'p', 't'], ['pu', 'd', 'ʁ', 't'], ['lø', 'ʁ', 'b', 'də', 'lø', 'də', 't'], ['pu', 'bɛ', 'də', 'də', 't'], ['a', 'n', 'fu', 't'], ['pu', 'tɔ', 'mɑ', 'a', 'də', 't'], ['pu', 'se', 't'], ['pu', 'tɛ', 'də', 'vo', 'pa', 't'], ['le', 't'], ['lø', 'n', 'fu', 't'], ['i', 'də', 'tɛ', 't'], ['a', 'ʁ'], ['a', 't'], ['lø', 'ʃœ', 't'], ['i', 't'], ['le', 't'], ['ʒe', 'ɑ̃', 't'], ['ʁ', 'd', 'ye', 'nə', 'sa', 't'], ['la', 'də', 'lœ', 't'], ['dɛ', 't'], ['i', 'a', 't'], ['pu', 'ʁ', 's', 'lɑ̃', 't'], ['le', 'mɔ̃', 'ji', 'lø', 't'], ['a', 't'], ['la', 'p', 'la', 't'], ['la', 'də', 't'], ['a', 'mɑ̃', 'ʒe', 't'], ['le', 'o', 'ʃɔ', 'ce', 't', 'ʃo', 'ze', 't', 'ʒa', 't'], ['i', 't'], ['ʁi', 'z', 'kə', 'sɔ̃', 't'], ['jɔ̃', 'nə', 'tu', 'ʃe', 't'], ['pu', 'nə', 'pa', 'v', 'wa', 'ʁ', 'fɛ̃', 'i', 'lo', 't'], ['dɛ', 'ʁ'], ['o', 'də', 'ʁɔ', 'zi', 'ke', 't'], ['pu', 'kə', 'e', 'ci', 'də', 't'], ['le', 't'], ['na', 'tɑ̃', 'se', 'ʃe', 't'], ['dɛ', 't']]\n",
      "True training phoneme sequences: [['də', 'p', 'ɥi', 't', 'ʁ', 'wɑ', 'ɑ̃', 'ɛ', 'l', 't', 'ʁa', 'va', 'ja', 'jɛ', 'dɑ̃', 'la', 'ʁ', 'm'], ['kɔ', 'm', 'se', 'v', 'wa', 'zɛ̃', 'ɛ', 'la', 'ɛ', 'l', 'mɛ', 'm', 'ʁə', 'pɛ̃', 'sa', 'v', 'wa', 't', 'y', 'ʁ'], ['mɔ̃', 'p', 'ʁɔ', 'fɛ', 'sœ', 'ʁ', 'ma', 'di', 'ci', 'la', 'vɛ', 'tu', 'ʒu', 'ʁ', 'y', 'pœ', 'ʁ', 'də', 'la', 'po', 'v', 'ʁə', 'te'], ['di', 'na', 'a', 'e', 'te', 's', 'y', 'ʁ', 'p', 'ʁi', 'z', 'da', 'p', 'ʁɑ̃', 'd', 'ʁ', 'kə', 'sɔ̃', 'mɛ', 'jœ', 'ʁa', 'mi', 'e', 'tɛ', 'mɔ', 'ʁ'], ['sa', 'nu', 'vɛ', 'l', 'ʃɑ̃', 'b', 'ʁa', 'ɛ̃', 'm', 'y', 'ʁɑ̃', 't', 'jɛ', 'ʁ', 'mɑ̃', 'pɛ̃', 'ɑ̃', 'ʁu', 'ʒ'], ['dɛ', 'sɔ̃', 'a', 'ʁi', 've', 'dɑ̃', 'lø', 'ɡ', 'ʁɑ̃', 'ma', 'ɡa', 'zɛ̃', 'ɛ', 'l', 'sɛ', 'a', 'ʃ', 'te', 'ɛ̃', 'pa', 'ʁa', 'p', 'l', 'ɥi'], ['la', 'fi', 'jɛ', 'ta', 'ʁə', 's', 'y', 'sə', 'ka', 'do', 'pu', 'ʁ', 'sɔ̃', 'a', 'i', 'vɛ', 'ʁ', 'sɛ', 'ʁ'], ['ɑ̃', 'ʁɑ̃', 't', 'ʁɑ̃', 'le', 'a', 't', 'ʁu', 'va', 's', 'y', 'ʁ', 'la', 'ta', 'b', 'l', 'sɔ̃', 'v', 'jø', 'ku', 'to'], ['ɔ̃', 'fɛ', 'lø', 'a', 'vɛ', 'k', 'lø', 'ʒ', 'y', 'de', 'pɔ', 'm'], ['a', 'p', 'ʁɛ', 'a', 'v', 'wa', 'ʁe', 'te', 'la', 've', 'lø', 'kɔ', 'l', 'ne', 'tɛ', 'p', 'l', 'y', 'ta', 'ʃe', 'də', 'b', 'wa'], ['i', 'la', 'p', 'ʁe', 'si', 'la', 'ʁi', 'ʃɛ', 's', 'ka', 'ʁi', 'lɑ', 'ɡ', 'ʁɑ̃', 'di', 'dɑ̃', 'la', 'po', 'v', 'ʁə', 'te'], ['y', 'ɡo', 'vi', 'vɛ', 'dɑ̃', 'y', 'n', 'ka', 'ba', 'n', 'ci', 'la', 'vɛ', 'l', 'ɥi', 'mɛ', 'm', 'kɔ̃', 's', 't', 'ʁ', 'ɥi', 't'], ['ɛ̃', 't', 'ʁɛ', 'ʒœ', 'n', 'ʃ', 'jɛ̃', 'sa', 'pɛ', 'lɛ̃', 'ʃ', 'jo'], ['i', 'lɛ', 'ɑ̃', 't', 'ʁe', 'dɑ̃', 'sɛ', 't', 'ʃɑ̃', 'b', 'ʁe', 'ɛ', 'mɛ̃', 't', 'nɑ̃', 'ma', 'la', 'd'], ['pu', 'ʁ', 'ʁe', 'y', 'si', 'ʁa', 'lɛ', 'ɡ', 'za', 'mɛ̃', 'ɛ', 'ma', 'nə', 'mɑ̃', 'cɛ', 'pa', 'də', 'fɔ', 'ʁ', 's'], ['dɑ̃', 'sɛ', 't', 'b', 'wa', 't', 'sə', 'ka', 'ʃɛ̃', 'ʃ', 'jo'], ['ʒə', 's', 'ɥi', 'a', 'le', 'a', 'ʃ', 'te', 'ɛ̃', 'ʃa'], ['a', 'vɛ', 'k', 'lø', 'p', 'wa', 'sɔ̃', 'la', 'so', 's', 'ne', 'tɛ', 'pa', 't', 'ʁɛ', 'fɔ', 'ʁ', 't'], ['i', 'za', 'bɛ', 'le', 'tɛ', 'sɔ', 'ʁ', 'ti', 'kœ', 'ji', 'ʁɛ̃', 'bu', 'cɛ', 'də', 'mɑ̃', 't'], ['le', 'dø', 'v', 'wa', 'zɛ̃', 'ɔ̃', 'di', 's', 'k', 'y', 'te', 'e', 'ɔ̃', 'kɔ̃', 'p', 'ʁi', 'ci', 'le', 'tɛ', 'a', 'mu', 'ʁø'], ['ma', 'nɔ̃', 'a', 'e', 'te', 't', 'ʁi', 's', 't', 'də', 'de', 'ku', 'v', 'ʁi', 'ʁ', 'kə', 'sɔ̃', 'p', 'wa', 'sɔ̃', 'ʁu', 'ʒe', 'tɛ', 'mɔ', 'ʁ'], ['le', 'nɔ', 'ʁ', 'm', 'va', 'i', 'z', 'sɑ̃', 'b', 'lɛ', 'pə', 'ze', 'y', 'n', 'tɔ', 'n'], ['a', 'p', 'ʁɛ', 'dø', 'sə', 'mɛ', 'ni', 'i', 'a', 'vɛ', 'p', 'lɛ̃', 'də', 'vɛ', 't', 'mɑ̃', 'sa', 'la', 'la', 've'], ['i', 'l', 'vu', 'lɛ', 't', 'ʁa', 'va', 'je', 'pu', 'ʁ', 'y', 'nɑ̃', 't', 'ʁə', 'p', 'ʁi', 'ze', 'na', 'pa', 'e', 'zi', 'te', 'a', 'si', 'e', 'lø', 'kɔ̃', 't', 'ʁa'], ['la', 'nɔ̃', 's', 'də', 'sɔ', 'l', 'd', 't', 'ʁɛ', 'ɛ̃', 'pɔ', 'ʁ', 'tɑ̃', 'ta', 'a', 'ti', 'ʁe', 'y', 'ni', 'mɑ̃', 's', 'fu', 'l'], ['la', 'ku', 'lœ', 'ʁ', 'la', 'p', 'l', 'ya', 'sɔ', 's', 'je', 'a', 'la', 'kɔ', 'lɛ', 'ʁɛ', 'lø', 'ʁu', 'ʒ'], ['pu', 'ʁa', 'ʁi', 've', 'a', 'tɑ̃', 'i', 'lo', 'ʁɛ', 'd', 'y', 'ma', 'ʁ', 'ʃe', 'p', 'l', 'y', 'vi', 't'], ['pu', 'ʁɔ', 'nɔ', 'ʁe', 'la', 'mɔ', 'ʁ', 'də', 'sɔ̃', 'kɔ', 'lɛ', 'ɡ', 'ci', 'ma', 'a', 'si', 's', 'te', 'a', 'sɔ̃', 'ɑ̃', 'tɛ', 'ʁ', 'mɑ̃'], ['lø', 'ʒ', 'y', 'də', 'ɛ', 't', 'ʁɛ', 'a', 'si', 'd'], ['pɑ̃', 'dɑ̃', 'la', 'ʁe', 'y', 'ɔ̃', 'le', 'ɑ̃', 'fɑ̃', 'də', 'sa', 'sœ', 'ʁɔ̃', 'de', 'si', 'de', 'də', 'fɛ', 'ʁɛ̃', 'pɑ'], ['pu', 'ʁ', 'ʁɑ̃', 't', 'ʁe', 'dɑ̃', 'la', 'mɛ', 'zɔ̃', 'lɛ̃', 's', 'pɛ', 'k', 'tœ', 'ʁa', 'd', 'y', 'mɔ̃', 't', 'ʁe', 'sɔ̃', 'ba'], ['ɛ', 'la', 's', 'y', 'ɡ', 'ʒe', 'ʁe', 'a', 'sɔ̃', 'pə', 'ti', 'a', 'mi', 'ba', 'ʁ', 'b', 'y', 'də', 'sə', 'ʁa', 'ze'], ['i', 'la', 'di', 'a', 'sɔ̃', 'f', 'ʁɛ', 'ʁ', 'ci', 'l', 'də', 'vɛ', 'v', 'ʁɛ', 'mɑ̃', 'sə', 'ʁa', 'ze'], ['sɛ', 'si', 'd'], ['a', 'p', 'ʁɛ', 'sɛ', 't', 'ɡ', 'ʁo', 's', 'be', 'ti', 'zi', 'l', 'ʁu', 'ʒi', 'də', 'ɔ̃', 't'], ['mɔ̃', 'f', 'ʁɛ', 'ʁ', 'pɔ', 'se', 'dɛ', 'de', 'a', 'i', 'mo', 'di', 'fi', 'si', 'la', 'nu', 'ʁi', 'ʁ'], ['i', 'la', 'vɛ', 'pœ', 'ʁ', 'de', 'mi', 'k', 'ʁɔ', 'be', 'k', 'ʁɛ', 'ɛ', 'də', 'tɔ̃', 'be', 'ma', 'la', 'd'], ['bɛ', 'ʁ', 'na', 'ʁ', 'la', 'vɛ', 'a', 'kœ', 'ji', 'a', 'vɛ', 'kɛ̃', 'te', 'a', 'la', 'mɑ̃', 't'], ['ɑ̃', 'sa', 'p', 'ʁɔ', 'ʃɑ̃', 'd', 'y', 'n', 'ʁ', 'y', 'ʃɛ', 'la', 'e', 'te', 'pi', 'ce', 'pa', 'ʁ', 'y', 'na', 'bɛ', 'j'], ['le', 'pɔ', 'i', 's', 'je', 'na', 'vɛ', 'pa', 'd', 'y', 'tu', 'kɔ̃', 'p', 'ʁi', 'cɛ', 'l', 'se', 'tɛ', 'ʁɑ̃', 'kɔ̃', 't', 'ʁe'], ['ɑ̃', 'l', 'y', 'k', 'sɛ', 'ku', 'pe', 'lø', 'd', 'wa', 'a', 'vɛ', 'kɛ̃', 'ku', 'to'], ['la', 'mɛ', 'ʁɛ̃', 'cɛ', 'ta', 'vɛ', 'p', 'l', 'y', 'z', 'jœ', 'ʁ', 'bu', 'ʃa', 'nu', 'ʁi', 'ʁ'], ['dɛ', 'lœ', 'ʁ', 'ʁə', 'tu', 'ʁa', 'la', 'mɛ', 'zɔ̃', 'va', 'lɑ̃', 'tɛ̃', 'l', 'ɥi', 'a', 'dɔ', 'ne', 'y', 'n', 'sɛ', 'ʁ', 'v', 'jɛ', 't'], ['la', 'pə', 'ti', 't', 'fi', 'je', 'tɛ', 'kɔ̃', 'tɑ̃', 't', 'də', 'pɛ', 'ʁ', 'd', 'ʁ', 'sa', 'p', 'ʁə', 'ɛ', 'ʁ', 'dɑ̃'], ['le', 'ma', 'ʁ', 'ʃɑ̃', 'də', 'le', 'ɡ', 'y', 'm', 'nɔ̃', 'pa', 'la', 'bi', 't', 'y', 'd', 'də', 'fɛ', 'ʁ', 'la', 'ɛ', 'ʁ'], ['le', 'ci', 'p', 'sɛ', 'ɑ̃', 't', 'ʁɛ', 'ne', 'də', 't', 'ʁɛ', 'lɔ̃', 'ɡ', 'sə', 'mɛ', 'n', 's', 'y', 'ʁ', 'sɔ̃', 'ba', 'to'], ['o', 'p', 'ʁɛ̃', 'tɑ̃', 'dɑ̃', 'lø', 'ʒa', 'ʁ', 'dɛ̃', 'sɔ̃', 'pɛ', 'ʁ', 'tɔ̃', 'bɛ', 'la', 'pə', 'lu', 'z'], ['la', 'p', 'ʁɛ', 'mi', 'di', 'i', 'l', 's', 'y', 'pu', 'ʁ', 'k', 'wa', 'i', 'la', 'vɛ', 'ɔ̃', 't'], ['pu', 'ʁa', 'me', 'ɔ', 'ʁe', 'sɛ', 't', 'ʁə', 'sɛ', 'ti', 'l', 'fo', 'd', 'ʁɛ', 'a', 'ʒu', 'te', 'ɛ̃', 'pø', 'də', 's', 'y', 'k', 'ʁ'], ['də', 'lœ', 'ʁ', 'ʃɑ̃', 'b', 'ʁ', 'le', 'ɑ̃', 'fɑ̃', 'ɑ̃', 'tɑ̃', 'dɛ', 'b', 'jɛ̃', 'lø', 'b', 'ʁ', 'ɥi', 'də', 'la', 'fɛ', 't'], ['ɑ̃', 'ʁə', 'ɡa', 'ʁ', 'dɑ̃', 'sɔ̃', 'i', 'v', 'ʁi', 'l', 'də', 'vi', 'ne', 'lɛ', 's', 'pɛ', 's', 'də', 'la', 'bɛ', 'j'], ['la', 'v', 'jɛ', 'j', 'fa', 'm', 'mɛ', 'dɑ̃', 'sɔ̃', 'sa', 'kɛ̃', 'pə', 'ti', 'b', 'ʁi', 'cɛ', 'e', 'y', 'n', 'fu', 'ʁ', 'ʃɛ', 't'], ['pu', 'ʁu', 'v', 'ʁi', 'ʁ', 'la', 'pɔ', 'ʁ', 't', 'tɔ', 'mɑ', 'a', 'ti', 'ʁe', 'də', 'tu', 't', 'se', 'fɔ', 'ʁ', 's'], ['dɛ', 'sa', 'ma', 'ʒɔ', 'ʁi', 'te', 'i', 'l', 'se', 'tɛ', 'ɑ̃', 'ɡa', 'ʒe', 'dɑ̃', 'la', 'ʁ', 'me'], ['ʒ', 'y', 'a', 'e', 'tɛ', 'də', 'nu', 'vo', 'pa', 'ʁ', 'ti', 'ɑ̃', 'va', 'kɑ̃', 'sa', 'vɛ', 'k', 'sɔ̃', 'a', 'mi'], ['ɛ', 'l', 'nɛ', 'ɑ̃', 'kɔ', 'ʁ', 'ʒa', 'mɛ', 'pɑ', 'se', 'də', 'vɑ̃', 'la', 'bu', 'ʁ', 's'], ['lø', 'pə', 'ti', 'ɡa', 'ʁ', 'sɔ̃', 'mɑ̃', 'ʒ', 'sɔ̃', 's', 'tɛ', 'ka', 'vɛ', 'kɛ̃', 'ku', 'to', 'e', 'y', 'n', 'fu', 'ʁ', 'ʃɛ', 't'], ['e', 'mi', 'i', 'la', 'vɛ', 'de', 'tɛ', 's', 'te', 'dɛ', 'lø', 'mɔ', 'mɑ̃', 'u', 'ɛ', 'l', 'se', 'tɛ', 'ʁɑ̃', 'kɔ̃', 't', 'ʁe'], ['sa', 'a', 'di', 'mi', 'n', 'ɥe', 'e', 'ɛ', 'l', 'd', 'wa', 'a', 'ʃ', 'te', 'y', 'n', 'nu', 'vɛ', 'l', 'pɛ', 'ʁ', 'də', 'l', 'y', 'nɛ', 't'], ['sɔ̃', 'pɛ', 'ʁa', 'vɛ', 'dɑ̃', 'lø', 'tɑ̃', 'y', 'nɛ̃', 'k', 'ʁ', 'wa', 'ja', 'b', 'l', 'kɔ', 'lɛ', 'k', 's', 'jɔ̃', 'də', 'fɔ', 'tɔ', 's'], ['lø', 'pɛ', 'ʃœ', 'ʁɛ', 'mɛ', 'pa', 'se', 'la', 'ʒu', 'ʁ', 'ne', 'ɑ̃', 'mɛ', 'ʁ', 's', 'y', 'ʁ', 'sɔ̃', 'ba', 'to'], ['a', 'p', 'ʁɛ', 'y', 'n', 'ba', 'la', 'd', 'dɑ̃', 'le', 'ʃɑ̃', 'me', 'bɔ', 'te', 'tɛ', 'ku', 'vɛ', 'ʁ', 't', 'də', 'bu'], ['le', 'tɑ̃', 's', 'jɔ̃', 'ɑ̃', 't', 'ʁə', 'le', 'pɛ', 'i', 'pu', 'ʁɛ', 'a', 'bu', 'ti', 'ʁa', 'la', 'ɛ', 'ʁ'], ['ʒe', 'ɑ̃', 'tɑ̃', 'd', 'y', 'v', 'jɔ', 'le', 'ɛ̃', 'ʃa'], ['ɡa', 's', 'pa', 'ʁe', 'tɛ', 'pɛ', 'ʁ', 'd', 'ye', 'nə', 'sa', 'vɛ', 'pa', 'k', 'wa', 'fɛ', 'ʁ'], ['ɛ', 'la', 'a', 'ʃ', 'te', 'ɛ̃', 'po', 'də', 'f', 'lœ', 'ʁ', 'pu', 'ʁ', 'sɔ̃', 'a', 'i', 'vɛ', 'ʁ', 'sɛ', 'ʁ'], ['le', 'fo', 'ʒ', 'y', 'mo', 'e', 'tɛ', 'o', 'si', 'di', 'fe', 'ʁɑ̃', 'kə', 'lø', 'ʒu', 'ʁe', 'la', 'n', 'ɥi'], ['a', 'p', 'ʁɛ', 'sa', 'pa', 'nɛ̃', 'pa', 'sɑ̃', 'la', 'ɛ', 'de', 'a', 'pu', 'se', 'sa', 'v', 'wa', 't', 'y', 'ʁ'], ['pu', 'ʁa', 'v', 'wa', 'ʁ', 'pa', 'ʁ', 'ti', 'si', 'pe', 'a', 'la', 'ku', 'ʁ', 's', 'lɑ̃', 'fɑ̃', 'a', 'ʁə', 's', 'yɛ̃', 'ba'], ['le', 'ɔ', 'mɔ̃', 'fa', 'ji', 'tɔ̃', 'be', 'dɑ̃', 'lø', 't', 'ʁu', 'ci', 'la', 'vɛ', 'k', 'ʁø', 'ze'], ['i', 'l', 'va', 'a', 'ʃ', 'te', 'de', 'a', 'k', 's', 'jɔ̃', 'kɔ', 'te', 'ɑ̃', 'bu', 'ʁ', 's'], ['sə', 'p', 'la', 'ɛ', 'bo', 'ku', 't', 'ʁo', 'a', 'si', 'd'], ['la', 'bu', 'ti', 'k', 'da', 'k', 'sɛ', 's', 'wa', 'ʁe', 'tɛ', 'dɑ̃', 'ɛ̃', 'ka', 'ʁ', 't', 'je', 'e', 'l', 'wa', 'n', 'je', 'də', 'la', 'pu', 'l'], ['sɔ̃', 'a', 'mi', 'tu', 'ʒu', 'ʁa', 'dɔ', 'ʁe', 'mɑ̃', 'ʒe', 'de', 'pɔ', 'm'], ['le', 'o', 'e', 'tɛ', 'ʃɔ', 'ce', 'də', 'vɑ̃', 'sɛ', 't', 'ʃo', 'ze', 't', 'ʁɑ̃', 'ʒa', 'fɛ', 'ʁ'], ['i', 'l', 'l', 'ɥi', 'a', 'mi', 'ɛ̃', 'ku', 'to', 'su', 'la', 'ɡɔ', 'ʁ', 'ʒ'], ['ka', 'mi', 'je', 'tɛ', 't', 'ʁɛ', 's', 'y', 'ʁ', 'p', 'ʁi', 'z', 'kə', 'sɔ̃', 'ma', 'ʁi', 'nɛ', 'pɑ', 'ɡa', 'e'], ['fɛ', 'ta', 'tɑ̃', 's', 'jɔ̃', 'a', 'nə', 'pa', 'tu', 'ʃe', 'la', 'p', 'wa', 'l', 'kɑ̃', 'tɛ', 'lɛ', 'ʃo', 'd'], ['pu', 'ʁ', 'nə', 'pa', 'a', 'v', 'wa', 'ʁ', 'fɛ̃', 'i', 'lo', 'ʁɛ', 'd', 'y', 'mɑ̃', 'ʒe', 'p', 'l', 'y', 'vi', 't'], ['ɛ', 'la', 'ɑ̃', 'pɔ', 'ʁ', 'te', 'sɔ̃', 'nu', 'vɛ', 'la', 'pa', 'ʁɛ', 'j', 'pu', 'ʁ', 'p', 'ʁɑ̃', 'd', 'ʁ', 'də', 'bɛ', 'l', 'fɔ', 'tɔ', 's'], ['o', 'kɔ̃', 'sɛ', 'ʁ', 'də', 'ʁɔ', 'k', 'la', 'm', 'y', 'zi', 'ke', 'tɛ', 't', 'ʁo', 'fɔ', 'ʁ', 't'], ['ce', 'vi', 'ne', 'tɛ', 'de', 's', 'y', 'kə', 'sɔ̃', 'e', 'ci', 'p', 'də', 'ba', 's', 'cɛ', 't', 'nɛ', 'pa', 'ɡa', 'e'], ['a', 'p', 'ʁɛ', 'cɛ', 'l', 'k', 'm', 'wa', 'i', 'lɔ̃', 'ɑ̃', 'ɡa', 'ʒe', 'ɛ̃', 'ɔ', 'm', 'pu', 'ʁ', 'tu', 'la', 've'], ['a', 'p', 'ʁɛ', 'sɛ', 't', 'ʁ', 'la', 've', 'le', 'mɛ̃', 'na', 'tɑ̃', 'le', 'se', 'ʃe', 'a', 'vɛ', 'k', 'y', 'n', 'sɛ', 'ʁ', 'vo'], ['dɛ', 'la', 'p', 'ʁə', 'ɛ', 'ʁ', 'ɡu', 't', 'də', 'p', 'l', 'ɥi', 'ɛ', 'la', 'u', 'vɛ', 'ʁ', 'sɔ̃', 'pa', 'ʁa', 'p', 'li']]\n",
      "Decoded validation phoneme sequences: [['pu', 't'], ['la', 'də', 't'], ['le', 't'], ['pu', 'də', 'də', 't'], ['dɛ', 't'], ['la', 't', 'tɛ', 'a', 'kə', 'də', 'lø', 'kœ', 'nə', 'ji', 't'], ['a', 'ʁ'], ['wa', 'jɔ̃', 'nə', 'ʃe', 't', 'də', 'ti', 't'], ['la', 'tɛ', 'pa', 'nə', 't'], ['i', 't']]\n",
      "True validation phoneme sequences: [['pu', 'ʁ', 'fɛ', 'te', 'sɔ̃', 'de', 'pa', 'ʁ', 'se', 'a', 'mi', 'l', 'ɥi', 'ɔ̃', 'ɔ', 'ʁ', 'ɡa', 'i', 'ze', 'y', 'n', 'fɛ', 't'], ['lø', 'po', 'v', 'ʁɔ', 'm', 'sɛ̃', 'ce', 'tɛ', 'tu', 'ʒu', 'ʁ', 'də', 'pɛ', 'ʁ', 'd', 'ʁ', 'y', 'n', 'nu', 'vɛ', 'l', 'dɑ̃'], ['ɛ', 'lɛ', 'mɛ', 'bo', 'ku', 'la', 'i', 'bɛ', 'ʁ', 'te', 'kə', 'lœ', 'ʁɔ', 'f', 'ʁɛ', 'la', 'n', 'ɥi'], ['pu', 'ʁ', 'ʁɑ̃', 'd', 'ʁə', 'sɔ̃', 'ka', 'fe', 'm', 'wɛ̃', 'a', 'mɛ', 'ʁɛ', 'l', 'mi', 'ɛ̃', 'mɔ', 'ʁ', 'so', 'də', 's', 'y', 'k', 'ʁ'], ['le', 'dø', 'e', 't', 'y', 'd', 'jɑ̃', 'sə', 'sɔ̃', 'ʁɑ̃', 'kɔ̃', 't', 'ʁe', 'e', 'sɔ̃', 'ʁa', 'pi', 'd', 'mɑ̃', 'tɔ̃', 'be', 'a', 'mu', 'ʁø'], ['pɑ̃', 'dɑ̃', 'la', 'ʁɑ̃', 'dɔ', 'ne', 'le', 'p', 'je', 'də', 'i', 'la', 'l', 'ɥi', 'fə', 'zɛ', 'p', 'l', 'y', 'ma', 'la', 'ʃa', 'k', 'pɑ'], ['i', 'lɛ', 'm', 'lø', 'kɔ', 'mɛ', 'ʁ', 'se', 'a', 'de', 'si', 'de', 'du', 'v', 'ʁi', 'ʁɛ̃', 'ma', 'ɡa', 'zɛ̃', 'də', 'l', 'y', 'nɛ', 't'], ['k', 'ʁi', 'di', 'kə', 'sɔ̃', 'ʃ', 'jɛ̃', 'a', 'vɛ', 'tu', 'ʒu', 'ʁe', 'te', 'sɔ̃', 'mɛ', 'jœ', 'ʁa', 'mi'], ['a', 'tɑ̃', 'dɔ̃', 'nu', 'a', 'sə', 'kə', 'sɛ', 't', 'mɛ', 'zɔ̃', 'nə', 's', 'wa', 'pa', 'v', 'ʁɛ', 'mɑ̃', 'ʃo', 'd'], ['ɛ', 'l', 'tu', 'ʒu', 'ʁ', 'y', 'ma', 'l']]\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(output, blank):\n",
    "    \"\"\"\n",
    "    Decode model outputs using a greedy decoder.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model outputs of shape (batch_size, sequence_length, num_classes).\n",
    "        blank (int): Index of the blank token.\n",
    "\n",
    "    Returns:\n",
    "        list: List of decoded sequences.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)  # Get the most likely class for each time step\n",
    "    decodes = []\n",
    "    for args in arg_maxes:\n",
    "        decode = []\n",
    "        previous_idx = None\n",
    "        for index in args:\n",
    "            if index != blank and (previous_idx is None or index != previous_idx):\n",
    "                decode.append(index.item())  # Append non-blank and non-repeated tokens\n",
    "            previous_idx = index\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "\n",
    "def decode_loader(model, loader, blank, index_to_phoneme):\n",
    "    \"\"\"\n",
    "    Decode outputs for all batches in a DataLoader and return both decoded and true sequences.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        loader (torch.utils.data.DataLoader): DataLoader containing input data and labels.\n",
    "        blank (int): Index of the blank token.\n",
    "        index_to_phoneme (dict): Mapping from indices to phonemes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (decoded_sequences, true_sequences), where:\n",
    "            - decoded_sequences: List of decoded phoneme sequences.\n",
    "            - true_sequences: List of true phoneme sequences.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_decoded_sequences = []\n",
    "    all_true_sequences = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, y_batch in loader:  # Iterate over batches (X_batch: inputs, y_batch: labels)\n",
    "            X_batch = X_batch.to(device)  # Move inputs to device\n",
    "            y_batch = y_batch.to(device)  # Move labels to device\n",
    "            outputs = model(X_batch)  # Get model predictions\n",
    "            decoded_phoneme_sequences = greedy_decoder(outputs, blank=blank)  # Decode outputs\n",
    "            decoded_phonemes = [[index_to_phoneme[idx] for idx in sequence] for sequence in decoded_phoneme_sequences]  # Convert indices to phonemes\n",
    "            all_decoded_sequences.extend(decoded_phonemes)  # Add to the list of decoded sequences\n",
    "\n",
    "            # Convert true labels to phoneme sequences\n",
    "            true_phoneme_sequences = [[index_to_phoneme[idx.item()] for idx in sequence if idx != blank and \n",
    "                                       index_to_phoneme[idx.item()] != \" \"] for sequence in y_batch]\n",
    "            all_true_sequences.extend(true_phoneme_sequences)  # Add to the list of true sequences\n",
    "\n",
    "    return all_decoded_sequences, all_true_sequences\n",
    "\n",
    "\n",
    "# Example usage\n",
    "blank_token = len(phoneme_to_index)  # Index of the blank token\n",
    "decoded_train_sequences, true_train_sequences = decode_loader(model, train_loader, blank_token, index_to_phoneme)\n",
    "decoded_val_sequences, true_val_sequences = decode_loader(model, val_loader, blank_token, index_to_phoneme)\n",
    "\n",
    "# Print results\n",
    "print(\"Decoded training phoneme sequences:\", decoded_train_sequences)\n",
    "print(\"True training phoneme sequences:\", true_train_sequences)\n",
    "print(\"Decoded validation phoneme sequences:\", decoded_val_sequences)\n",
    "print(\"True validation phoneme sequences:\", true_val_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PER (jiwer): 0.84232868405094 1 - PER:  0.15767131594906003\n",
      "Validation PER (jiwer): 0.9512195121951219 1 - PER:  0.04878048780487809\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_per_with_jiwer(decoded_sequences, true_sequences):\n",
    "    \"\"\"\n",
    "    Calculate the Phoneme Error Rate (PER) using jiwer.\n",
    "\n",
    "    Args:\n",
    "        decoded_sequences (list): List of decoded phoneme sequences.\n",
    "        true_sequences (list): List of true phoneme sequences.\n",
    "\n",
    "    Returns:\n",
    "        float: Phoneme Error Rate (PER).\n",
    "    \"\"\"\n",
    "    # Convert phoneme sequences to space-separated strings\n",
    "    decoded_str = [\" \".join(seq) for seq in decoded_sequences]\n",
    "    true_str = [\" \".join(seq) for seq in true_sequences]\n",
    "\n",
    "    # Calculate PER using jiwer\n",
    "    per = jiwer.wer(true_str, decoded_str)\n",
    "    return per\n",
    "\n",
    "# Example usage\n",
    "train_per = calculate_per_with_jiwer(decoded_train_sequences, true_train_sequences)\n",
    "val_per = calculate_per_with_jiwer(decoded_val_sequences, true_val_sequences)\n",
    "\n",
    "print(\"Training PER (jiwer):\", train_per, \"1 - PER: \", 1 - train_per)\n",
    "print(\"Validation PER (jiwer):\", val_per, \"1 - PER: \", 1 - val_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-jnt0UJEK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
