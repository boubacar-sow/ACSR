{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import whisper\n",
    "import torch\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip\n",
    "from epitran.backoff import Backoff\n",
    "import re\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "\n",
    "# Step 1: Extract audio from the video\n",
    "def extract_audio(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file and saves it as a WAV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    video_clip.audio.write_audiofile(audio_path, codec=\"pcm_s16le\")\n",
    "    print(f\"Audio extracted and saved to: {audio_path}\")\n",
    "\n",
    "# Step 2: Convert text to IPA\n",
    "def text_to_ipa(text, language=\"fra-Latn\"):\n",
    "    \"\"\"\n",
    "    Convert a text sentence into its IPA representation.\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        language (str): Language code for IPA conversion (e.g., \"fra-Latn\" for French).\n",
    "    Returns:\n",
    "        str: IPA representation of the text.\n",
    "    \"\"\"\n",
    "    backoff = Backoff([language])\n",
    "    ipa_text = backoff.transliterate(text)\n",
    "    return ipa_text\n",
    "\n",
    "# Step 3: Syllabify IPA text\n",
    "# Define Cued Speech consonants (hand shapes) and vowels (mouth shapes)\n",
    "consonants = \"ptkbdgmnlrsfvzʃʒɡʁjwŋtrɥʀ\"\n",
    "vowels = \"aeɛioɔuyøœəɑ̃ɛ̃ɔ̃œ̃ɑ̃ɔ̃ɑ̃ɔ̃\"\n",
    "\n",
    "# Regex pattern for syllabification\n",
    "syllable_pattern = re.compile(\n",
    "    f\"[{consonants}]?[{vowels}]|[{consonants}]\", re.IGNORECASE\n",
    ")\n",
    "\n",
    "def syllabify_word(word):\n",
    "    \"\"\"\n",
    "    Syllabify a single word based on the allowed patterns: CV, V, C.\n",
    "    \"\"\"\n",
    "    syllables = syllable_pattern.findall(word)\n",
    "    return \" \".join(syllables)\n",
    "\n",
    "def syllabify_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Syllabify an entire sentence.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = text_to_ipa(sentence)\n",
    "    words = sentence.split()\n",
    "    syllabified_sentence = []\n",
    "    for word in words:\n",
    "        syllabified_sentence.append(syllabify_word(word))\n",
    "    return \" \".join(syllabified_sentence)\n",
    "\n",
    "# Step 4: Transcribe the entire audio using Whisper\n",
    "def transcribe_audio(audio_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Transcribes the entire audio file using OpenAI's Whisper model.\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        device (str): Device to use for inference (\"cuda\" for GPU or \"cpu\" for CPU).\n",
    "    Returns:\n",
    "        list: A list of tuples containing (start_time, end_time, text, ipa_text, syllabified_text).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    # Check if the specified device is available\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # Load the Whisper model on the specified device\n",
    "    model = whisper.load_model(\"medium\", device=device)  # Use \"medium\" or \"large\" for better accuracy\n",
    "\n",
    "    # Transcribe the entire audio file\n",
    "    result = model.transcribe(audio_path, language=\"fr\")\n",
    "    print(\"Audio transcription completed.\")\n",
    "\n",
    "    # Extract segments from the result\n",
    "    segments = []\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"]\n",
    "        ipa_text = text_to_ipa(text)  # Convert text to IPA\n",
    "        syllabified_text = syllabify_sentence(ipa_text)  # Syllabify IPA text\n",
    "        segments.append((segment[\"start\"], segment[\"end\"], text, ipa_text, syllabified_text))\n",
    "    \n",
    "    return segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y go vi vɛ dɑ̃ y n ka ba n ki la vɛ lu mɛ m kɔ̃ s t ru t\n"
     ]
    }
   ],
   "source": [
    "# Example: Manually annotate syllables for a sentence\n",
    "syllable_annotations = [\n",
    "    {\"syllable\": \"y\", \"start_frame\": 13, \"end_frame\": 20},\n",
    "    {\"syllable\": \"go\", \"start_frame\": 24, \"end_frame\": 38},\n",
    "    {\"syllable\": \"vi\", \"start_frame\": 42, \"end_frame\": 45},\n",
    "    {\"syllable\": \"vɛ\", \"start_frame\": 47, \"end_frame\": 55},\n",
    "    {\"syllable\": \"dɑ̃\", \"start_frame\": 60, \"end_frame\": 67},\n",
    "    {\"syllable\": \"y\", \"start_frame\": 72, \"end_frame\": 80},\n",
    "    {\"syllable\": \"n\", \"start_frame\": 82, \"end_frame\": 90}, \n",
    "    {\"syllable\": \"ka\", \"start_frame\": 93, \"end_frame\": 100},\n",
    "    {\"syllable\": \"ba\", \"start_frame\": 101, \"end_frame\": 104},\n",
    "    {\"syllable\": \"n\", \"start_frame\": 105, \"end_frame\": 124},\n",
    "    {\"syllable\": \"ki\", \"start_frame\": 128, \"end_frame\": 134},\n",
    "    {\"syllable\": \"la\", \"start_frame\": 138, \"end_frame\": 140},\n",
    "    {\"syllable\": \"vɛ\", \"start_frame\": 146, \"end_frame\": 152},\n",
    "    {\"syllable\": \"lu\", \"start_frame\": 154, \"end_frame\": 160},\n",
    "    {\"syllable\": \"mɛ\", \"start_frame\": 164, \"end_frame\": 172},\n",
    "    {\"syllable\": \"m\", \"start_frame\": 174, \"end_frame\": 182},\n",
    "    {\"syllable\": \"kɔ̃\", \"start_frame\": 193, \"end_frame\": 202},\n",
    "    {\"syllable\": \"s\", \"start_frame\": 207, \"end_frame\": 211},\n",
    "    {\"syllable\": \"t\", \"start_frame\": 212, \"end_frame\": 217},\n",
    "    {\"syllable\": \"ru\", \"start_frame\": 218, \"end_frame\": 222},\n",
    "    {\"syllable\": \"t\", \"start_frame\": 224, \"end_frame\": 227}\n",
    "]\n",
    "text = \" \".join([s[\"syllable\"] for s in syllable_annotations])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract landmarks using MediaPipe\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extract head and hand landmarks from a video frame using MediaPipe Holistic.\n",
    "    Args:\n",
    "        frame: Input video frame.\n",
    "    Returns:\n",
    "        dict: Landmarks for face, right hand, and left hand.\n",
    "    \"\"\"\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "\n",
    "    # Extract landmarks\n",
    "    landmarks = {\n",
    "        \"face\": results.face_landmarks,\n",
    "        \"right_hand\": results.right_hand_landmarks,\n",
    "        \"left_hand\": results.left_hand_landmarks,\n",
    "    }\n",
    "    return landmarks\n",
    "\n",
    "# Step 6: Build syllable-to-gesture mappings\n",
    "def build_syllable_mappings(video_path, segments):\n",
    "    \"\"\"\n",
    "    Build syllable-to-gesture mappings by extracting hand coordinates during annotated frames.\n",
    "    Args:\n",
    "        video_path: Path to the video file.\n",
    "        segments: List of tuples containing (start_time, end_time, text, ipa_text, syllabified_text).\n",
    "    Returns:\n",
    "        dict: Syllable-to-gesture mappings.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = 33\n",
    "    syllable_mappings = {}\n",
    "\n",
    "    for segment in segments:\n",
    "        start_time, end_time, text, ipa_text, syllabified_text = segment\n",
    "        syllabified_text = text\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        # Set the video to the start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Extract hand coordinates for the syllable\n",
    "        hand_coordinates = []\n",
    "        for _ in range(start_frame, end_frame + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            landmarks = extract_landmarks(frame)\n",
    "            if landmarks[\"right_hand\"]:\n",
    "                hand_coordinates.append(landmarks[\"right_hand\"])\n",
    "\n",
    "        # Map the syllable to the average hand coordinates\n",
    "        if hand_coordinates:\n",
    "            avg_hand_coordinates = np.mean(hand_coordinates, axis=0)\n",
    "            syllable_mappings[syllabified_text] = avg_hand_coordinates\n",
    "\n",
    "    cap.release()\n",
    "    return syllable_mappings\n",
    "\n",
    "# Step 7: Render gestures on the video\n",
    "def render_gestures(video_path, syllable_mappings, output_video_path):\n",
    "    \"\"\"\n",
    "    Render gestures on the video by overlaying hand positions on the head.\n",
    "    Args:\n",
    "        video_path: Path to the input video.\n",
    "        syllable_mappings: Syllable-to-gesture mappings.\n",
    "        output_video_path: Path to save the output video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract landmarks\n",
    "        landmarks = extract_landmarks(frame)\n",
    "\n",
    "        # Overlay hand gestures based on syllable mappings\n",
    "        for syllable, hand_coordinates in syllable_mappings.items():\n",
    "            if landmarks[\"face\"] and landmarks[\"right_hand\"]:\n",
    "                # Draw hand on the face\n",
    "                cv2.circle(frame, (int(hand_coordinates.x * frame_width), int(hand_coordinates.y * frame_height)), 10, (0, 255, 0), -1)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved to: /scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemporary audio file removed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Step 2: Transcribe the entire audio\u001b[39;00m\n\u001b[1;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Set to \"cuda\" for GPU or \"cpu\" for CPU\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Build syllable-to-gesture mappings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m syllable_mappings \u001b[38;5;241m=\u001b[39m build_syllable_mappings(video_path, segments)\n",
      "Cell \u001b[0;32mIn[18], line 82\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[0;34m(audio_path, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# Use \"medium\" or \"large\" for better accuracy\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Transcribe the entire audio file\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio transcription completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Extract segments from the result\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/whisper/transcribe.py:133\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[1;32m    135\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/whisper/audio.py:140\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(audio):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(audio)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/whisper/audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/shared/opt/linux-rocky9-x86_64/gcc-11.4.1/python-3.11.7-nulyujyzmrzmtbbazfbrmx7ygzyn5yua/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/shared/opt/linux-rocky9-x86_64/gcc-11.4.1/python-3.11.7-nulyujyzmrzmtbbazfbrmx7ygzyn5yua/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/shared/opt/linux-rocky9-x86_64/gcc-11.4.1/python-3.11.7-nulyujyzmrzmtbbazfbrmx7ygzyn5yua/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # File paths\n",
    "    video_path = \"/scratch2/bsow/Documents/ACSR/data/training_videos/sent_01.mp4\"  # Replace with your video file path\n",
    "    audio_path = \"/scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\"  # Temporary audio file\n",
    "    output_video_path = \"/scratch2/bsow/Documents/ACSR/data/transcriptions/output_video.mp4\"  # Output video file\n",
    "\n",
    "    # Step 1: Extract audio from the video\n",
    "    extract_audio(video_path, audio_path)\n",
    "\n",
    "    # Step 2: Transcribe the entire audio\n",
    "    device = \"cuda\"  # Set to \"cuda\" for GPU or \"cpu\" for CPU\n",
    "    segments = transcribe_audio(audio_path, device=device)\n",
    "\n",
    "    # Step 3: Build syllable-to-gesture mappings\n",
    "    syllable_mappings = build_syllable_mappings(video_path, segments)\n",
    "\n",
    "    # Step 4: Render gestures on the video\n",
    "    render_gestures(video_path, syllable_mappings, output_video_path)\n",
    "\n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "    print(\"Temporary audio file removed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\u001b[m\n",
      " 1) \u001b[100mglibc/2.34-dtaq\u001b[0m   2) \u001b[100mgcc-runtime/11.4.1-hyx3\u001b[0m   3) miniconda3/24.3.0-ui7c  \u001b[m\n",
      "\u001b[m\n",
      "Key:\u001b[m\n",
      "\u001b[100mauto-loaded\u001b[0m  \u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
