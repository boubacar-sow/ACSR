{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp.holistic.Holistic()\n",
    "\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extract head and hand landmarks from a video frame using MediaPipe Holistic.\n",
    "    Args:\n",
    "        frame: Input video frame.\n",
    "    Returns:\n",
    "        dict: Landmarks for face, right hand, and left hand.\n",
    "    \"\"\"\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "\n",
    "    # Extract landmarks\n",
    "    landmarks = {\n",
    "        \"face\": results.face_landmarks,\n",
    "        \"right_hand\": results.right_hand_landmarks,\n",
    "        \"left_hand\": results.left_hand_landmarks,\n",
    "    }\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manually annotate syllables for a sentence\n",
    "syllable_annotations = [\n",
    "    {\"syllable\": \"ba\", \"start_frame\": 10, \"end_frame\": 30},\n",
    "    {\"syllable\": \"ku\", \"start_frame\": 31, \"end_frame\": 50},\n",
    "    # Add more syllables as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_syllable_mappings(video_path, syllable_annotations):\n",
    "    \"\"\"\n",
    "    Build syllable-to-syllable mappings by extracting hand coordinates \n",
    "    during annotated frames.\n",
    "    Args:\n",
    "        video_path: Path to the video file.\n",
    "        syllable_annotations: List of syllable annotations (start_frame,\n",
    "                            end_frame, syllable).\n",
    "    Returns:\n",
    "        dict: Syllable-to-syllable mappings.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    syllable_mappings = {}\n",
    "\n",
    "    for annotation in syllable_annotations:\n",
    "        syllable = annotation[\"syllable\"]\n",
    "        start_frame = annotation[\"start_frame\"]\n",
    "        end_frame = annotation[\"end_frame\"]\n",
    "\n",
    "        # Set the video to the start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Extract hand coordinates for the syllable\n",
    "        hand_coordinates = []\n",
    "        for _ in range(start_frame, end_frame + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            landmarks = extract_landmarks(frame)\n",
    "            if landmarks[\"right_hand\"]:\n",
    "                hand_coordinates.append(landmarks[\"right_hand\"])\n",
    "\n",
    "        # Map the syllable to the average hand coordinates\n",
    "        if hand_coordinates:\n",
    "            avg_hand_coordinates = np.mean(hand_coordinates, axis=0)\n",
    "            syllable_mappings[syllable] = avg_hand_coordinates\n",
    "\n",
    "    cap.release()\n",
    "    return syllable_mappings\n",
    "\n",
    "def render_gestures(video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-jnt0UJEK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
