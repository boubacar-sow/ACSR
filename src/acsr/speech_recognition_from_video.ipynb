{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import whisper\n",
    "import torch\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip\n",
    "from epitran.backoff import Backoff\n",
    "import re\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y ɡo vi vɛ y n ka ba n dɑ ̃ s y n ka ba n k i l a vɛ l ɥi mə m kɔ ̃ s t ʀ ɥi t'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllabify_sentence(\"Hugo vivait une cabane dans une cabane qu'il avait lui-meme construite\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "\n",
    "# Step 1: Extract audio from the video\n",
    "def extract_audio(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file and saves it as a WAV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    video_clip.audio.write_audiofile(audio_path, codec=\"pcm_s16le\")\n",
    "    print(f\"Audio extracted and saved to: {audio_path}\")\n",
    "\n",
    "# Step 2: Convert text to IPA\n",
    "def text_to_ipa(text, language=\"fra-Latn\"):\n",
    "    \"\"\"\n",
    "    Convert a text sentence into its IPA representation.\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        language (str): Language code for IPA conversion (e.g., \"fra-Latn\" for French).\n",
    "    Returns:\n",
    "        str: IPA representation of the text.\n",
    "    \"\"\"\n",
    "    backoff = Backoff([language])\n",
    "    ipa_text = backoff.transliterate(text)\n",
    "    return ipa_text\n",
    "\n",
    "# Step 3: Syllabify IPA text\n",
    "# Define Cued Speech consonants (hand shapes) and vowels (mouth shapes)\n",
    "consonants = \"ptkbdgmnlrsfvzʃʒɡʁjwŋtrɥʀ\"\n",
    "vowels = \"aeɛioɔuyøœəɑ̃ɛ̃ɔ̃œ̃ɑ̃ɔ̃ɑ̃ɔ̃\"\n",
    "\n",
    "# Regex pattern for syllabification\n",
    "syllable_pattern = re.compile(\n",
    "    f\"[{consonants}]?[{vowels}]|[{consonants}]\", re.IGNORECASE\n",
    ")\n",
    "\n",
    "def syllabify_word(word):\n",
    "    \"\"\"\n",
    "    Syllabify a single word based on the allowed patterns: CV, V, C.\n",
    "    \"\"\"\n",
    "    syllables = syllable_pattern.findall(word)\n",
    "    return \" \".join(syllables)\n",
    "\n",
    "def syllabify_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Syllabify an entire sentence.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = text_to_ipa(sentence)\n",
    "    words = sentence.split()\n",
    "    syllabified_sentence = []\n",
    "    for word in words:\n",
    "        syllabified_sentence.append(syllabify_word(word))\n",
    "    return \" \".join(syllabified_sentence)\n",
    "\n",
    "# Step 4: Transcribe the entire audio using Whisper\n",
    "def transcribe_audio(audio_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Transcribes the entire audio file using OpenAI's Whisper model.\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        device (str): Device to use for inference (\"cuda\" for GPU or \"cpu\" for CPU).\n",
    "    Returns:\n",
    "        list: A list of tuples containing (start_time, end_time, text, ipa_text, syllabified_text).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    # Check if the specified device is available\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # Load the Whisper model on the specified device\n",
    "    model = whisper.load_model(\"medium\", device=device)  # Use \"medium\" or \"large\" for better accuracy\n",
    "\n",
    "    # Transcribe the entire audio file\n",
    "    result = model.transcribe(audio_path, language=\"fr\")\n",
    "    print(\"Audio transcription completed.\")\n",
    "\n",
    "    # Extract segments from the result\n",
    "    segments = []\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"]\n",
    "        ipa_text = text_to_ipa(text)  # Convert text to IPA\n",
    "        syllabified_text = syllabify_sentence(ipa_text)  # Syllabify IPA text\n",
    "        segments.append((segment[\"start\"], segment[\"end\"], text, ipa_text, syllabified_text))\n",
    "    \n",
    "    return segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manually annotate syllables for a sentence\n",
    "syllable_annotations = [\n",
    "    {\"syllable\": \"y\", \"start_frame\": 13, \"end_frame\": 20},\n",
    "    {\"syllable\": \"go\", \"start_frame\": 24, \"end_frame\": 38},\n",
    "    {\"syllable\": \"vi\", \"start_frame\": 42, \"end_frame\": 45},\n",
    "    {\"syllable\": \"vɛ\", \"start_frame\": 47, \"end_frame\": 55},\n",
    "    {\"syllable\": \"dɑ̃\", \"start_frame\": 60, \"end_frame\": 67},\n",
    "    {\"syllable\": \"y\", \"start_frame\": 72, \"end_frame\": 80},\n",
    "    {\"syllable\": \"n\", \"start_frame\": 82, \"end_frame\": 90}, \n",
    "    {\"syllable\": \"ka\", \"start_frame\": 93, \"end_frame\": 100},\n",
    "    {\"syllable\": \"ba\", \"start_frame\": 101, \"end_frame\": 104},\n",
    "    {\"syllable\": \"n\", \"start_frame\": 105, \"end_frame\": 124},\n",
    "    {\"syllable\": \"ki\", \"start_frame\": 128, \"end_frame\": 134},\n",
    "    {\"syllable\": \"la\", \"start_frame\": 138, \"end_frame\": 140},\n",
    "    {\"syllable\": \"vɛ\", \"start_frame\": 146, \"end_frame\": 152},\n",
    "    {\"syllable\": \"lu\", \"start_frame\": 154, \"end_frame\": 160},\n",
    "    {\"syllable\": \"mɛ\", \"start_frame\": 164, \"end_frame\": 172},\n",
    "    {\"syllable\": \"m\", \"start_frame\": 174, \"end_frame\": 182},\n",
    "    {\"syllable\": \"kɔ̃\", \"start_frame\": 193, \"end_frame\": 202},\n",
    "    {\"syllable\": \"s\", \"start_frame\": 207, \"end_frame\": 211},\n",
    "    {\"syllable\": \"t\", \"start_frame\": 212, \"end_frame\": 217},\n",
    "    {\"syllable\": \"ru\", \"start_frame\": 218, \"end_frame\": 222},\n",
    "    {\"syllable\": \"t\", \"start_frame\": 224, \"end_frame\": 227}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract landmarks using MediaPipe\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extract head and hand landmarks from a video frame using MediaPipe Holistic.\n",
    "    Args:\n",
    "        frame: Input video frame.\n",
    "    Returns:\n",
    "        dict: Landmarks for face, right hand, and left hand.\n",
    "    \"\"\"\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "\n",
    "    # Extract landmarks\n",
    "    landmarks = {\n",
    "        \"face\": results.face_landmarks,\n",
    "        \"right_hand\": results.right_hand_landmarks,\n",
    "        \"left_hand\": results.left_hand_landmarks,\n",
    "    }\n",
    "    return landmarks\n",
    "\n",
    "# Step 6: Build syllable-to-gesture mappings\n",
    "def build_syllable_mappings(video_path, segments):\n",
    "    \"\"\"\n",
    "    Build syllable-to-gesture mappings by extracting hand coordinates during annotated frames.\n",
    "    Args:\n",
    "        video_path: Path to the video file.\n",
    "        segments: List of tuples containing (start_time, end_time, text, ipa_text, syllabified_text).\n",
    "    Returns:\n",
    "        dict: Syllable-to-gesture mappings.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    syllable_mappings = {}\n",
    "\n",
    "    for segment in segments:\n",
    "        start_time, end_time, text, ipa_text, syllabified_text = segment\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        # Set the video to the start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Extract hand coordinates for the syllable\n",
    "        hand_coordinates = []\n",
    "        for _ in range(start_frame, end_frame + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            landmarks = extract_landmarks(frame)\n",
    "            if landmarks[\"right_hand\"]:\n",
    "                hand_coordinates.append(landmarks[\"right_hand\"])\n",
    "\n",
    "        # Map the syllable to the average hand coordinates\n",
    "        if hand_coordinates:\n",
    "            avg_hand_coordinates = np.mean(hand_coordinates, axis=0)\n",
    "            syllable_mappings[syllabified_text] = avg_hand_coordinates\n",
    "\n",
    "    cap.release()\n",
    "    return syllable_mappings\n",
    "\n",
    "# Step 7: Render gestures on the video\n",
    "def render_gestures(video_path, syllable_mappings, output_video_path):\n",
    "    \"\"\"\n",
    "    Render gestures on the video by overlaying hand positions on the head.\n",
    "    Args:\n",
    "        video_path: Path to the input video.\n",
    "        syllable_mappings: Syllable-to-gesture mappings.\n",
    "        output_video_path: Path to save the output video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract landmarks\n",
    "        landmarks = extract_landmarks(frame)\n",
    "\n",
    "        # Overlay hand gestures based on syllable mappings\n",
    "        for syllable, hand_coordinates in syllable_mappings.items():\n",
    "            if landmarks[\"face\"] and landmarks[\"right_hand\"]:\n",
    "                # Draw hand on the face\n",
    "                cv2.circle(frame, (int(hand_coordinates.x * frame_width), int(hand_coordinates.y * frame_height)), 10, (0, 255, 0), -1)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # File paths\n",
    "    video_path = \"/scratch2/bsow/Documents/ACSR/data/training_videos/sent_01.mp4\"  # Replace with your video file path\n",
    "    audio_path = \"/scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\"  # Temporary audio file\n",
    "    output_video_path = \"/scratch2/bsow/Documents/ACSR/data/transcriptions/output_video.mp4\"  # Output video file\n",
    "\n",
    "    # Step 1: Extract audio from the video\n",
    "    extract_audio(video_path, audio_path)\n",
    "\n",
    "    # Step 2: Transcribe the entire audio\n",
    "    device = \"cuda\"  # Set to \"cuda\" for GPU or \"cpu\" for CPU\n",
    "    segments = transcribe_audio(audio_path, device=device)\n",
    "\n",
    "    # Step 3: Build syllable-to-gesture mappings\n",
    "    syllable_mappings = build_syllable_mappings(video_path, segments)\n",
    "\n",
    "    # Step 4: Render gestures on the video\n",
    "    render_gestures(video_path, syllable_mappings, output_video_path)\n",
    "\n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "    print(\"Temporary audio file removed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
