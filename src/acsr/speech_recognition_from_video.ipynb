{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 249 frames to /scratch2/bsow/Documents/ACSR/output/rois\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "\n",
    "def extract_lip_roi(frame, prev_points, prev_gray):\n",
    "    \"\"\"\n",
    "    Extract lip ROI using KLT feature tracker.\n",
    "    :param frame: Current video frame.\n",
    "    :param prev_points: Previous tracked points (for KLT).\n",
    "    :param prev_gray: Previous frame in grayscale.\n",
    "    :return: Lip ROI and updated tracked points.\n",
    "    \"\"\"\n",
    "    # Parameters for KLT\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate optical flow (KLT)\n",
    "    if prev_points is not None and prev_gray is not None:\n",
    "        # Ensure prev_points are within the bounds of the current frame\n",
    "        h, w = gray.shape\n",
    "        prev_points = np.clip(prev_points, [0, 0], [w - 1, h - 1]).astype(np.float32)\n",
    "        \n",
    "        new_points, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, gray, prev_points, None, **lk_params)\n",
    "        \n",
    "        # Reshape status to match the dimensions of new_points\n",
    "        status = status.reshape(-1)\n",
    "        \n",
    "        # Filter points based on status\n",
    "        good_new = new_points[status == 1]\n",
    "        good_old = prev_points[status == 1]\n",
    "        \n",
    "        # Compute bounding box around tracked points\n",
    "        if len(good_new) > 0:\n",
    "            x, y, w, h = cv2.boundingRect(good_new)\n",
    "            # Expand the bounding box by a larger margin (e.g., 100 pixels)\n",
    "            margin = 100  # Increased margin for larger lip ROI\n",
    "            x = max(0, x - margin)\n",
    "            y = max(0, y - margin)\n",
    "            w = min(frame.shape[1] - x, w + 2 * margin)\n",
    "            h = min(frame.shape[0] - y, h + 2 * margin)\n",
    "            lip_roi = frame[y:y+h, x:x+w]\n",
    "            return lip_roi, good_new, gray\n",
    "    \n",
    "    # If no previous points or tracking fails, return None\n",
    "    return None, None, gray\n",
    "\n",
    "def extract_hand_roi(frame, fgbg):\n",
    "    \"\"\"\n",
    "    Extract hand ROI using Adaptive Background Mixture Models.\n",
    "    :param frame: Current video frame.\n",
    "    :param fgbg: Background subtractor object (cv2.createBackgroundSubtractorMOG2).\n",
    "    :return: Hand ROI.\n",
    "    \"\"\"\n",
    "    # Apply background subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    # Remove noise using morphological operations\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))\n",
    "    \n",
    "    # Find contours in the foreground mask\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours by area to ignore small contours\n",
    "    min_contour_area = 5000  # Minimum area to consider as a hand\n",
    "    large_contours = [c for c in contours if cv2.contourArea(c) > min_contour_area]\n",
    "    \n",
    "    # Find the largest contour among the filtered contours\n",
    "    if large_contours:\n",
    "        largest_contour = max(large_contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        # Expand the bounding box by a much larger margin (e.g., 200 pixels)\n",
    "        margin = 200  # Increased margin for larger hand ROI\n",
    "        x = max(0, x - margin)\n",
    "        y = max(0, y - margin)\n",
    "        w = min(frame.shape[1] - x, w + 2 * margin)\n",
    "        h = min(frame.shape[0] - y, h + 2 * margin)\n",
    "        hand_roi = frame[y:y+h, x:x+w]\n",
    "        return hand_roi\n",
    "    return None\n",
    "\n",
    "def preprocess_roi(roi, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Convert ROI to grayscale and resize to target size.\n",
    "    :param roi: Input ROI.\n",
    "    :param target_size: Target size (default: 64x64).\n",
    "    :return: Preprocessed ROI.\n",
    "    \"\"\"\n",
    "    if roi is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize using cubic interpolation\n",
    "    resized = resize(gray, target_size, order=3, mode='reflect', anti_aliasing=True)\n",
    "    return (resized * 255).astype(np.uint8)  # Scale back to 0-255 range\n",
    "\n",
    "def preprocess_cued_speech_video(video_path, output_dir):\n",
    "    \"\"\"\n",
    "    Preprocess a cued speech video to extract lip and hand ROIs and save them.\n",
    "    :param video_path: Path to the input video.\n",
    "    :param output_dir: Directory to save the ROIs.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    prev_points = None\n",
    "    prev_gray = None\n",
    "    \n",
    "    # Create background subtractor object\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Initialize KLT points in the first frame\n",
    "        if frame_count == 0:\n",
    "            # Define a region of interest (ROI) for the lips in the first frame\n",
    "            h, w = frame.shape[:2]\n",
    "            lip_region = frame[h//2 - 50:h//2 + 50, w//2 - 50:w//2 + 50]  # Adjust this region as needed\n",
    "            lip_gray = cv2.cvtColor(lip_region, cv2.COLOR_BGR2GRAY)\n",
    "            prev_points = cv2.goodFeaturesToTrack(lip_gray, maxCorners=100, qualityLevel=0.01, minDistance=10)\n",
    "            if prev_points is not None:\n",
    "                prev_points = prev_points.reshape(-1, 1, 2)\n",
    "                # Adjust prev_points to the full frame coordinates\n",
    "                prev_points[:, :, 0] += w//2 - 50\n",
    "                prev_points[:, :, 1] += h//2 - 50\n",
    "        \n",
    "        # Extract lip ROI\n",
    "        lip_roi, prev_points, prev_gray = extract_lip_roi(frame, prev_points, gray)\n",
    "        \n",
    "        # Extract hand ROI\n",
    "        hand_roi = extract_hand_roi(frame, fgbg)\n",
    "        \n",
    "        # Preprocess ROIs\n",
    "        lip_roi_processed = preprocess_roi(lip_roi)\n",
    "        hand_roi_processed = preprocess_roi(hand_roi)\n",
    "        \n",
    "        # Save ROIs if they exist\n",
    "        if lip_roi_processed is not None:\n",
    "            lip_path = os.path.join(output_dir, f\"lip_frame_{frame_count:04d}.png\")\n",
    "            cv2.imwrite(lip_path, lip_roi_processed)\n",
    "        \n",
    "        if hand_roi_processed is not None:\n",
    "            hand_path = os.path.join(output_dir, f\"hand_frame_{frame_count:04d}.png\")\n",
    "            cv2.imwrite(hand_path, hand_roi_processed)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Saved {frame_count} frames to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"/scratch2/bsow/Documents/ACSR/data/training_videos/videos/sent_01.mp4\"\n",
    "output_dir = \"/scratch2/bsow/Documents/ACSR/output/rois\"\n",
    "preprocess_cued_speech_video(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import whisper\n",
    "import torch\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip\n",
    "from epitran.backoff import Backoff\n",
    "import re\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "\n",
    "# Step 1: Extract audio from the video\n",
    "def extract_audio(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file and saves it as a WAV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    video_clip.audio.write_audiofile(audio_path, codec=\"pcm_s16le\")\n",
    "    print(f\"Audio extracted and saved to: {audio_path}\")\n",
    "\n",
    "# Step 2: Convert text to IPA\n",
    "def text_to_ipa(text, language=\"fra-Latn\"):\n",
    "    \"\"\"\n",
    "    Convert a text sentence into its IPA representation.\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        language (str): Language code for IPA conversion (e.g., \"fra-Latn\" for French).\n",
    "    Returns:\n",
    "        str: IPA representation of the text.\n",
    "    \"\"\"\n",
    "    backoff = Backoff([language])\n",
    "    ipa_text = backoff.transliterate(text)\n",
    "    return ipa_text\n",
    "\n",
    "# Step 3: Syllabify IPA text\n",
    "# Define Cued Speech consonants (hand shapes) and vowels (mouth shapes)\n",
    "consonants = \"ptkbdgmnlrsfvzʃʒɡʁjwŋtrɥʀ\"\n",
    "vowels = \"aeɛioɔuyøœəɑ̃ɛ̃ɔ̃œ̃ɑ̃ɔ̃ɑ̃ɔ̃\"\n",
    "\n",
    "# Regex pattern for syllabification\n",
    "syllable_pattern = re.compile(\n",
    "    f\"[{consonants}]?[{vowels}]|[{consonants}]\", re.IGNORECASE\n",
    ")\n",
    "\n",
    "def syllabify_word(word):\n",
    "    \"\"\"\n",
    "    Syllabify a single word based on the allowed patterns: CV, V, C.\n",
    "    \"\"\"\n",
    "    syllables = syllable_pattern.findall(word)\n",
    "    return \" \".join(syllables)\n",
    "\n",
    "def syllabify_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Syllabify an entire sentence.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = text_to_ipa(sentence)\n",
    "    words = sentence.split()\n",
    "    syllabified_sentence = []\n",
    "    for word in words:\n",
    "        syllabified_sentence.append(syllabify_word(word))\n",
    "    return \" \".join(syllabified_sentence)\n",
    "\n",
    "# Step 4: Transcribe the entire audio using Whisper\n",
    "def transcribe_audio(audio_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Transcribes the entire audio file using OpenAI's Whisper model.\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        device (str): Device to use for inference (\"cuda\" for GPU or \"cpu\" for CPU).\n",
    "    Returns:\n",
    "        list: A list of tuples containing (start_time, end_time, text, ipa_text, syllabified_text).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    # Check if the specified device is available\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # Load the Whisper model on the specified device\n",
    "    model = whisper.load_model(\"medium\", device=device)  # Use \"medium\" or \"large\" for better accuracy\n",
    "\n",
    "    # Transcribe the entire audio file\n",
    "    result = model.transcribe(audio_path, language=\"fr\")\n",
    "    print(\"Audio transcription completed.\")\n",
    "\n",
    "    # Extract segments from the result\n",
    "    segments = []\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"]\n",
    "        ipa_text = text_to_ipa(text)  # Convert text to IPA\n",
    "        syllabified_text = syllabify_sentence(ipa_text)  # Syllabify IPA text\n",
    "        segments.append((segment[\"start\"], segment[\"end\"], text, ipa_text, syllabified_text))\n",
    "    \n",
    "    return segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y go vi vɛ dɑ̃ y n ka ba n ki la vɛ lu mɛ m kɔ̃ s t ru t\n"
     ]
    }
   ],
   "source": [
    "# Example: Manually annotate syllables for a sentence\n",
    "syllable_annotations = [\n",
    "    {\"syllable\": \"y\", \"start_frame\": 13, \"end_frame\": 20},\n",
    "    {\"syllable\": \"go\", \"start_frame\": 24, \"end_frame\": 38},\n",
    "    {\"syllable\": \"vi\", \"start_frame\": 42, \"end_frame\": 45},\n",
    "    {\"syllable\": \"vɛ\", \"start_frame\": 47, \"end_frame\": 55},\n",
    "    {\"syllable\": \"dɑ̃\", \"start_frame\": 60, \"end_frame\": 67},\n",
    "    {\"syllable\": \"y\", \"start_frame\": 72, \"end_frame\": 80},\n",
    "    {\"syllable\": \"n\", \"start_frame\": 82, \"end_frame\": 90}, \n",
    "    {\"syllable\": \"ka\", \"start_frame\": 93, \"end_frame\": 100},\n",
    "    {\"syllable\": \"ba\", \"start_frame\": 101, \"end_frame\": 104},\n",
    "    {\"syllable\": \"n\", \"start_frame\": 105, \"end_frame\": 124},\n",
    "    {\"syllable\": \"ki\", \"start_frame\": 128, \"end_frame\": 134},\n",
    "    {\"syllable\": \"la\", \"start_frame\": 138, \"end_frame\": 140},\n",
    "    {\"syllable\": \"vɛ\", \"start_frame\": 146, \"end_frame\": 152},\n",
    "    {\"syllable\": \"lu\", \"start_frame\": 154, \"end_frame\": 160},\n",
    "    {\"syllable\": \"mɛ\", \"start_frame\": 164, \"end_frame\": 172},\n",
    "    {\"syllable\": \"m\", \"start_frame\": 174, \"end_frame\": 182},\n",
    "    {\"syllable\": \"kɔ̃\", \"start_frame\": 193, \"end_frame\": 202},\n",
    "    {\"syllable\": \"s\", \"start_frame\": 207, \"end_frame\": 211},\n",
    "    {\"syllable\": \"t\", \"start_frame\": 212, \"end_frame\": 217},\n",
    "    {\"syllable\": \"ru\", \"start_frame\": 218, \"end_frame\": 222},\n",
    "    {\"syllable\": \"t\", \"start_frame\": 224, \"end_frame\": 227}\n",
    "]\n",
    "text = \" \".join([s[\"syllable\"] for s in syllable_annotations])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract landmarks using MediaPipe\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extract head and hand landmarks from a video frame using MediaPipe Holistic.\n",
    "    Args:\n",
    "        frame: Input video frame.\n",
    "    Returns:\n",
    "        dict: Landmarks for face, right hand, and left hand.\n",
    "    \"\"\"\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "\n",
    "    # Extract landmarks\n",
    "    landmarks = {\n",
    "        \"face\": results.face_landmarks,\n",
    "        \"right_hand\": results.right_hand_landmarks,\n",
    "        \"left_hand\": results.left_hand_landmarks,\n",
    "    }\n",
    "    return landmarks\n",
    "\n",
    "# Step 6: Build syllable-to-gesture mappings\n",
    "def build_syllable_mappings(video_path, segments):\n",
    "    \"\"\"\n",
    "    Build syllable-to-gesture mappings by extracting hand coordinates during annotated frames.\n",
    "    Args:\n",
    "        video_path: Path to the video file.\n",
    "        segments: List of tuples containing (start_time, end_time, text, ipa_text, syllabified_text).\n",
    "    Returns:\n",
    "        dict: Syllable-to-gesture mappings.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = 33\n",
    "    syllable_mappings = {}\n",
    "\n",
    "    for segment in segments:\n",
    "        start_time, end_time, text, ipa_text, syllabified_text = segment\n",
    "        syllabified_text = text\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        # Set the video to the start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Extract hand coordinates for the syllable\n",
    "        hand_coordinates = []\n",
    "        for _ in range(start_frame, end_frame + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            landmarks = extract_landmarks(frame)\n",
    "            if landmarks[\"right_hand\"]:\n",
    "                hand_coordinates.append(landmarks[\"right_hand\"])\n",
    "\n",
    "        # Map the syllable to the average hand coordinates\n",
    "        if hand_coordinates:\n",
    "            avg_hand_coordinates = np.mean(hand_coordinates, axis=0)\n",
    "            syllable_mappings[syllabified_text] = avg_hand_coordinates\n",
    "\n",
    "    cap.release()\n",
    "    return syllable_mappings\n",
    "\n",
    "# Step 7: Render gestures on the video\n",
    "def render_gestures(video_path, syllable_mappings, output_video_path):\n",
    "    \"\"\"\n",
    "    Render gestures on the video by overlaying hand positions on the head.\n",
    "    Args:\n",
    "        video_path: Path to the input video.\n",
    "        syllable_mappings: Syllable-to-gesture mappings.\n",
    "        output_video_path: Path to save the output video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract landmarks\n",
    "        landmarks = extract_landmarks(frame)\n",
    "\n",
    "        # Overlay hand gestures based on syllable mappings\n",
    "        for syllable, hand_coordinates in syllable_mappings.items():\n",
    "            if landmarks[\"face\"] and landmarks[\"right_hand\"]:\n",
    "                # Draw hand on the face\n",
    "                cv2.circle(frame, (int(hand_coordinates.x * frame_width), int(hand_coordinates.y * frame_height)), 10, (0, 255, 0), -1)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved to: /scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemporary audio file removed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Step 2: Transcribe the entire audio\u001b[39;00m\n\u001b[1;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Set to \"cuda\" for GPU or \"cpu\" for CPU\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Build syllable-to-gesture mappings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m syllable_mappings \u001b[38;5;241m=\u001b[39m build_syllable_mappings(video_path, segments)\n",
      "Cell \u001b[0;32mIn[18], line 82\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[0;34m(audio_path, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# Use \"medium\" or \"large\" for better accuracy\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Transcribe the entire audio file\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio transcription completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Extract segments from the result\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/whisper/transcribe.py:133\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[1;32m    135\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/whisper/audio.py:140\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(audio):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(audio)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acsr-jnt0UJEK-py3.11/lib/python3.11/site-packages/whisper/audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/shared/opt/linux-rocky9-x86_64/gcc-11.4.1/python-3.11.7-nulyujyzmrzmtbbazfbrmx7ygzyn5yua/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/shared/opt/linux-rocky9-x86_64/gcc-11.4.1/python-3.11.7-nulyujyzmrzmtbbazfbrmx7ygzyn5yua/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/shared/opt/linux-rocky9-x86_64/gcc-11.4.1/python-3.11.7-nulyujyzmrzmtbbazfbrmx7ygzyn5yua/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # File paths\n",
    "    video_path = \"/scratch2/bsow/Documents/ACSR/data/training_videos/sent_01.mp4\"  # Replace with your video file path\n",
    "    audio_path = \"/scratch2/bsow/Documents/ACSR/data/transcriptions/output_audio.wav\"  # Temporary audio file\n",
    "    output_video_path = \"/scratch2/bsow/Documents/ACSR/data/transcriptions/output_video.mp4\"  # Output video file\n",
    "\n",
    "    # Step 1: Extract audio from the video\n",
    "    extract_audio(video_path, audio_path)\n",
    "\n",
    "    # Step 2: Transcribe the entire audio\n",
    "    device = \"cuda\"  # Set to \"cuda\" for GPU or \"cpu\" for CPU\n",
    "    segments = transcribe_audio(audio_path, device=device)\n",
    "\n",
    "    # Step 3: Build syllable-to-gesture mappings\n",
    "    syllable_mappings = build_syllable_mappings(video_path, segments)\n",
    "\n",
    "    # Step 4: Render gestures on the video\n",
    "    render_gestures(video_path, syllable_mappings, output_video_path)\n",
    "\n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "    print(\"Temporary audio file removed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\u001b[m\n",
      " 1) \u001b[100mglibc/2.34-dtaq\u001b[0m   2) \u001b[100mgcc-runtime/11.4.1-hyx3\u001b[0m   3) miniconda3/24.3.0-ui7c  \u001b[m\n",
      "\u001b[m\n",
      "Key:\u001b[m\n",
      "\u001b[100mauto-loaded\u001b[0m  \u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved landmarks and additional information for handshape_1 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_1.json\n",
      "Saved landmarks and additional information for handshape_2 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_2.json\n",
      "Saved landmarks and additional information for handshape_3 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_3.json\n",
      "Saved landmarks and additional information for handshape_4 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_4.json\n",
      "Saved landmarks and additional information for handshape_5 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_5.json\n",
      "Saved landmarks and additional information for handshape_6 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_6.json\n",
      "Saved landmarks and additional information for handshape_7 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_7.json\n",
      "Saved landmarks and additional information for handshape_8 to /scratch2/bsow/Documents/ACSR/data/handshapes/coordinates/handshape_8.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "# Initialize MediaPipe FaceMesh and Hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = \"/scratch2/bsow/Documents/ACSR/data/handshapes/images\"\n",
    "output_dir = \"/scratch2/bsow/Documents/ACSR/data/handshapes/coordinates\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each image and extract hand landmarks, nose coordinates, and additional face information\n",
    "for i in range(1, 9):  # Loop through handshape_1 to handshape_8\n",
    "    image_path = os.path.join(input_dir, f\"handshape_{i}.jpg\")\n",
    "    output_path = os.path.join(output_dir, f\"handshape_{i}.json\")\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image with MediaPipe FaceMesh to get nose coordinates and face bounding box\n",
    "    face_results = face_mesh.process(rgb_image)\n",
    "    nose_landmarks = None\n",
    "    face_bbox = None\n",
    "    eye_distance = None\n",
    "\n",
    "    if face_results.multi_face_landmarks:\n",
    "        for face_landmarks in face_results.multi_face_landmarks:\n",
    "            # Extract nose landmark (landmark 1)\n",
    "            nose_landmarks = [\n",
    "                face_landmarks.landmark[1].x,\n",
    "                face_landmarks.landmark[1].y,\n",
    "                face_landmarks.landmark[1].z,\n",
    "            ]\n",
    "\n",
    "            # save landmarks 227 (left), 454 (right), 10 (top), 159 (-)\n",
    "            # Extract all x and y coordinates of the face landmarks\n",
    "            x_coords = [landmark.x for landmark in face_landmarks.landmark]\n",
    "            y_coords = [landmark.y for landmark in face_landmarks.landmark]\n",
    "\n",
    "            # Calculate the bounding box of the face\n",
    "            face_bbox = {\n",
    "                \"x_min\": min(x_coords),\n",
    "                \"x_max\": max(x_coords),\n",
    "                \"y_min\": min(y_coords),\n",
    "                \"y_max\": max(y_coords),\n",
    "            }\n",
    "\n",
    "            # Calculate the distance between the eyes (landmarks 33 and 263)\n",
    "            left_eye = face_landmarks.landmark[33]\n",
    "            right_eye = face_landmarks.landmark[263]\n",
    "            eye_distance = ((left_eye.x - right_eye.x) ** 2 + (left_eye.y - right_eye.y) ** 2) ** 0.5\n",
    "            break\n",
    "\n",
    "    # Process the image with MediaPipe Hands to get hand landmarks\n",
    "    hand_results = hands.process(rgb_image)\n",
    "    hand_landmarks = None\n",
    "\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            # Extract 3D coordinates of the hand landmarks\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.append([landmark.x, landmark.y, landmark.z])\n",
    "            hand_landmarks = landmarks\n",
    "            break\n",
    "\n",
    "    # Save the nose, hand landmarks, face bounding box, and eye distance to a JSON file\n",
    "    if nose_landmarks and hand_landmarks and face_bbox and eye_distance:\n",
    "        data = {\n",
    "            \"nose_landmarks\": nose_landmarks,\n",
    "            \"hand_landmarks\": hand_landmarks,\n",
    "            \"face_bbox\": face_bbox,\n",
    "            \"eye_distance\": eye_distance,\n",
    "        }\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "        print(f\"Saved landmarks and additional information for handshape_{i} to {output_path}\")\n",
    "    else:\n",
    "        print(f\"No face or hand detected in {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Draw the landmarks on the black image\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m landmarks:\n\u001b[0;32m---> 49\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlandmark\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(landmark[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     50\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mcircle(black_image, (x, y), \u001b[38;5;241m5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Green circles for landmarks\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Draw connections between landmarks\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = \"/scratch2/bsow/Documents/ACSR/data/handshapes/images\"\n",
    "landmarks_dir = \"/scratch2/bsow/Documents/ACSR/data/handshapes/coordinates\"\n",
    "output_dir = \"/scratch2/bsow/Documents/ACSR/data/handshapes/rendered_handshapes\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define connections between hand landmarks (based on MediaPipe hand connections)\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),  # Thumb\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),  # Index finger\n",
    "    (0, 9), (9, 10), (10, 11), (11, 12),  # Middle finger\n",
    "    (0, 13), (13, 14), (14, 15), (15, 16),  # Ring finger\n",
    "    (0, 17), (17, 18), (18, 19), (19, 20),  # Pinky\n",
    "    (5, 9), (9, 13), (13, 17)  # Palm\n",
    "]\n",
    "\n",
    "# Process each image and draw landmarks on a black background\n",
    "for i in range(1, 9):  # Loop through handshape_1 to handshape_8\n",
    "    image_path = os.path.join(input_dir, f\"handshape_{i}.jpg\")\n",
    "    landmarks_path = os.path.join(landmarks_dir, f\"handshape_{i}.json\")\n",
    "    output_path = os.path.join(output_dir, f\"handshape_{i}_rendered.jpg\")\n",
    "\n",
    "    # Load the image to get its dimensions\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Create a blank black image of the same size\n",
    "    black_image = np.zeros_like(image)\n",
    "\n",
    "    # Load the landmarks from the JSON file\n",
    "    if not os.path.exists(landmarks_path):\n",
    "        print(f\"Error: Could not find landmarks file {landmarks_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(landmarks_path, \"r\") as f:\n",
    "        landmarks = json.load(f)\n",
    "\n",
    "    # Draw the landmarks on the black image\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0] * image.shape[1]), int(landmark[1] * image.shape[0])\n",
    "        cv2.circle(black_image, (x, y), 5, (0, 255, 0), -1)  # Green circles for landmarks\n",
    "\n",
    "    # Draw connections between landmarks\n",
    "    for connection in HAND_CONNECTIONS:\n",
    "        start_idx, end_idx = connection\n",
    "        start_x, start_y = int(landmarks[start_idx][0] * image.shape[1]), int(landmarks[start_idx][1] * image.shape[0])\n",
    "        end_x, end_y = int(landmarks[end_idx][0] * image.shape[1]), int(landmarks[end_idx][1] * image.shape[0])\n",
    "        cv2.line(black_image, (start_x, start_y), (end_x, end_y), (255, 0, 0), 2)  # Blue lines for connections\n",
    "\n",
    "    # Save the rendered image\n",
    "    cv2.imwrite(output_path, black_image)\n",
    "    print(f\"Saved rendered handshape_{i} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable: si -> Hand Shape: 3, Hand Position: (0.1, 0.15, 0.0)\n",
      "Syllable: ne -> Hand Shape: 4, Hand Position: (0.0, 0.3, 0.0)\n",
      "Syllable: ma -> Hand Shape: 5, Hand Position: (0.15, 0.1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping of consonants to hand shapes\n",
    "consonant_to_handshape = {\n",
    "    \"p\": 1, \"t\": 5, \"k\": 2, \"b\": 4, \"d\": 1, \"g\": 7, \"m\": 5, \"n\": 4,\n",
    "    \"l\": 6, \"r\": 3, \"s\": 3, \"f\": 5, \"v\": 2, \"z\": 2, \"ʃ\": 6, \"ʒ\": 1,\n",
    "    \"ɡ\": 7, \"ʁ\": 3, \"j\": 8, \"w\": 6, \"ŋ\": 8, \"ɥ\": 4, \"ʀ\": 3, \"y\": 8, \"c\": 2\n",
    "}\n",
    "\n",
    "# Define vowel positions relative to the nose (right side of the face/body)\n",
    "vowel_positions = {\n",
    "    # Position 1: /a/, /o/, /œ/, /ə/\n",
    "    \"a\": (0.15, 0.1, 0.0),   # Right side of the mouth\n",
    "    \"o\": (0.15, 0.1, 0.0),   # Same as /a/\n",
    "    \"œ\": (0.15, 0.1, 0.0),   # Same as /a/\n",
    "    \"ə\": (0.15, 0.1, 0.0),   # Same as /a/\n",
    "\n",
    "    # Position 2: /ɛ̃/, /ø/\n",
    "    \"ɛ̃\": (0.2, 0.05, 0.0),   # Right cheek\n",
    "    \"ø\": (0.2, 0.05, 0.0),   # Same as /ɛ̃/\n",
    "\n",
    "    # Position 3: /i/, /ɔ̃/, /ɑ̃/\n",
    "    \"i\": (0.1, 0.15, 0.0),   # Right corner of the mouth\n",
    "    \"ɔ̃\": (0.1, 0.15, 0.0),   # Same as /i/\n",
    "    \"ɑ̃\": (0.1, 0.15, 0.0),   # Same as /i/\n",
    "\n",
    "    # Position 4: /u/, /ɛ/, /ɔ/\n",
    "    \"u\": (0.0, 0.2, 0.0),    # Chin (below the mouth)\n",
    "    \"ɛ\": (0.0, 0.2, 0.0),    # Same as /u/\n",
    "    \"ɔ\": (0.0, 0.2, 0.0),    # Same as /u/\n",
    "\n",
    "    # Position 5: /œ̃/, /y/, /e/\n",
    "    \"œ̃\": (0.0, 0.3, 0.0),    # Throat (below the chin)\n",
    "    \"y\": (0.0, 0.3, 0.0),    # Same as /œ̃/\n",
    "    \"e\": (0.0, 0.3, 0.0),    # Same as /œ̃/\n",
    "}\n",
    "\n",
    "def map_syllable_to_cue(syllable):\n",
    "    \"\"\"\n",
    "    Map a syllable to its corresponding hand shape and hand position.\n",
    "    Args:\n",
    "        syllable (str): Syllable in IPA format (e.g., \"si\", \"ne\", \"ma\").\n",
    "    Returns:\n",
    "        tuple: (hand_shape, hand_position)\n",
    "    \"\"\"\n",
    "    # Define vowels and consonants\n",
    "    vowels = set(\"aeɛioɔuøœəɑ̃ɛ̃ɔ̃œ̃y\")\n",
    "    consonants = set(\"ptkbdgmnlrsfvzʃʒɡʁjwŋtrɥgʀyc\")\n",
    "\n",
    "    # Check if the syllable is CV, C, or V\n",
    "    if len(syllable) == 2:  # CV syllable\n",
    "        consonant, vowel = syllable[0], syllable[1]\n",
    "        if consonant in consonants and vowel in vowels:\n",
    "            hand_shape = consonant_to_handshape.get(consonant, 1)  # Default to Hand Shape 1\n",
    "            hand_position = vowel_positions.get(vowel, (0.15, 0.1, 0.0))  # Default to Position 1\n",
    "            return hand_shape, hand_position\n",
    "\n",
    "    elif len(syllable) == 1:  # Single letter (C or V)\n",
    "        if syllable in consonants:  # Single consonant\n",
    "            hand_shape = consonant_to_handshape.get(syllable, 1)  # Default to Hand Shape 1\n",
    "            hand_position = vowel_positions[\"a\"]  # Default to Position 1\n",
    "            return hand_shape, hand_position\n",
    "        elif syllable in vowels:  # Single vowel\n",
    "            hand_shape = 5  # Default to Hand Shape 5\n",
    "            hand_position = vowel_positions.get(syllable, (0.15, 0.1, 0.0))  # Default to Position 1\n",
    "            return hand_shape, hand_position\n",
    "\n",
    "    # Default fallback\n",
    "    return 1, (0.15, 0.1, 0.0)  # Hand Shape 1, Position 1\n",
    "\n",
    "# Example usage\n",
    "syllables = [\"si\", \"ne\", \"ma\"]\n",
    "for syllable in syllables:\n",
    "    hand_shape, hand_position = map_syllable_to_cue(syllable)\n",
    "    print(f\"Syllable: {syllable} -> Hand Shape: {hand_shape}, Hand Position: {hand_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsr-jnt0UJEK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
